[{
  "id": "UG_registration",
  "version": "2024020714",
  "title": "Registration",
  "subTitles": null,
  "location": "MIL UG P03: 2D processing and analysis",
  "pageURL": "content\\UserGuide\\registration\\ChapterInformation.htm",
  "text": " Chapter 11: Registration This chapter explains how to use the MIL Registration module to perform depth-from-focus registration, extended depth of field registration, and photometric stereo registration. The MIL Registration module can also perform correlation-stitching registration and mosaicing, refer to Chapter 12: Correlation stitching registration and mosaicing . MIL Registration module Extended depth of field registration Depth-from-focus registration Photometric stereo registration Basic concepts for the MIL Registration module Extending your depth of field Steps to performing an extended depth of field registration Customizing your extended depth of field context Registration process Retrieving and using extended depth of field results Extended depth of field example Generating an index map from a scene Steps to performing a depth-from-focus registration Customizing your depth-from-focus context Computing a depth-from-focus registration Retrieving and using depth-from-focus results Depth from focus example Surface feature enhancement and defect detection using photometric stereo registration Steps to performing a photometric stereo registration Customizing your photometric stereo context Remap factor Performing the photometric stereo registration operation and retrieving results Example of photometric stereo on non-moving objects Photometric stereo registration of a moving object High dynamic range registration Steps to performing a high dynamic range registration operation Input images Customizing your HDR operation Fusion Gain Tone mapping Example of HDR Troubleshooting M_HDR_STATUS value below 1.0 Image status of M_HDR_IMAGE_LIMITED_TO_SINGLE_COLOR Image status of M_HDR_INSUFFICIENT_CONTRAST Image status of M_HDR_INSUFFICIENT_CONTRIBUTION Image status of M_HDR_MERGE_AREA_GAIN_INCORRECT Image status of M_HDR_MERGE_AREA_SIZE_INSUFFICIENT Blocks appearing in the resulting HDR image Darker areas appearing flat with few details Resulting image is too dark or too bright ",
  "wordCount": 255,
  "subEntries": [
    {
      "id": "UG_registration_MIL_Registration_module",
      "version": null,
      "title": "MIL Registration module",
      "subTitles": [
        "Extended depth of field registration",
        "Depth-from-focus registration",
        "Photometric stereo registration"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\registration\\MIL_Registration_module.htm",
      "text": " MIL Registration module The MIL Registration module can perform four different types of registration operations. The first type is referred to as correlation-stitching registration, and can find the positional correlation between images in a common coordinate system. This information can be used for mosaicing and super-resolution. The second type, referred to as extended depth of field registration, or fusion, finds and uses the focused objects in images of the same scene to produce one image that combines all of the objects in focus. The third type is referred to as depth-from-focus registration; it computes an index map of a scene whereby each index value corresponds to a specific depth based on images of the same scene taken at different focus distances. The fourth type is referred to as photometric stereo registration; it uses multiple lighting orientations to enhance details, and detect defects from images of objects with reflective surfaces. This chapter discusses the last three types of registration operations. For information on performing a correlation-stitching registration or mosaicing operation, see Chapter 12: Correlation stitching registration and mosaicing. Extended depth of field operations, depth-from-focus operations, and photometric stereo operations require 1-band images; if images containing more than 1 band are used for these operations, an error occurs. The Registration module does not take into account the camera calibration of images. Extended depth of field registration A camera setup with a given depth of field does not always capture all the information in focus. You can instead take multiple images of the scene at different focus distances, and use the MIL Registration module to perform an extended depth of field registration to create a single image with the objects in focus. This simulates a camera setup with an extended depth of field, and can be very useful for overcoming camera and lens limitations. Microscopy, for example, is one area where the possible depth of field is very shallow. The images in an extended depth of field registration must overlap completely; they should be taken with exactly the same camera setup with different focus distances. Depth-from-focus registration When grabbing an image of a scene, you might need to perform 3D analysis of the objects grabbed in the scene. The MIL Registration module can generate an image that conveys depth information, from a set of two-dimensional images of the same scene taken at different focus distances. Essentially, each pixel in the resulting image corresponds to the index of the image in the set where the corresponding pixel appears most in focus. The resulting image is referred to as an index map and the operation is referred to as a depth-from-focus registration operation. The source images used in a depth-from-focus registration must overlap completely; they should be taken with exactly the same camera setup, with only the focus distance differing. Photometric stereo registration An image taken with a camera setup using a single light source does not always correctly capture all the details or defects on an object's surface. You can instead take multiple images of the same scene using different lighting orientations, and use the MIL Registration module to create a single enhanced image that reveals raised and recessed features, surface reflectance changes, and scratches or other flaws that are not apparent with an image taken with only one light source. Photometric stereo registration operations use material reflectance properties and the surface curvature of objects when calculating the resulting enhanced image. Operations available include computing the albedo image, Gaussian curvature image, mean curvature image, local shape image, local contrast image, and local texture image. The following image illustrates the process of photometric stereo registration. The images in a photometric stereo registration must overlap completely; they should be taken with exactly the same camera setup, with different lighting directions. MIL Registration module Extended depth of field registration Depth-from-focus registration Photometric stereo registration ",
      "wordCount": 639,
      "subEntries": []
    },
    {
      "id": "UG_registration_Basic_concepts",
      "version": null,
      "title": "Basic concepts for the MIL Registration module",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\registration\\Basic_concepts.htm",
      "text": " Basic concepts for the MIL Registration module The basic concepts and vocabulary conventions for the MIL Registration module are: Albedo image. An image that represents a measure of the reflectance of an object's surface. Confidence map. An image where the gray value of each pixel represents how reliable the index map is at that location. Depth of field. The tolerated distance from the focus distance of a camera setup in which objects are considered in focus. Index map. An image where the gray value of each pixel represents the index of an image, at which the pixel is most in focus. Intensity map. An image where the gray value of each pixel is the intensity of the corresponding pixel in the image where that pixel is found to be most in focus. Mosaicing. The process of combining many smaller images to form one larger image, using the results of the registration calculation. Registration. The registration of images involves the fusion of multiple images of a common scene to obtain some resulting image. Registration element. A data structure that stores the settings used to control the registration of a specific image. Registration elements are located in the registration context. Registration element's image. The image whose index in the image array is the same as the index of a particular registration element. The registration of an image is controlled by the contents of its associated registration element. Registration context. A MIL object that stores the registration information and each individual registration element. Basic concepts for the MIL Registration module ",
      "wordCount": 259,
      "subEntries": []
    },
    {
      "id": "UG_registration_Extending_your_depth_of_field",
      "version": null,
      "title": "Extending your depth of field",
      "subTitles": [
        "Steps to performing an extended depth of field registration",
        "Customizing your extended depth of field context",
        "Registration process",
        "Retrieving and using extended depth of field results",
        "Extended depth of field example"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\registration\\Extending_your_depth_of_field.htm",
      "text": " Extending your depth of field At a given focus distance, the entire image is not always in focus, resulting in blurry objects. Only objects within a tolerated distance from this focus distance are in focus. The tolerated distance is referred to as the depth of field, and it increases as the focus distance increases, assuming no other camera settings are changed. To emulate a larger depth of field, so that all objects of interest in the field of view are in focus, you can perform an extended depth of field (EDoF) registration operation using the MIL Registration module. This operation takes multiple images of the same scene, taken at different focus distances, and combines the focused objects in all of the images to create an image with all the objects of interest in focus. This process is also known as focus stacking. Extended depth of field registration is useful for applications where normal camera focusing techniques cannot capture the entire image in focus, such as microscopy or macro photography. The extended depth of field registration operation first preprocesses each input image. Considering the sharp portions in the input images, the operation computes the best combination for a fusion of the focused image sections. The above image shows two source images of the same scene, taken with different focus distances. The smiley face is in the foreground, and the lightning bolt is in the middle-ground. The source images each contain a portion of the scene in focus; after the extended depth of field registration operation, the smiley face and the lightning bolt are both in focus. Since the source images do not contain a focused version of the dark shape in the background, it cannot be resolved in the extended depth of field (EDoF) image. Steps to performing an extended depth of field registration The following steps provide a basic methodology for performing an extended depth of field operation on multiple images: Allocate an EDoF registration context, using MregAlloc() with M_EXTENDED_DEPTH_OF_FIELD. Allocate an EDoF registration result buffer to hold the results of the registration operation, using MregAllocResult() with M_EXTENDED_DEPTH_OF_FIELD_RESULT. If necessary, specify the control settings of your EDoF registration context using MregControl(). Set up an array with the identifiers of image buffers that hold the input images. All the images should be of the same scene, taken with all the same camera settings except focus, and in buffers of the same size, type, and number of bands. If necessary, allocate an image buffer to hold the EDoF image (alternatively, the EDoF image can be stored in the EDoF result buffer). The image buffer should be of the same size, type, and number of bands as the input images. Perform the extended depth of field operation, using MregCalculate(). If you want to perform the registration with all the default settings, skip steps 1 and 3 and use the M_DEFAULT_EXTENDED_DEPTH_OF_FIELD_CONTEXT context. If necessary, retrieve the required results from the result buffer using MregGetResult(). If you passed a result buffer to MregCalculate(), you can draw the EDoF image from the result buffer into an image buffer using MregDraw() with M_DRAW_EDOF_IMAGE. You can use MregGetResult() to establish the size and type of the image buffer required to draw the EDoF image. If necessary, save your registration context, using MregSave() or MregStream(). You can clear the result buffer to perform additional operations using MregCalculate() with M_RESET. Free your allocated registration objects, using MregFree(), unless M_UNIQUE_ID was specified during allocation. The following animation demonstrates how extended depth of field is achieved. This example presents a subset of the source images used to calculate the EDoF of a single object, and that of multiple objects in a scene. Each image is taken at a different focus distance, such that the focus is moved from the farthest point to the closest point in the scene. At each focus distance, a part of an object is in focus. In this case, the final extended depth of field image is calculated using MregCalculate() with M_DEFAULT_EXTENDED_DEPTH_OF_FIELD_CONTEXT and the source images. Note that even when the animation indicates that the bottle's label is in focus, the entire label is not completely in focus because the image is taken at an angle and the label is on a slanted surface of the object. Several images taken at different focus distances are required to have the bottle's label completely in focus. It is illustrated as \"in focus\" only for display purposes. Customizing your extended depth of field context You can customize an extended depth of field registration operation using MregControl(). If the default value for the maximum radius of the circle of confusion in your input images is incorrect, adjust it using MregControl() with M_CIRCLE_OF_CONFUSION_RADIUS_MAX. Also known as a blurring circle, a circle of confusion is the image representation of an out-of-focus point source of light. When the circle of confusion of a point source of light falls within a single pixel, the point source is considered in focus. A larger circle of confusion radius indicates that the point source is less in focus. The following image illustrates the circle of confusion of a point of an object as a source; in this case, the point is out of focus, since it occupies several pixels. Note that it is not drawn to scale. For MIL to optimally perform an extended depth of field operation, it must know the maximum radius of the circle of confusion among all input images. Overestimating or underestimating this value will not produce optimal results. In multiple images with varying focus distances, an out of focus object can appear to be at a different location than in an image where that object is in focus. If necessary, adjust the maximum translation distance between any two input images, in pixels, with the control type M_TRANSLATION_TOLERANCE. You can also reduce processing time by setting M_MODE to M_FAST, but this can potentially reduce the accuracy of the operation. Note that since the input images must be of the same scene, it is assumed that the images completely overlap. Consequently, you do not set rough locations with MregSetLocation(). Registration process Using MregCalculate(), each image is analyzed for focus, and the best parts of the image are taken into account with a weighted contribution. An image with a totally out of focus portion will contribute less of that portion to the final EDoF image, while the image with the best focus for that area of the image will contribute the most. The registration algorithm finds these portions by preprocessing the images before the EDoF image is computed. Depending on your needs, you can create the EDoF image right away or preprocess images using MregCalculate() as they are being grabbed and then, once all the images have been grabbed and preprocessed, call MregCalculate() to just compute the EDoF image. If all the images are available, you can pass them all at once to MregCalculate() with M_COMPUTE to create the extended depth of field (EDoF) image immediately. MregCalculate() can save the EDoF image in an image buffer; alternatively, it can save it in a result buffer, along with other preprocessing results. In the latter case, use MregDraw() with M_DRAW_EDOF_IMAGE to access the EDoF image. Note that with M_COMPUTE, MregCalculate() clears the result buffer before writing to it. If the images are not all available at once, you can preprocess them in stages by passing them as they become available to MregCalculate() with M_ACCUMULATE and passing the same result buffer. Using M_ACCUMULATE can reduce memory consumption when creating an extended depth of field image, since the source images don't have to be available all at once. When adding the last of the source images for an EDoF operation, call MregCalculate() with M_ACCUMULATE_AND_COMPUTE. M_ACCUMULATE_AND_COMPUTE preprocesses these additional images, and then the function computes the EDoF image using this new information and all of the information previously processed and stored in the result buffer. If you want to clear the result buffer of previous results, call MregCalculate() with M_RESET. The following diagram shows the ways to use MregCalculate() to perform an extended depth of field registration operation. Retrieving and using extended depth of field results After performing an extended depth of field registration operation, the EDoF image is stored either in a result buffer or an image buffer. If MregCalculate() used an image buffer as the destination, you can access the image immediately after the registration operation. You cannot access or draw other results of the operation (using MregGetResult() or MregDraw()) because only the EDoF image is saved in the buffer. If MregCalculate() used a result buffer as the destination, the EDoF image is stored in the result buffer along with some results and some information about the source images. You can use MregGetResult() with M_STATUS to check if the EDoF image is computed. A return value of M_ACCUMULATE means the result buffer contains preprocessing information, but the final EDoF image is not available. If MregCalculate() returns M_COMPLETE, the preprocessing information and the EDoF image are available. Many of the results that you can retrieve using MregGetResult() depend on their availability within the result buffer. For these result types, you should add the combination constant M_AVAILABLE when calling MregGetResult(). If you retrieve M_TRUE, the information is available and can be retrieved using another call to MregGetResult() without M_AVAILABLE. If the function returns M_FALSE, the requested information is not available, and calling MregGetResult() without M_AVAILABLE will cause an error. If you want to access the EDoF image stored in the result buffer, you must allocate an image buffer, and then, fill the image buffer with the EDoF image, using MregDraw() with M_DRAW_EDOF_IMAGE. To allocate a buffer of the appropriate size, you can retrieve the size and characteristics of the EDoF image using MregGetResult() with M_IMAGE_.... If you call MregDraw() with M_DRAW_EDOF_IMAGE and specify a destination image buffer that is too small, the EDoF image will be clipped to fit into the buffer. Extended depth of field example The extended depth of field example ExtendedDepthOfField.cpp illustrates how the operation can create a single image of a scene with all the objects in focus. extendeddepthoffield.cpp To run this example, use the Matrox Example Launcher in the MIL Control Center. Extending your depth of field Steps to performing an extended depth of field registration Customizing your extended depth of field context Registration process Retrieving and using extended depth of field results Extended depth of field example ",
      "wordCount": 1733,
      "subEntries": []
    },
    {
      "id": "UG_registration_Depth_from_focus",
      "version": null,
      "title": "Generating an index map from a scene",
      "subTitles": [
        "Steps to performing a depth-from-focus registration",
        "Customizing your depth-from-focus context",
        "Computing a depth-from-focus registration",
        "Retrieving and using depth-from-focus results",
        "Depth from focus example"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\registration\\Depth_from_focus.htm",
      "text": " Generating an index map from a scene From a set of two-dimensional images of the same scene taken at different focus distances, the MIL Registration module can generate an image that conveys depth information. Essentially, each pixel in the resulting image corresponds to the index of the image in the set where the corresponding pixel appears most in focus. The resulting image is referred to as an index map and the operation is referred to as a depth-from-focus registration operation. This operation uses multiple images of the same scene, taken at different focus distances, and calculates the index map, confidence map, and intensity map of the scene. The depth-from-focus registration operation works by considering the sharp portions in the input images. Then, for each pixel, it computes in which image that pixel is found most in focus. It replaces the corresponding pixel entry with the index of the image. You can also apply a regularization operation which ensures that neighboring index map pixels are coherent (smooth transitions between nearby pixels) in your resulting index map, if regularization is enabled. The above image shows a set of source images of the same scene, taken with different focus distances. The source images each contain a portion of the scene in focus; after the depth-from-focus registration operation, the index map for the object is generated based on the best focus images for the object. In a depth-from-focus registration operation, the order of the source images is important; the source images must either be in an order from longest to shortest focal length (or vice versa). In addition to generating the index map of a scene, you can also use a depth-from-focus registration operation to calculate a confidence map and/or intensity map of the scene. The confidence map indicates how reliable the index map is for a particular pixel; the higher the grayscale value of a pixel in the confidence map, the more confident you can be that the corresponding pixel in the index map is correct. The intensity map represents the scene in focus; each pixel in the intensity map is set to the intensity of the corresponding pixel in the image where that pixel is found to be most in focus. The following image demonstrates the confidence map and intensity map generated from a set of source images using a depth-from-focus registration operation. Steps to performing a depth-from-focus registration The following steps provide a basic methodology for performing a depth-from-focus operation on multiple images: Allocate a depth-from-focus registration context, using MregAlloc() with M_DEPTH_FROM_FOCUS. If necessary, allocate a depth-from-focus registration result buffer to store the results of the registration operation, using MregAllocResult() with M_DEPTH_FROM_FOCUS_RESULT. You don't need to allocate a result buffer if you intend on passing an image buffer to MregCalculate(). If necessary, specify the control settings for your depth-from-focus registration operation using MregControl(). Set up an array with the identifiers of image buffers that hold the input images. All the images should be of the same scene, all taken with the same camera settings except focus, and in buffers of the same size, type, and with a single band. If necessary, allocate an image buffer to hold the depth-from-focus index map image (alternatively, the index map image can be stored in the depth-from-focus result buffer). The image buffer should be of the same size, type, and number of bands as the input images. Perform the depth-from-focus operation, using MregCalculate(). You can pass and process the images in batches using MregCalculate() with M_ACCUMULATE_AND_COMPUTE, or all at once using M_COMPUTE. You can filter the index map using MimFilterMajority() to remove noise while preserving valid index values. Note, this does not preserve edges. If you passed a result buffer to MregCalculate(), retrieve the required results from the result buffer, using MregGetResult(). If you passed a result buffer to MregCalculate(), you can draw the index map image from the result buffer into an image buffer using MregDraw() with M_DRAW_DEPTH_INDEX_MAP. You can use MregGetResult() to establish the size and type of the image buffer required to draw the depth-from-focus index map image. In addition, you can use MregDraw() with M_DRAW_DEPTH_CONFIDENCE_MAP to draw the depth confidence map, or with M_DRAW_DEPTH_INTENSITY_MAP to draw the depth intensity map, if those settings were previously enabled using MregControl(). If necessary, save your registration context, using MregSave() or MregStream(). You can clear the result buffer to perform additional operations, using MregCalculate() with M_RESET. Free your allocated registration objects, using MregFree(), unless M_UNIQUE_ID was specified during allocation. When using MregCalculate() with M_ACCUMULATE_AND_COMPUTE, you can repeat steps 6 through 8 as many times as are required. Customizing your depth-from-focus context You can customize a depth-from-focus registration operation using MregControl(). The coherency of neighboring pixels in an index map can be important, to ensure the accuracy of the generated index map. You can apply a regularization operation to ensure that neighboring index map pixels are coherent. To do so, you must enable regularization and select the mode in which it should be performed using MregControl() with M_REGULARIZATION_MODE. The regularization mode controls how the registration operation treats the smoothing of pixels from the source images, when generating the index map, in terms of their neighbors. By default, the registration operation will establish pixels in the index map using the average dominance of the neighboring pixels. You can also use an adaptive smoothing regularization mode. This mode uses the local geometry of the content of the images. In this case, you must use MregControl() with M_ADAPTIVE_INTENSITY_DELTA to identify the maximum variation in pixel values for objects in the image. You must also use MregControl() to specify an M_ADAPTIVE_SMOOTHING value, to control the threshold used to perform the smoothing. You can use MregControl() to set M_REGULARIZATION_MODE to M_DISABLE to disable regularization, and use the raw index map values. The following image illustrates the different regularization modes that can be used to generate index maps. The specified regularization mode affects the processing time of the depth-from-focus registration operation. The M_ADAPTIVE and M_AVERAGE regularization modes use the values of neighboring pixels to compute the new value of a given pixel in the index map. The efficiency of these modes depends on the M_REGULARIZATION_SIZE control type, which controls the size of the neighborhood over which regularization takes place. The efficiency of a depth-from-focus registration operation also depends on the M_FOCUS_DEPTH_SIZE control type, which specifies the range of images in which an object in the image is in focus; that is, the number of images in which an object in the images goes from out of focus, to in focus, to out of focus again. A larger focus depth size indicates a more precise operation, and a longer processing time. You can reduce processing time by disabling regularization (M_REGULARIZATION_MODE set to M_DISABLE); however, this can potentially reduce the accuracy of the operation, since nothing will be done to ensure coherence between neighboring pixels in the index map. Note that since the input images must be of the same scene, it is assumed that the images completely overlap. Consequently, you do not set rough locations with MregSetLocation(). Computing a depth-from-focus registration To calculate the index map from images of a scene at different focus depths, use MregCalculate(). The calculation can be done in one operation, or in stages; regardless of the method of calculation, an index map is generated after each call to MregCalculate(). To calculate the index map in stages, call MregCalculate() with M_ACCUMULATE_AND_COMPUTE one or more times, passing a new array of images in each call. In this case, M_ACCUMULATE_AND_COMPUTE computes the depth-from-focus index map using this new information and all of the information previously processed and stored in the depth-from-focus registration result buffer. This kind of operation is useful if you are grabbing and don't have all the source images available at once. To calculate the index map in one operation, call MregCalculate() with M_COMPUTE, and pass the array of images required to generate the index map. Subsequent calls to MregCalculate() with M_COMPUTE will clear the depth-from-focus registration result buffer before storing new results. When generating the index map in stages, you can only store the results in a depth-from-focus registration result buffer; however, when generating the index map in one operation, you can store the index map directly in an image buffer or in a result buffer. When results are stored in a depth-from-focus result buffer, the index map can be drawn into an image buffer using MregDraw() with M_DRAW_DEPTH_INDEX_MAP. Whenever you draw the index map into an image, care must be taken so that the image in which the index map is drawn, is big enough to hold all index values; for instances, an 8-bit image cannot be used to store the index map when the depth-from-focus operation uses 300 images, since each source image requires a unique index in the index map. In addition to generating an index map for a set of images of a scene, MregCalculate() can also generate a confidence map and the intensity map for that set of images. Use MregControl() with M_CONFIDENCE_MAP or M_INTENSITY_MAP to control whether these maps will be computed. When you are computing the confidence map or intensity map for a scene, you must use a depth-from-focus registration result buffer to store the results. You can then use MregDraw() to retrieve the resulting confidence map or intensity map image. To do so, you can use MregDraw() with M_DRAW_DEPTH_CONFIDENCE_MAP or M_DRAW_DEPTH_INTENSITY_MAP, respectively, and the identifier of an image buffer. The following diagram shows the ways to use MregCalculate() to perform a depth-from-focus registration operation. Retrieving and using depth-from-focus results If MregCalculate() used a result buffer as the destination, the index map image is stored in the result buffer along with some results and some information about the source images. You can use MregGetResult() with M_STATUS to check if the index map image is computed. If MregGetResult() returns M_COMPLETE, the preprocessing information and the index map image are available. Many of the results that you can retrieve using MregGetResult() depend on their availability within the result buffer. For these result types, you should add the combination constant M_AVAILABLE when calling MregGetResult(). If you retrieve M_TRUE, the information is available and can be retrieved using another call to MregGetResult() without M_AVAILABLE. If the function returns M_FALSE, the requested information is not available, and calling MregGetResult() without M_AVAILABLE will cause an error. You can use MimFilterMajority() to filter noise in your index map while preserving valid index values. The following image shows how MimFilterMajority() filters indices that cover very small areas on an image. Depth from focus example The depth from focus example DepthFromFocus.cpp illustrates how the operation can be used to combine multiple images taken at different focus distances to obtain a resulting index map. depthfromfocus.cpp To run this example, use the Matrox Example Launcher in the MIL Control Center. Generating an index map from a scene Steps to performing a depth-from-focus registration Customizing your depth-from-focus context Computing a depth-from-focus registration Retrieving and using depth-from-focus results Depth from focus example ",
      "wordCount": 1830,
      "subEntries": []
    },
    {
      "id": "UG_registration_Photometric_Stereo",
      "version": null,
      "title": "Surface feature enhancement and defect detection using photometric stereo registration",
      "subTitles": [
        "Steps to performing a photometric stereo registration",
        "Customizing your photometric stereo context",
        "Remap factor",
        "Performing the photometric stereo registration operation and retrieving results",
        "Example of photometric stereo on non-moving objects",
        "Photometric stereo registration of a moving object"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\registration\\Photometric_Stereo.htm",
      "text": " Surface feature enhancement and defect detection using photometric stereo registration With a given lighting orientation, all the details on an object's surface in an image might not be detectable. To detect these characteristics, you can perform a photometric stereo registration operation using the MIL Registration module. This operation takes multiple images of the same scene taken under different lighting orientations. It is assumed that the camera does not move and no other camera settings are changed while grabbing the series of images. The images are used together to create a single composite image. This single image reveals raised and recessed features and surface reflectance changes that are not apparent with an image taken with only one light source. Photometric stereo registration operations use material reflectance properties and the surface curvature of objects when calculating the resulting enhanced image. Operations available include computing the albedo image, Gaussian curvature image, mean curvature image, local shape image, local contrast image, and local texture image. The following animation illustrates the process of photometric stereo registration, resulting in an enhanced image. You can use four or more lights in the photometric stereo setup. Note that when setting up for a local shape image operation, while each input image uses one light source, your lights must be set up in opposite pairs. That is, for each light in the setup, there should be another light positioned 180 degrees opposite. You can also perform photometric stereo registration on a moving object (for example, on a conveyor belt). In this case, the source images must be aligned before the photometric stereo registration can take place. For more information, see the Photometric stereo registration of a moving object subsection of this section. Steps to performing a photometric stereo registration The following steps provide a basic methodology for performing a photometric stereo operation using multiple images: Allocate a photometric stereo registration context, using MregAlloc() with M_PHOTOMETRIC_STEREO. Optionally, allocate a photometric stereo registration result buffer to hold the results of the registration operation, using MregAllocResult() with M_PHOTOMETRIC_STEREO_RESULT. If you are only interested in one of the resulting photometric stereo images, you don't need a result buffer. If necessary, allocate an image buffer to hold the resulting image (alternatively, the image can be stored in the result buffer). The image buffer should be of the same size, type, and number of bands as the input images. Set the type of photometric stereo image(s) to calculate, using MregControl(), depending on whether a result buffer or image buffer will hold the resulting photometric stereo image(s): For a result buffer, you can enable one or more types of photometric stereo images for calculation using their dedicated control types. For example, enable a local shape operation using MregControl() with M_LOCAL_SHAPE and M_ENABLE. For an image buffer, specify the photometric stereo image(s) to calculate, using MregControl() with M_DRAW_WITH_NO_RESULT and the appropriate control value (for example, M_DRAW_LOCAL_SHAPE_IMAGE for a local shape image). Specify the position of the light used for each input image using MregControl() with M_LIGHT_VECTOR_COMPONENT_1, M_LIGHT_VECTOR_COMPONENT_2, and M_LIGHT_VECTOR_COMPONENT_3. You must specify your lights in a counter-clockwise order, starting at the positive X-axis. To illustrate, if you have four lights spaced evenly around a scene, and your first light is placed in line with the X-axis (zero degrees), lights 2, 3, and 4 are placed at 90, 180, and 270 degrees, respectively, and must be specified in the same order. Specify any additional control settings (for example, M_SHAPE_SMOOTHNESS for a local shape image) using MregControl(). If necessary, specify remap factor settings. The remap factor applies when writing results to a result buffer for a local shape, Gaussian curvature, or mean curvature operation. Set up an array with the identifiers of image buffers that hold the input images. All the images should be of the same scene, taken with all the same camera settings except lighting, and in buffers of the same size, type, and number of bands. Perform the photometric stereo operation, using MregCalculate() with M_COMPUTE, the input image array, and either a destination image or a result buffer. If necessary, retrieve the required results from the result buffer using MregGetResult(). If you passed a result buffer to MregCalculate(), you can draw the photometric stereo image(s) from the result buffer into an image buffer using MregDraw(). To draw any resulting image, you must have previously enabled the calculation of that image using MregControl() as discussed above. Note that you do not have to enable an albedo image; it is always available to draw from the result buffer after MregCalculate() is called. Drawable images include but are not limited to a local shape image (M_DRAW_LOCAL_SHAPE_IMAGE) and a Gaussian curvature image (M_DRAW_GAUSSIAN_CURVATURE_IMAGE). Use MregGetResult() to establish the size and type of the image buffer required to draw each of the resulting images. If necessary, save your registration context, using MregSave() or MregStream(). Free your allocated registration objects, using MregFree(), unless M_UNIQUE_ID was specified during allocation. Customizing your photometric stereo context To perform a photometric stereo registration operation, you must first specify the number of registration elements using MregControl() with M_NUMBER_OF_REGISTRATION_ELEMENTS. Each registration element holds information related to the position and relative intensity of the light source, for its associated image. Each image in the image array passed to MregCalculate() is associated with the registration element of the same array index. The number of registration elements also sets the maximum number of light sources. You can specify the position of the light source using the spherical coordinate system (default) or the Cartesian coordinate system, using MregControl() with M_LIGHT_VECTOR_TYPE. The spherical coordinate system defines orientations with 3 values: the polar (zenith) angle (angle between a light and the camera), the azimuth angle (angle of the light vector projected onto the XY-plane, measured from the positive X-axis), and the relative light intensity value. The Cartesian coordinate system specifies that the light vector is defined with Cartesian coordinates (X,Y, and Z), with the axes oriented to follow the right-hand rule (see the Coordinate systems section of Chapter 28: Calibrating your camera setup). Note that, when setting the coordinates for either system, the origin is located at the center of the scene, directly below the camera (assuming the camera is perpendicular to the scene). Typically, spherical coordinates are used to specify lighting positions. Use MregControl() to set the vector components for each light source with M_LIGHT_VECTOR_COMPONENT_1 for the polar (zenith) angle, M_LIGHT_VECTOR_COMPONENT_2 for the azimuth angle, and M_LIGHT_VECTOR_COMPONENT_3 for the relative light intensity, which is typically set to 1.0. If relative light intensity is set to 1.0 for all light sources, each light provides the same intensity to the setup, which is the ideal arrangement. If equal light intensities are not possible, set the strongest light to 1.0 and set the other light intensity values relative to this. The primary axis, used to specify the coordinates of the location of the light sources, is the vector that goes from the camera to the image. It is typical for the camera to be perpendicular to the object's surface for this application. Remap factor For consistent results when performing a photometric stereo operation based on shape (such as a local shape, Gaussian curvature, or mean curvature operation), you can set a remap factor. Shape-based operations are calculated using normal vectors whose magnitudes can vary widely depending on image content. Internal calculations are done in floating-point. However, to generate the photometric stereo image with the same depth as the source images, the calculated values must be mapped or scaled to integer intensity values. By default, a remap factor is automatically calculated (M_DRAW_REMAP_FACTOR_MODE set to M_DEFAULT or M_AUTO), which remaps values using the full range of values in the current image. However, with some objects and lighting, it is possible for some (but not all) source images to have peak values that are much bigger than the others in the set (for example, a random, very sharp reflection), which causes that image to use a very different remap factor, possibly diminishing important details in key parts of the image. To avoid the effect of occasional sharp transitions, you can set a constant factor to use for the remapping (M_DRAW_REMAP_FACTOR_MODE set to M_USER_DEFINED). This is useful when you are performing several shape-based operations and want to ensure that the stored results are directly comparable. Before setting the remap factor (M_DRAW_REMAP_FACTOR_VALUE), it is recommended to find the reported automatic mode factor (using MregGetResult() with M_RANGE_FACTOR_LOCAL_SHAPE, M_RANGE_FACTOR_GAUSSIAN_CURVATURE or M_RANGE_FACTOR_MEAN_CURVATURE) for some typical images. If the resulting output is satisfactory, set M_DRAW_REMAP_FACTOR_VALUE to the automatically calculated value. If not, you can try setting the remap factor to a higher value; this increases image contrast and helps to bring out smaller details in the photometric stereo image. Note that you can only set the remap factor when writing results to a result buffer. Performing the photometric stereo registration operation and retrieving results To perform the photometric stereo registration operation, call MregCalculate() with M_COMPUTE, and pass the array of input images required to generate the photometric stereo image(s) and either a destination image buffer or a photometric stereo result buffer. Subsequent calls to MregCalculate() with M_COMPUTE will clear the photometric stereo registration result buffer before storing new results if a result buffer is specified. You can specify the photometric stereo image(s) to compute using MregControl(). These images are: local shape image, albedo image (the default setting), Gaussian curvature image, mean curvature image, local contrast image, and texture image. Each computed photometric stereo image highlights different features or surface properties of the source scene. The following are descriptions of each operation. The related images show a source image on the left and the photometric stereo result (calculated from multiple source images) on the right. A local shape image operation (M_LOCAL_SHAPE) captures raised and recessed features on a surface. When there are 4 input images and the azimuth angles of their light sources are 0, 90, 180, and 270, respectively, you can use M_IMPROVED_LOCAL_SHAPE, which computes an improved version of the local shape image. Note that M_LOCAL_SHAPE must be enabled to use M_IMPROVED_LOCAL_SHAPE. The operation is fast with minimal noise in the resulting image. Embossed text often shows up well, which can facilitate MIL SureDotOCR and String Reader operations. Use M_SHAPE_SMOOTHNESS to control shape smoothness in the photometric stereo result. Note how the raised and recessed surfaces on the object are revealed with the local shape operation. If opposite-paired lighting is not available for a local shape operation, a mean curvature operation can give similar, and sometimes better, results. An albedo image operation (M_DRAW_ALBEDO_IMAGE) highlights changes in surface reflectance, and is useful for finding a defect that reveals an underlying material that reflects differently than the surface material. Any change in surface reflectance, such as from shiny to dull, should be apparent in an albedo image result. Note the scratch revealed by the albedo image operation. A Gaussian curvature image operation (M_GAUSSIAN_CURVATURE) reveals sharp curvature changes, such as the corner of an object or a puncture in an otherwise flat surface. This operation will not pick up fine curvature detail. Also, if a surface contains flat printed areas, such as text or a logo, M_GAUSSIAN_CURVATURE ignores those differences, since a change in curvature is what matters. Use M_SHAPE_SMOOTHNESS to control shape smoothness in the photometric stereo result. Note how the printed text no longer obscures the defect, which is a large change in the surface curvature. A Gaussian curvature operation will not detect subtle surface curvature differences, such as the paper texture in the source image. The mean curvature image operation (M_MEAN_CURVATURE) can reveal small (local) curvature changes, because the curvature is calculated at every pixel, resulting in more local information. Fine scratches or very small features on a surface are revealed. Use M_SHAPE_SMOOTHNESS to control shape smoothness in the photometric stereo result. The operation reveals the defect while also showing finer surface features (the paper texture above). As with the Gaussian curvature operation, a mean curvature result is not sensitive to printed text. Note that the above result image was obtained using opposite-paired lighting, which is not required for the mean curvature operation, but is required for local shape results. Therefore, the mean curvature can substitute for local shape when opposite-paired lighting is not achievable. The local contrast image operation (M_LOCAL_CONTRAST) reveals surface height variations by increasing the contrast between highlights and shadows. A local contrast operation can remove printed text on flat surface areas in the resulting enhanced image, provided source image illumination is even (equal relative intensities across the light sources). This operation performs morphological operations using M_OBJECT_SIZE to control the number of iterations to perform. The number of iterations to perform is related to the feature size of the object(s) in the scene. For larger features, more iterations might be needed. The default is 1 iteration, but it is recommended to experiment to find the best setting for your needs. Using the same source images as the Gaussian curvature operation, a local contrast operation reveals the defect and removes or reduces the printed text, while still showing surface texture. A texture image operation (M_TEXTURE_IMAGE) will show printing on a surface, since surface luminance is targeted. This operation can remove spectral reflections, such as those from plastic wrapping. For example, the printed address on a magazine wrapped in plastic becomes readable because the spectral reflections, which otherwise obscure the printing, are removed. The texture image operation has removed spectral reflections, rendering the printed text and bar code readable in the photometric stereo result. If MregCalculate() used an image buffer as the destination, you can access the resulting image immediately after the registration operation. If MregCalculate() used a result buffer as the destination, the result buffer holds the photometric stereo image(s) along with other results. You can draw the photometric stereo image(s) and/or other results into an image buffer using MregDraw(). Use MregGetResult() with M_STATUS to check if the photometric stereo image(s) is/are computed. Example of photometric stereo on non-moving objects The animations below illustrate what happens in the subsequent example: how surface details can be extracted with the photometric stereo registration operation. The first animation shows an M_LOCAL_SHAPE operation, revealing the raised and recessed features of the source object. The subsequent animation shows an M_DRAW_ALBEDO_IMAGE operation that detects flaws on a leather surface. The example PhotometricStereo.cpp demonstrates several photometric stereo use cases. photometricstereo.cpp Photometric stereo registration of a moving object Typically, the scene or object undergoing photometric stereo registration is stationary. However, photometric stereo registration is possible with images taken from a moving object (for example, on a conveyor). In the case of a moving object, the source images must be aligned before you can perform the photometric stereo registration operation. To help calculate the object's displacement, take a start and an end image with all lights on. These two images serve as anchors for locating the object in the intermediate images, which are the photometric stereo source images taken with directional lighting. Once you have acquired the start, end, and photometric stereo source images, apply the following: Use the Pattern Matching or Model Finder module to define a model from the start image. Find the model in the end image. Using the model's location in the start and end images, calculate the model's displacement per photometric stereo image. Note that the object is assumed to have moved at a constant speed and in a straight line. Translate all the photometric stereo source images such that the model is positioned in the same location for each image. Perform the photometric stereo calculation. The key step for a successful photometric stereo operation on a moving object is source image alignment. Using the above method, the defined model must be visible in the start and end images. In addition, the object's displacement must be small, relative to the lighting setup, so that distinct directional lighting is maintained between each image. A small displacement also minimizes perspective distortion and parallax errors. Your application could require a different methodology (for example, to account for source object rotation). For an illustration of photometric stereo registration on a moving object, see the following example. photometricstereowithmotion.cpp Surface feature enhancement and defect detection using photometric stereo registration Steps to performing a photometric stereo registration Customizing your photometric stereo context Remap factor Performing the photometric stereo registration operation and retrieving results Example of photometric stereo on non-moving objects Photometric stereo registration of a moving object ",
      "wordCount": 2725,
      "subEntries": []
    },
    {
      "id": "UG_registration_High_dynamic_range",
      "version": null,
      "title": "High dynamic range registration",
      "subTitles": [
        "Steps to performing a high dynamic range registration operation",
        "Input images",
        "Customizing your HDR operation",
        "Fusion",
        "Gain",
        "Tone mapping",
        "Example of HDR",
        "Troubleshooting",
        "M_HDR_STATUS value below 1.0",
        "Image status of M_HDR_IMAGE_LIMITED_TO_SINGLE_COLOR",
        "Image status of M_HDR_INSUFFICIENT_CONTRAST",
        "Image status of M_HDR_INSUFFICIENT_CONTRIBUTION",
        "Image status of M_HDR_MERGE_AREA_GAIN_INCORRECT",
        "Image status of M_HDR_MERGE_AREA_SIZE_INSUFFICIENT",
        "Blocks appearing in the resulting HDR image",
        "Darker areas appearing flat with few details",
        "Resulting image is too dark or too bright"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\registration\\High_dynamic_range.htm",
      "text": " High dynamic range registration The MIL Registration module offers high dynamic range (HDR) registration to improve detail in images. HDR registration merges two or more grayscale images of the same subject that were taken with different exposure times, resulting in different levels of saturation. Longer exposure times produce brighter images in which details in dark areas appear, but brighter areas become saturated and lose detail. Similarly, shorter exposure times produce darker images in which details in bright areas appear, but darker areas lose detail. When the images are fused together, the resulting HDR image will show details in both dark areas and bright areas. Steps to performing a high dynamic range registration operation The following steps provide a basic methodology for using the Registration module to perform an HDR registration operation. Allocate a high dynamic range registration context, using MregAlloc() with M_HIGH_DYNAMIC_RANGE. Optionally, allocate a registration result buffer to hold the results of the registration operation, using MregAllocResult() with M_HIGH_DYNAMIC_RANGE_RESULT. If you are only interested in obtaining an HDR image from one set of input images contained in a single input image array, you do not need a result buffer. If necessary, allocate an image buffer to hold the resulting HDR image (alternatively, the image can be stored in the result buffer). The image buffer should be 1-band and of the same size and type as the input images; the recommended bit depth is twice that of the input images. Set up an array with the identifiers of image buffers that hold the input images. All the images should be of the same scene, arranged from longest exposure (first index) to shortest exposure (last index), in 1-band buffers of the same size and type. Verify that the number of registration elements is sufficient for your application (by default, it is 16). A registration element contains the information required to register an image; therefore, the context must contain at least the same number of registration elements as images that need to be registered (per call to MregCalculate()). You can inquire the number using MregInquire() with M_NUMBER_OF_REGISTRATION_ELEMENTS. To change the number of registration elements, use MregControl() with M_NUMBER_OF_REGISTRATION_ELEMENTS. The minimum is 2. Specify the control settings for your HDR registration operation using MregControl() (for example, the fusion settings control the criteria by which images and pixels are included in the final HDR image calculation). Perform the registration using MregCalculate() with the HDR registration context, the array of input images, a destination image or result buffer, the number of images in the image array, and the processing mode (M_COMPUTE, M_ACCUMULATE, or M_ACCUMULATE_AND_COMPUTE). If you passed a result buffer to MregCalculate(), retrieve the required results from the result buffer, using MregGetResult(). If you passed a result buffer to MregCalculate(), you can draw the HDR image from the result buffer into an image buffer using MregDraw() with M_DRAW_HDR_IMAGE. You can use MregGetResult() to establish the size and type of the image buffer required to draw the HDR image. If necessary, save your registration context using MregSave() or MregStream(). You can clear the result buffer to perform additional operations, using MregCalculate() with M_RESET. Free all your allocated objects, using MregFree(), unless M_UNIQUE_ID was specified during allocation. Input images Input images must be of the same scene, type, and resolution. Additionally, they must be 1-band 8- or 16-bit unsigned images. The minimum number of input images is 2. The order of input images must go from the image with the longest exposure (the brightest image with details in dark areas) to the image with the shortest exposure (the darkest image with details in bright areas). If you want to calculate the HDR image in one call to MregCalculate() using provided input images, set M_COMPUTE as the processing mode. In this case, you can have MregCalculate() write the resulting HDR image directly in the specified image buffer. However, if it would be too time consuming to wait for all required input images, you can call MregCalculate() multiple times with M_ACCUMULATE and the available set of input images. Then, for the last set of images to merge into the same working HDR image, call MregCalculate() with M_ACCUMULATE_AND_COMPUTE. The last call will compute the resulting HDR image from all previously accumulated images. For both M_ACCUMULATE and M_ACCUMULATE_AND_COMPUTE, you must use a result buffer instead of an image buffer. If you are using multiple calls, the Nth input image used to generate the current HDR image will use the settings of the registration element at index N-1. For example, if you are merging your eighth image, the settings of the registration element at index 7 will be used. Note that if the difference between the exposures of two consecutive input images is too large, subsequent input images will fail until the end of the series is reached. This is because the common merge area between the second image and the working HDR image will be too small, and since all subsequent images will have increasingly shorter exposures, the difference of exposure will only become greater, preventing a sufficiently large merge area from ever being established. In such cases, the resulting HDR image will consist of the fusion of only the images preceding the exposure gap. Customizing your HDR operation You can customize your high dynamic range registration operation using MregControl(). The animation below illustrates the HDR registration process. The first input image becomes the working HDR image; no control settings are applied to this first image. Subsequent images, including the working HDR image, are evaluated using the limits set with M_FUSION_LOW_THRESHOLD and M_FUSION_HIGH_THRESHOLD. For each input image, M_FUSION_COVERAGE is used to determine whether the input image has enough pixels in the common merge area to be included in the HDR calculation. If the image meets this requirement, the image merges with the working HDR image and this new image becomes the working HDR image. The process will then repeat until the last image is reached, after which point tone mapping is applied. Play speed: Default (x1.0) Quarter (x0.25) Half (x0.5) Double (x2.0) Start position: Current image First image Looping: Continuous play Single iteration 1 of 16 Fusion To set the criteria that determines which pixels will be included in the HDR image based on their saturation levels, use M_FUSION_LOW_THRESHOLD and M_FUSION_HIGH_THRESHOLD. These thresholds are set as a percentage of the total number of pixels in the working HDR image; MIL will internally convert them to a pixel value threshold. In the case of M_FUSION_LOW_THRESHOLD, which eliminates underexposed pixels in the input images, this pixel threshold is the highest pixel value that excludes at most the percentage of pixels set by M_FUSION_LOW_THRESHOLD. All pixels with values below this threshold will be excluded. Conversely, in the case of M_FUSION_HIGH_THRESHOLD, which eliminates overexposed (saturated) pixels, this pixel threshold is the lowest pixel value that includes at least the percentage of pixels set by M_FUSION_HIGH_THRESHOLD. All pixels with values above this threshold will be excluded. For example, if M_FUSION_HIGH_THRESHOLD is set to 95%, then MIL will internally select the lowest pixel value that allows for at least 95% of pixels in the image to be included (that is, at most 5% of the highest pixels in the image to be excluded). M_FUSION_LOW_THRESHOLD and M_FUSION_HIGH_THRESHOLD are not applied to the first input image; it becomes the working HDR image. The thresholds will only be applied to subsequent input images and working HDR images. Once the underexposed and overexposed pixels have been excluded from both the working HDR image and the input image, the pixels that remain and are common to both images will form the common merge area. Only pixels in the common merge area will be merged with the working HDR image. The input image will be included in the calculation of the HDR image if there are enough pixels in the common merge area established by M_FUSION_LOW_THRESHOLD and M_FUSION_HIGH_THRESHOLD. This requirement is met if the ratio between the pixels in the common merge area and the total number of pixels is at least the value of M_FUSION_COVERAGE. You can adjust this requirement to be more restrictive by setting a higher value for M_FUSION_COVERAGE. Note that this could result in less input images being included in the HDR calculation, since those which do not contribute enough will be excluded. Gain Once the common merge area has been established and the input image is almost ready to be fused with the working HDR image, gain is applied to the input image. By default, the gain for each input image is calculated automatically. To retrieve the automatically calculated gain used for a specific input image when M_GAIN_MODE is set to M_AUTO, use MregGetResult() with M_IMAGE_GAIN. To manually set the gain, use MregControl() with M_GAIN_MODE set to M_USER_DEFINED. Then, you can set the gain for a specific input image by setting its corresponding registration element in the registration context, using MregControl() with M_IMAGE_GAIN. Tone mapping The resulting raw HDR image has twice the bit depth of the input images. By default, MIL applies tone mapping to the resulting raw HDR image to reduce its bit depth to that of the original input images. You can disable tone mapping using M_TONE_MAPPING_MODE, or you can draw the resulting HDR image without tone mapping using MregDraw() with M_DRAW_HDR_FULL_IMAGE. Tone mapping distributes pixel intensities non-linearly within a specified range, typically to maintain the tonal details of the resulting raw HDR image when it is being mapped to a lower bit depth and/or for display on a monitor. To modify how tone mapping is applied, use M_TONE_MAPPING_COEFFICIENT, M_TONE_MAPPING_HIGH_THRESHOLD, and M_TONE_MAPPING_LOW_THRESHOLD. To prioritize maintaining details in dark or bright areas, adjust the value of M_TONE_MAPPING_COEFFICIENT. Increasing the value towards 1 will give bright areas a broader intensity range, which improves the detail in bright areas and lowers the overall brightness of the final image. Conversely, decreasing the value towards 0 will give dark areas a broader intensity range, which improves the detail in dark areas and increases the overall brightness of the final image. To improve the intensity distribution of the image, adjust M_TONE_MAPPING_HIGH_THRESHOLD and M_TONE_MAPPING_LOW_THRESHOLD, which will filter out the most saturated and the least saturated pixels, respectively. This allows more intensity distribution elsewhere in the image. The thresholds are set as a percentage of the resulting raw HDR image's pixel intensity levels. For example, if M_TONE_MAPPING_HIGH_THRESHOLD is set to 90%, pixels in the brightest 10% of intensities will be excluded. Note that M_TONE_MAPPING_LOW_THRESHOLD and M_TONE_MAPPING_HIGH_THRESHOLD should only be used to remove unneeded pixel intensity levels and provide a better intensity resolution to the final HDR image. Example of HDR The example SimpleHDR.cpp demonstrates a use of HDR registration in MIL. simplehdr.cpp To run this example, use the Matrox Example Launcher in the MIL Control Center. Troubleshooting The following subsections describe some common issues encountered when performing an HDR registration operation and how to solve them. Use MregGetResult() with M_HDR_STATUS to get the status of the entire HDR registration operation and use MregGetResult() with M_HDR_IMAGE_STATUS to get the status of the merge of a specific image. Note that M_HDR_IMAGE_STATUS reports only one issue (for example, M_HDR_INSUFFICIENT_CONTRAST) per image, even if the criteria for multiple are met. M_HDR_STATUS value below 1.0 The value of M_HDR_STATUS represents the ratio between the number of input images that were successfully processed and the total number of input images. When this value is below 1.0, it means that some images were not successfully processed. This can happen for multiple reasons: The input images were not in the correct order. The input images must be added in the order of darkest details (brightest image) to brightest details (darkest image). Otherwise, the resulting HDR image will only consist of the first image. There are several ways to solve this: Use decreasing exposure times, with the first image having the longest exposure time and the last image having the shortest exposure time. This approach is limited by the time available to take the necessary images. Use a different aperture for each image, starting from the largest aperture (smallest number) and ending with the smallest (largest number). Note that this approach will cause variations in the focal length and might cause the resulting HDR image to be blurry. Use a different ISO setting for each image, starting from the most sensitive (largest number) and ending with the least sensitive (smallest number). Note that images taken with high ISO settings might have more noise, which will result in an HDR image with more noise. The level of visible detail is not consistent from one input image to the next. For example, if you have a series of images of a checkerboard in black and white, the images will show a strong level of detail for blacks and whites, but will have little detail in the shades of gray in between. This can cause problems when merging the middle images. To solve this, add an object with varying tones of gray in the scene to be photographed. This will provide a reference and allow every input image to meet the minimum level of detail to contribute. The difference between the exposure of each image is too small, resulting in many similar-looking images. To solve this, increase the difference of exposure between each image. Note that in this case, a less than perfect M_HDR_STATUS value can sometimes be acceptable, as details will not be missing. It becomes a problem when multiple adjacent images have been rejected. The difference between the exposure of two consecutive input images is too large, resulting in a common merge area that is too small between the second image and the working HDR image. This causes the subsequent input images to fail until the end of the series is reached. Since the order of images is from darkest details to brightest details, the resulting HDR image will have fewer details in the brightest areas. To solve this, decrease the difference of exposure between consecutive images. M_FUSION_LOW_THRESHOLD and M_FUSION_HIGH_THRESHOLD are too extreme and are causing a loss of detail. These values are meant to eliminate only the most saturated pixels (using M_FUSION_HIGH_THRESHOLD) and underexposed pixels (using M_FUSION_LOW_THRESHOLD). To solve this, decrease the value of M_FUSION_LOW_THRESHOLD or increase the value of M_FUSION_HIGH_THRESHOLD. M_FUSION_COVERAGE is too high. This means that the inclusion criteria is too restrictive and not enough images are being included in the resulting HDR image. To solve this, set M_FUSION_COVERAGE to the lowest value that yields acceptable improvement in the detail level of the HDR image. The first input image is too saturated. When this happens, a common merge area between the first and second image cannot be established, and therefore the HDR fusion operation cannot proceed. To solve this, decrease the exposure time of the first image. Alternatively, if you have more than two input images, remove the first image. Image status of M_HDR_IMAGE_LIMITED_TO_SINGLE_COLOR If the value of M_HDR_IMAGE_STATUS is M_HDR_IMAGE_LIMITED_TO_SINGLE_COLOR, it means that the grayscale tones in the current HDR image have been reduced to a single tone within the merge area. There are two ways to solve this: Reduce the difference between the exposures of each input image to increase the size of the merge area. Try less extreme values for M_FUSION_LOW_THRESHOLD and M_FUSION_HIGH_THRESHOLD. Image status of M_HDR_INSUFFICIENT_CONTRAST If the value of M_HDR_IMAGE_STATUS is M_HDR_INSUFFICIENT_CONTRAST, it means that the number of tones available in the merge area is insufficient. There are two ways to solve this: Improve the dynamic range of each input image by making sure that it uses the full range of tones available with the image type used. Try less extreme values for M_FUSION_LOW_THRESHOLD and M_FUSION_HIGH_THRESHOLD. Image status of M_HDR_INSUFFICIENT_CONTRIBUTION If the value of M_HDR_IMAGE_STATUS is M_HDR_INSUFFICIENT_CONTRIBUTION, it means that the added image does not increase the dynamic range already present in the current working HDR image. There are multiple ways to solve this: Verify the order of the input images to make sure that they are in the proper sequence (from the longest exposure time to the shortest). Increase the difference between the exposures of each input image to better differentiate them. When the image has few tones that have a high contrast between them, add an object with varying tones of gray in the scene to be photographed. This will provide a reference. Image status of M_HDR_MERGE_AREA_GAIN_INCORRECT If the value of M_HDR_IMAGE_STATUS is M_HDR_MERGE_AREA_GAIN_INCORRECT, it means that the gain to be applied to the input image is less than 1. To solve this, verify the order of the input images to make sure that they are in the proper sequence (from the longest exposure time to the shortest). Image status of M_HDR_MERGE_AREA_SIZE_INSUFFICIENT If the value of M_HDR_IMAGE_STATUS is M_HDR_MERGE_AREA_SIZE_INSUFFICIENT, it means that the size of the merge area is below the threshold specified by M_FUSION_COVERAGE. There are multiple ways to solve this: Reduce the difference between the exposures of each input image to increase the size of the merge area. Improve the dynamic range of each input image by making sure that it uses the full range of tones offered by the image. Try less stringent values for M_FUSION_LOW_THRESHOLD, M_FUSION_HIGH_THRESHOLD, and M_FUSION_COVERAGE. Blocks appearing in the resulting HDR image Black or white blocks can sometimes be seen in the resulting HDR image. There are multiple ways to solve this. In the case of black blocks, the values of M_FUSION_LOW_THRESHOLD and M_TONE_MAPPING_LOW_THRESHOLD need to be validated. One of them could have a value that is too high, causing the elimination of key pixels in the end result. In the case of white blocks, the values of M_FUSION_HIGH_THRESHOLD and M_TONE_MAPPING_HIGH_THRESHOLD need to be validated. One of them could have a value that is too low, causing the elimination of key pixels in the end result. The value of M_TONE_MAPPING_COEFFICIENT might need to be lower. If an object in the image uses a smaller range of shades of gray and gets mapped to a larger range of shades of gray, the tone transitions will look blocky due the increased step between each tone displayed. For example, an object with a range of 40 shades of gray out a possible 256 (8-bit) that gets mapped to a range of 160 shades of gray will look blocky. No post-processing is applied to minimize this effect. Additionally, other details in the image might see their intensity range reduced, resulting in a flat appearance. Darker areas appearing flat with few details When the order of input images is incorrect, the darker areas of an image can have significantly fewer details than expected, as well as severe blocking and banding. To solve this, adjust the order of the input images so that the first image has the darkest details (brightest image with the longest exposure) and the last image has the brightest details (darkest image with the shortest exposure). Resulting image is too dark or too bright When the resulting image is too dark or too bright, modify the M_TONE_MAPPING_COEFFICIENT, which controls the distribution of pixel intensities during the tone mapping of the raw HDR image. A higher value will lower the overall brightness by giving a broader intensity range to the brighter details. A lower value will increase the overall brightness by giving a broader intensity range to the darker details. High dynamic range registration Steps to performing a high dynamic range registration operation Input images Customizing your HDR operation Fusion Gain Tone mapping Example of HDR Troubleshooting M_HDR_STATUS value below 1.0 Image status of M_HDR_IMAGE_LIMITED_TO_SINGLE_COLOR Image status of M_HDR_INSUFFICIENT_CONTRAST Image status of M_HDR_INSUFFICIENT_CONTRIBUTION Image status of M_HDR_MERGE_AREA_GAIN_INCORRECT Image status of M_HDR_MERGE_AREA_SIZE_INSUFFICIENT Blocks appearing in the resulting HDR image Darker areas appearing flat with few details Resulting image is too dark or too bright ",
      "wordCount": 3266,
      "subEntries": []
    }
  ]
}]
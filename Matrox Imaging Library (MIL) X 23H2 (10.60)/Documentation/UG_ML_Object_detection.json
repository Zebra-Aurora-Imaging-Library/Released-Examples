[{
  "id": "UG_ML_Object_detection",
  "version": "2024020714",
  "title": "Object detection",
  "subTitles": null,
  "location": "MIL UG P08: Machine learning tasks",
  "pageURL": "content\\UserGuide\\ML_Object_detection\\ChapterInformation.htm",
  "text": " Chapter 53: Object detection This chapter explains how to perform object detection using machine learning with the MIL Classification module. Object detection overview Steps to perform object detection Perform all required allocations Build and populate your dataset Train your classifier context Predict with your classifier Save your classification contexts Free your allocated objects Building a dataset for object detection Add class definitions and entries to the source dataset context Add regions Splitting your dataset Augmentation and other data preparations Data preparation settings Augmenting images Exporting and importing a dataset Export Import Classifier and training settings for object detection Training folders Training related settings Training modes Training and analysis for object detection Monitoring the training process Prediction for object detection Prepare for prediction Predict Results Drawing results Object detection example ",
  "wordCount": 131,
  "subEntries": [
    {
      "id": "UG_ML_Object_detection_Object_detection_overview",
      "version": null,
      "title": "Object detection overview",
      "subTitles": null,
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Object_detection\\Object_detection_overview.htm",
      "text": " Object detection overview This chapter explains how to perform object detection using machine learning with the MIL Classification module. Note that this chapter expands on topics previously discussed in Machine learning fundamentals. It is recommended to review these topics if you have not already done so. For the most recent documentation of this chapter, particularly as it relates to statistical analysis (MclassStatCalculate()), check for an updated version of the MIL Help online at zebra.com/aurora-imaging-library-help. Object detection lets you to classify regions (objects) within an image. It typically requires a predefined object detection classifier context that was defined by Matrox (an ODNet), and that must be trained with an images dataset context. To train an object detection classifier, you must give it many images that represent the real-world problem it must solve, along with a label identifying the class of each rectangular region (object) in the images. The classifier learns from these training images how to detect and differentiate the classes. Once the classifier is trained, it can predict the class of similar regions within similar images. An example of object detection is finding instances of small and large knots on wood. Object detection is often performed when you want to identify regions in an image that you can bound within a rectangle (for example, rectangular regions enclosing potential defects like small or large knots on wood). With object detection: The class and location of each instance is returned (each instance found represents one class). Instances are found (predicted) when a rectangular region in the target represents one of the defined classes (for example, the rectangular regions enclosing the SmallKnot and LargeKnot classes). The location of instances returned by object detection can prove useful; for example, you can ignore results that do not occur in a predefined part of the target. Regions are defined by rectangles; this can be seen as a quick way to label your data. The use cases for object detection, such as finding various type of defects, are often like those for segmentation. In general, object detection is preferred when you want to simplify locating instances of objects within an image, and you do not need the high level of precision that segmentation performs. If you require the kind of precision that a pixel level segmentation provides, see Chapter 52: Segmentation. For more information about the differences between object detection and segmentation, see Object detection versus segmentation. Object detection overview ",
      "wordCount": 405,
      "subEntries": []
    },
    {
      "id": "UG_ML_Object_detection_Steps_to_perform_object_detection",
      "version": null,
      "title": "Steps to perform object detection",
      "subTitles": [
        "Perform all required allocations",
        "Build and populate your dataset",
        "Train your classifier context",
        "Predict with your classifier",
        "Save your classification contexts",
        "Free your allocated objects"
      ],
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Object_detection\\Steps_to_perform_object_detection.htm",
      "text": " Steps to perform object detection The following steps provide a basic methodology to perform object detection with the MIL Classification module: Perform all required allocations. Build and populate your dataset. Train your classifier context. Predict with your classifier. If necessary, save your classification contexts. Free your allocated objects. Perform all required allocations These allocations are required to perform object detection. In many cases, some of these contexts and buffers will be imported or restored from previous work, using MclassImport() or MclassRestore(). Allocate a predefined object detection classifier context, using MclassAlloc() with M_CLASSIFIER_DET_PREDEFINED and M_ODNET. Note, MclassTrain() can automatically allocate a classifier context if none is specified. Allocate an images dataset context to hold all of your data (source dataset), using MclassAlloc() with M_DATASET_IMAGES. Allocate an object detection training context, using MclassAlloc() with M_TRAIN_DET. A training context holds the settings with which to train a classifier context. Allocate an object detection training result buffer to hold training results, using MclassAllocResult() with M_TRAIN_DET_RESULT. Allocate an object detection result buffer to hold the prediction results, using MclassAllocResult() with M_PREDICT_DET_RESULT. Object allocation for object detection is summarized in the table below: MIL allocation Classifier context M_CLASSIFIER_DET_PREDEFINED Specific predefined classifier context M_ODNET Dataset context M_DATASET_IMAGES Data preparation context M_PREPARE_IMAGES_DET Training context M_TRAIN_DET Training result buffer M_TRAIN_DET_RESULT Prediction result buffer M_PREDICT_DET_RESULT Build and populate your dataset It is recommended that you build and manage your datasets interactively, using MIL CoPilot. For example, MIL CoPilot lets you interactively create, label, modify, import, and export datasets. When using MIL CoPilot, ensure that you have installed all related MIL updates. For more information, see the Requirements, recommendations, and troubleshooting section of Chapter 47: Machine learning with the MIL Classification module. If you restore or import a dataset that is fully built, you can skip this step. If you do not use MIL CoPilot, you can follow the steps below to build and populate the source dataset: Add class definitions to the source dataset, using MclassControl() with M_CLASS_ADD. Note, the number of classes with which to categorize your data is a key decision to make when performing object detection. Optionally, you can specify settings to help manage class definitions, using MclassControl(). For example, you can assign a color (M_CLASS_DRAW_COLOR) and an icon image (M_CLASS_ICON_ID) to class definitions. This allows you to draw and visually identify them with MclassDraw(). Add entries to the source dataset, using MclassControl() with M_ENTRY_ADD. Entries in an images dataset require image data (paths to where images are stored). To specify the location from which to get an entry's image data, use MclassControlEntry() with M_ENTRY_IMAGE_PATH. For each entry, add regions and specify the class definitions (ground truths) that are represented, using MclassEntryAddRegion() with the ClassIndexGroundTruth parameter. For object detection, you can only add rectangular regions (M_DESCRIPTOR_TYPE_BOX). This step is known as labeling your data. You must label your data before training (labeled data is a prerequisite to using the module). The quality, quantity, and proportionality of correctly labeled data is critical to building a good dataset, and developing a properly trained classifier. Split your source dataset into the training, development, and testing datasets. You can split datasets manually, using MclassSplitDataset(), or you can pass a single dataset to MclassTrain(), and let MIL split it into the training and development datasets. If you want to use a testing dataset, you must first split a portion of the source dataset into a testing dataset using MclassSplitDataset(). Optionally, prepare your data by cropping or resizing images, and adding augmented images to a dataset. If you are letting MclassTrain() split your source dataset, data preparation is managed by the training contexts' internally defined data preparation context. You can prepare your data by calling MclassControl() with the identifier of the internal data preparation context (which you can inquire with M_PREPARE_DATA_CONTEXT_ID) and specifying the required data preparation controls (for example, M_PRESET_CROP and M_PRESET_NOISE_SALT_PEPPER). These preparations will be applied when you call MclassTrain(). You can use MclassPrepareData() to prepare data outside of MclassTrain(). To do this you must allocate a data preparation context (M_PREPARE_IMAGES_DET). You should not use MclassPrepareData() on a dataset if you are letting MclassTrain() split the dataset. After you build a dataset, you can export it using MclassExport() so you can train a classifier with it at a later time. It is recommended that you export it with M_IMAGE_DATASET_FOLDER and M_COMPLETE. You can use MclassImport() to import previously defined and exported datasets (for example, from a folder or a CSV file). It is recommended that you familiarize yourself with how dataset information (such as images) is stored on disk, and how you can ensure that information is well organized and portable. For more information, see the Guidelines for managing an images dataset section of Chapter 48: Datasets. Train your classifier context To train your classifier context on your datasets, perform the following: Modify training settings, using MclassControl(). Optionally, hook functions to training events, using MclassHookFunction(). Preprocess the training context, using MclassPreprocess(). Perform the training operation, using MclassTrain(). Optionally, get information about training events that caused the hook-handler function to execute, using MclassGetHookInfo(). If results indicate that the current training operation will be unsuccessful, stop the training, and modify your training settings and if necessary your dataset, and re-train. Optionally, get training results, using MclassGetResult(). As indicated in the previous step, if results indicate an unsuccessful training (for example, the classifier performed poorly on the development dataset), modify your training settings and if necessary your dataset, and re-train. Copy the classification result buffer that MclassTrain() produced into a classifier context, using MclassCopyResult(). Once copied, the classifier context is considered trained. If your training results are unsatisfactory, adjust training settings, contexts, and datasets as required, and call MclassTrain() with the trained classifier context. Predict with your classifier To predict with the trained classifier context, perform the following: Optionally, set the values used by the non-maximum suppression algorithm (M_NMS_IOU_THRESHOLD, M_NMS_POST_K, M_NMS_TOP_K) and the threshold score for drawing the detection results (M_SCORE_THRESHOLD). Preprocess the trained classifier context, using MclassPreprocess(). Perform the prediction operation with the trained classifier context and the target data that you want to classify, using MclassPredict(). If your training images were prepared, then the target image must also be prepared in the same way. This maintains consistency between the training data and the target data. If you are predicting with a test dataset and the predicted classes are not what you expect, when compared to the actual classes in the test dataset (the ground truth), you can adjust your training setup, and continue the training process. Optionally, you can perform the prediction operation with a dataset as your target. When predicting with a dataset as your target, you can hook functions to prediction events, using MclassHookFunction(). Retrieve the required results from the prediction result buffer, using MclassGetResult(). You can also draw prediction results, using MclassDraw(). You can also retrieve the prediction results on a dataset, using MclassGetResultEntry() and draw prediction results from a dataset, using MclassDrawEntry(). Save your classification contexts If necessary, save your classification contexts, using MclassSave() or MclassStream(). Free your allocated objects Free all your allocated objects, using MclassFree(), unless M_UNIQUE_ID was specified during allocation. Steps to perform object detection Perform all required allocations Build and populate your dataset Train your classifier context Predict with your classifier Save your classification contexts Free your allocated objects ",
      "wordCount": 1217,
      "subEntries": []
    },
    {
      "id": "UG_ML_Object_detection_Building_a_dataset_for_object_detection",
      "version": null,
      "title": "Building a dataset for object detection",
      "subTitles": [
        "Add class definitions and entries to the source dataset context",
        "Add regions",
        "Splitting your dataset",
        "Augmentation and other data preparations",
        "Data preparation settings",
        "Augmenting images",
        "Exporting and importing a dataset",
        "Export",
        "Import"
      ],
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Object_detection\\Building_a_dataset_for_object_detection.htm",
      "text": " Building a dataset for object detection This section discusses specific options for building a dataset for object detection. For an overview of the many considerations you should make when building a dataset, see Chapter 48: Datasets. It is recommended that you use MIL CoPilot to create, label, modify, augment, and export your dataset interactively. Add class definitions and entries to the source dataset context After allocating your images dataset context, you must add class definitions and entries to it. Entries in an images dataset for object detection typically have multiple rectangular regions, which are smaller than the target image being predicted. Each region identifies a class. Although you can use an images dataset for image classification, segmentation, and object detection simultaneously, it is generally recommended to have a dataset that is exclusively for one task or the other. The process for adding class definitions for object detection are the same as for segmentation. For more information, see the Building a dataset for segmentation section of Chapter 52: Segmentation. Add regions To add regions to an entry in an images dataset context, call MclassEntryAddRegion(). Each time you call this function, one or more regions (classes) are added to the specified entry. Entry regions must properly and proportionally represent all the different classes. The number and type of regions that are added is determined by the specified region's descriptor. Typically, you would add regions by specifying rectangles in a graphics list (M_DESCRIPTOR_TYPE_BOX). Each added region must have a ground truth (class) assigned to it (just like each entry image for image classification has a ground truth). For example, when using box regions (M_DESCRIPTOR_TYPE_BOX), the pixels in the entry image that correspond to the areas within the bounding box are considered part of the ground truth established for this region, which you must set with the ClassIndexGroundTruth parameter. The table below illustrates the components of an image dataset for object detection. Entry Image Region Label Entry 0 Image 0 Region 0 Class 0 Region 1 Class 1 Region 2 Class 2 Entry 1 Image 1 Region 0 Class 0 Region 1 Class 1 Entry 2 Image 2 Region 0 Class 0 Region 1 Class 2 The following code snippet shows how to add rectangular entry regions to a dataset for object detection. // Allocate a graphics list MIL_UNIQUE_GRA_ID BBox = MgraAllocList(MilSystem, M_DEFAULT, M_UNIQUE_ID); // Add a rectangle graphic to the graphics list MgraRect(M_DEFAULT, BBox, 4, 4, 16, 16); // Add the rectangle to the entry in the dataset MclassEntryAddRegion(Dataset, EntryIdx, M_DEFAULT_KEY, M_DESCRIPTOR_TYPE_BOX, BBox, M_NULL, ClassIdx, M_DEFAULT); You can control and inquire about regions, by calling MclassControlEntry() and MclassInquireEntry(). To draw region information, call MclassDrawEntry(). When you are finished adding your data, you should ensure that all of the images in your source dataset are the same size. This is a requirement for training an object detection classifier. To do this, call MclassPrepareData() and specify your source dataset context. To specify how to resize images, use the M_SIZE_MODE and M_RESIZE_SCALE_FACTOR controls. Resizing can be performed automatically or with explicit image width and height values. Note, using smaller images will allow for faster training. Splitting your dataset You can call MclassSplitDataset() to split a dataset into two smaller datasets. You can do this to create a training, development, or testing dataset out of your source dataset. If you want to use a testing dataset, then you should first split a portion of the source dataset into a testing dataset by calling MclassSplitDataset() with M_SPLIT_DET_CONTEXT_DEFAULT or M_SPLIT_DET_CONTEXT_FIXED_SEED. For more information about splitting different datasets, see the Different datasets and how they are split section of Chapter 48: Datasets. Augmentation and other data preparations Optionally, prepare your data and add augmented images to a dataset using MclassPrepareData(). You can call MclassPrepareData() with an images dataset, or an individual image. You should not use MclassPrepareData() on a dataset if you are letting MclassTrain() split the dataset, since MIL will automatically augment your data with the internally defined data preparation context. You can call MclassControl() with the identifier of the internal data preparation context (which you can inquire with M_PREPARE_DATA_CONTEXT_ID) and specify the required data preparation controls (for example, M_PRESET_CROP and M_PRESET_NOISE_SALT_PEPPER). These preparations will be applied when you call MclassTrain(). For more information, see the Data augmentation and other data preparations section of Chapter 48: Datasets. Data preparation settings MclassPrepareData() requires that you allocate a data preparation context by calling MclassAlloc() with M_PREPARE_IMAGES_DET. The data preparation context holds the settings with which to modify the specified source (the images in a dataset or an individual image). Preprocess the data preparation context by calling MclassPreprocess(). You must also specify the location in which prepared images will be stored by setting M_PREPARED_DATA_FOLDER. For more information, see the Data augmentation and other data preparations section of Chapter 48: Datasets. Augmenting images Some augmentations will also affect the regions and labels of the image. For example, if you rotate an image, the bounding boxes are transformed to always be parallel with the image borders. This is done automatically during augmentation, by creating a new bounding box that completely surrounds the corners of the rotated bounding box. The rotation and translation augmentations shown below reveal new entries. Augmented entries, and the entries used to augment them, must only be in the training dataset. Augmented entries in the development dataset can cause errors. For more information, see the Data augmentation and other data preparations section of Chapter 48: Datasets. Exporting and importing a dataset Exporting and importing your datasets allows you to make your datasets portable and organized. This will allow you to easily reuse or combine your datasets. For more information, see the Guidelines for managing an images dataset section of Chapter 48: Datasets. Export After you build a dataset, export it using MclassExport(). It is recommended that you export it with M_IMAGE_DATASET_FOLDER to create an organized folder than can be easily imported and reused. Import You can use a folder or CSV file to define data for a dataset. To import it to a dataset context, use MclassImport() and specify what to import (for example, M_COMPLETE, M_ENTRIES, M_AUTHORS, and M_CLASS_DEFINITIONS). For more information about importing data, see the Importing data from a folder or CSV file section of Chapter 48: Datasets. You can also use MclassRestore() to restore a dataset that was previously saved to a file using MclassSave() or MclassStream(). Building a dataset for object detection Add class definitions and entries to the source dataset context Add regions Splitting your dataset Augmentation and other data preparations Data preparation settings Augmenting images Exporting and importing a dataset Export Import ",
      "wordCount": 1099,
      "subEntries": []
    },
    {
      "id": "UG_ML_Object_detection_Classifier_and_training_settings_for_object_detection",
      "version": null,
      "title": "Classifier and training settings for object detection",
      "subTitles": [
        "Training folders",
        "Training related settings",
        "Training modes"
      ],
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Object_detection\\Classifier_and_training_settings_for_object_detection.htm",
      "text": " Classifier and training settings for object detection Before you can start training, you need to allocate a classifier context and training objects, and set training related settings. For more information on training settings, see the Fundamental decisions and settings section of Chapter 49: Training. Training folders Set the destination folder that will store prepared images using MclassControl() with M_TRAIN_DESTINATION_FOLDER. Training related settings A training context holds the settings with which to train a classifier context, such as training modes. Set your training related settings as required. Once you have established your training settings for your training context, you must preprocess the context by calling MclassPreprocess() with the identifier of the training context. Training modes For object detection, the only training mode available is M_COMPLETE. This is for completely restarting the training of an object detection classifier context, or for training an object detection classifier context that is not trained. You do not have to manually set the training mode for object detection since, M_DEFAULT is the same as M_COMPLETE. MIL automatically sets the related training mode controls to the required settings for object detection. You can also modify these controls yourself to adjust the training process by calling MclassControl(). The training mode controls let you adjust the: Learning rate (M_INITIAL_LEARNING_RATE and M_LEARNING_RATE_DECAY). For object detection, the default learning rate is 0.001 and the default learning rate decay is 0.05. Maximum number of epochs (M_MAX_EPOCH). For object detection, the default maximum number of epochs is 60. Mini-batch size (M_MINI_BATCH_SIZE). For object detection, the default mini-batch size is 4. Schedule type (M_SCHEDULER_TYPE). Note, for object detection, only M_FACTOR_DECAY is available. Such training mode controls are also known as hyperparameters. If your classifier is not performing as expected, you can adjust the hyperparameters and retrain. Classifier and training settings for object detection Training folders Training related settings Training modes ",
      "wordCount": 308,
      "subEntries": []
    },
    {
      "id": "UG_ML_Object_detection_Training_and_analysis_for_object_detection",
      "version": null,
      "title": "Training and analysis for object detection",
      "subTitles": [
        "Monitoring the training process"
      ],
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Object_detection\\Training_and_analysis_for_object_detection.htm",
      "text": " Training and analysis for object detection When you call MclassTrain(), the classifier context will be trained with the settings specified in the training context, and trained on the data in the training and development datasets. The results from training will be stored in the classification result buffer. These must all be specified when you call MclassTrain(). For more information on the training process, and training analysis, see the Analysis, adjustment, and additional settings section of Chapter 49: Training. Monitoring the training process You can save time and improve the training process by using MclassHookFunction() to hook a function to a training event. Call MclassGetHookInfo() to get information about the event that caused the hook-handler function to be called. Training can take a long time, and you can use hook functions to monitor the training process. It is recommended that you call a hook function at the end of each mini-batch (M_MINI_BATCH_TRAINED), and also at the end of each epoch (M_EPOCH_TRAINED), to ensure that training is developing in the correct direction. While your classifier is training, you can get the information from the events that caused the hook-handler function to execute by using MclassGetHookInfo(). Typically, you should expect to monitor the training process for proper convergence, and to make modifications to the process. You might need to abort or restart it, if required. You can monitor the loss metric of the development dataset during training by using MclassGetResult() with M_DEV_DATASET_EPOCH_LOSS. You can also monitor the loss metric of the training dataset using, MclassGetResult() with M_TRAIN_DATASET_MINI_BATCH_LOSS. Training and analysis for object detection Monitoring the training process ",
      "wordCount": 266,
      "subEntries": []
    },
    {
      "id": "UG_ML_Object_detection_Prediction_for_object_detection",
      "version": null,
      "title": "Prediction for object detection",
      "subTitles": [
        "Prepare for prediction",
        "Predict",
        "Results",
        "Drawing results"
      ],
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Object_detection\\Prediction_for_object_detection.htm",
      "text": " Prediction for object detection MclassPredict() uses a trained classifier context to make class predictions on a target. For a trained object detection classifier, your target is either an image or a dataset of images. For more information about prediction, see the Prediction settings, results, and drawings section of Chapter 50: Prediction. Prepare for prediction Allocate a classification result buffer to hold the prediction results, using MclassAllocResult() with M_PREDICT_DET_RESULT. When predicting with an object detection classifier, the target image size must be the same size as the images used to train the classifier. Before predicting, preprocess your trained classifier context using MclassPreprocess(). Predict Perform the prediction operation with the trained classifier context and the target data that you want to classify using MclassPredict(). If your training images were prepared by cropping and resizing, then the target image must also be prepared in the same way. This maintains consistency between the training data and the target data. By default, MIL assumes that the target images are the same size as the training images used to train your ODNet classifier, which you can retrieve using MclassInquire() with M_SIZE_X and M_SIZE_Y. When performing the prediction operation with a dataset as your target, you can hook functions to prediction events, using MclassHookFunction(). To prevent the classification process from taking too long, you can set a maximum calculation time for MclassPredict(), by calling MclassControl() with M_TIMEOUT. You can also stop the current execution of MclassPredict() (from another thread of higher priority), by specifying M_STOP_PREDICT. Note, if you have a testing dataset, you should predict with it, using MclassPredict(). As previously discussed, predicting with a testing dataset serves as a quarantined final check for your trained classifier. If the results are what you expect (they should be approximately the same as your training results), you can continue with prediction using your trained classifier. If the results are not what you expect, it is a sign that you should retrain the classifier. Results Retrieve the required results from the classification result buffer, using MclassGetResult(). You can also get results from a specific entry in a dataset using MclassGetResultEntry(). Typically, the most important prediction results to retrieve are the best predicted class (M_BEST_CLASS_INDEX), its score (M_BEST_CLASS_SCORE), and the coordinates and shape of the bounding boxes (M_BOX_4_CORNERS, M_CENTER_..., M_HEIGHT, or M_WIDTH). Drawing results To draw prediction results, call MclassDraw(). You can also draw prediction results from a specific entry in a dataset using MclassDrawEntry(). To draw the icon image related to the class, use M_DRAW_CLASS_ICON. Note, this image is not a result; you specify it using MclassControl() with M_CLASS_ICON_ID, and you draw it from a classifier context or a dataset context (not a result buffer). By drawing the class' icon image, you are able to visually identify the class for which you are getting results. Similarly, you can also specify and draw a color related to a class, to help visually identify it (M_DRAW_CLASS_COLOR_LUT). Prediction results for object detection can prove complex to decipher, given that you can have multiple results for each training image. For example, an object detection prediction can identify two classes and four instances in the following target image. By calling MclassDraw(), you can draw a box around the classes found, the center of the box, the corresponding color of every class, the name of the class, as well as the prediction score. Such drawings help you better understand the results of the prediction. To perform these drawing operations, specify M_DRAW_BOX, M_DRAW_BOX_NAME, M_DRAW_BOX_SCORE (for the image above). You can also specify M_DRAW_BOX_CENTER to draw the center positions of the bounding boxes. You can also set the threshold score for drawing using MclassControl() with M_SCORE_THRESHOLD. Prediction for object detection Prepare for prediction Predict Results Drawing results ",
      "wordCount": 621,
      "subEntries": []
    },
    {
      "id": "UG_ML_Object_detection_Object_detection_example",
      "version": null,
      "title": "Object detection example",
      "subTitles": null,
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Object_detection\\Object_detection_example.htm",
      "text": " Object detection example The example ClassDetectionCompleteTrain.cpp demonstrates how to train a predefined (Matrox defined) object detection classifier context to detect instances of differently sized knots on wood. To run this example, use the Matrox Example Launcher in the MIL Control Center. To view this example, refer to the following: classdetectioncompletetrain.cpp Object detection example ",
      "wordCount": 55,
      "subEntries": []
    }
  ]
}]
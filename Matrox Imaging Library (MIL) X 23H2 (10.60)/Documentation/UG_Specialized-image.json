[{
  "id": "UG_Specialized-image",
  "version": "2024020714",
  "title": "Specialized image processing",
  "subTitles": null,
  "location": "MIL UG P03: 2D processing and analysis",
  "pageURL": "content\\UserGuide\\Specialized-image\\ChapterInformation.htm",
  "text": " Chapter 5: Specialized image processing This chapter describes different specialized image processing techniques. Specialized image processing in general Rearranging areas in your grabbed image Deinterlacing Discard algorithm on the entire image Averaging algorithm on the entire image Bob algorithm on the entire image Adaptive algorithms Correcting dead pixels Removing CCD artifacts from grabbed images How to perform a flat-field correction Electrical bias (offset image or constant) Thermal agitation (dark image or constant) CCD sensitivity (flat image or constant) Gain Transform and denoise images using wavelets Wavelet context Filters Operation mode Wavelet transformations Results and drawings Wavelet denoising Co-occurrence matrix statistics Building the grayscale co-occurrence matrix Tiling your image Supported co-occurrence statistics A co-occurrence example Limiting the bit depth of your image Fast Fourier Transform Magnitude and phase Filtering an image Discrete Cosine Transform Peak intensity detection and depth maps Setting up peak intensity detection Generating an uncorrected depth map Multiple peaks in a single lane Extraction of peaks from a multi-frame buffer Augmentation Steps to augment an image Priority Probability Randomness and seeds Random rotation angle ",
  "wordCount": 178,
  "subEntries": [
    {
      "id": "UG_Specialized-image_Specialized_image_processing_in_general",
      "version": null,
      "title": "Specialized image processing in general",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\Specialized-image\\Specialized_image_processing_in_general.htm",
      "text": " Specialized image processing in general Besides the image processing operations discussed in the previous chapters, MIL supports specialized image processing operations. These specialized operations allow you to, for example, perform various frequency transforms and remove artifacts introduced by the camera. Specialized image processing in general ",
      "wordCount": 47,
      "subEntries": []
    },
    {
      "id": "UG_Specialized-image_Rearranging_areas_in_your_grabbed_image",
      "version": null,
      "title": "Rearranging areas in your grabbed image",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\Specialized-image\\Rearranging_areas_in_your_grabbed_image.htm",
      "text": " Rearranging areas in your grabbed image If one or more areas appear out of order in your grabbed image (for example, when using a camera that returns nonsequential data), you might need to rearrange one or more areas in your grabbed image. Rather than creating multiple child buffers and performing multiple copies, you can use MimRearrange(). This function copies specified areas from a newly grabbed image to a specified destination image. MimRearrange() might save some processing time and overhead when compared to multiple child buffer copy operations. The MimRearrange() function uses a rearrangement image processing context to store processing settings. You must set up the rearrangement image processing context with the offsets of the required areas in the source image buffer, the offsets of the target areas in the destination image buffer, and the size of each area. Only the specified areas of the source image are copied from the source image to the areas specified in the destination image. The rest of the source image is ignored and not copied. The source and destination image buffers need not be of the same size, but must be equal to or larger than the largest supplied source and destination coordinates. You can copy areas or rows of the source image. In the following example, two areas are copied from the source image to the destination image. In the above illustration, you would have to set up the rearrangement image processing context with the following: Area Array with the X-offsets of the source areas Array with the Y-offsets of the source areas Array with the X-offsets of the target areas Array with the Y-offsets of the target areas Array with the X- size of the areas Array with the Y- size of the areas 1 20 39 0 72 72 39 2 60 93 0 0 139 136 Before calling MimRearrange(), you must first set up the rearrangement image processing context by performing the following: Allocate a rearrangement image processing context, using MimAlloc() with M_REARRANGE_CONTEXT. Set the processing mode, using MimControl() with M_MODE. The processing mode indicates how to perform the operation. If the processing mode is set to lines mode, each area is a single horizontal line. If the mode is set to rectangles mode, each area is a single rectangle. Allocate 4 or 6 arrays with the same number of entries. When copying lines, only 4 arrays are required. When copying rectangles, 6 arrays are required. There should be one entry for every area to be rearranged. Set up 2 arrays with the offsets of the areas in the source image: one array for the X-offset and one array for the Y-offset. Then, load these arrays into the context, using MimPut() with M_XY_SOURCE. If copying lines, the X-coordinate of all the (x, y) values must be set to 0. Set up 2 arrays with the offsets of the areas in the destination image: one array for the X-offset and one array for the Y-offset. Then, load these arrays into the context, using MimPut() with M_XY_DESTINATION. If copying lines, the X-coordinate of all the (x, y) values must be set to 0. The destination offset arrays must contain the same number of elements as the source offset arrays. If copying rectangles, set up 2 arrays with the sizes of the areas: one array for the dimension in X and one array for the dimension in Y. Then, load these arrays into the context, using MimPut() with M_XY_SIZE. If copying lines, you should not specify the size of the areas, otherwise an error is reported. By default, all lines (rows) are 1 pixel high and as wide as the source image. Preprocess the context by calling MimRearrange() with M_PREPROCESS. Both the source and/or destination image buffers can be set to M_NULL. If, however, the source or destination image buffer is provided, it should be a typical source or destination image buffer, respectively, and it will be used by the preprocess operation to better optimize future calls. If the preprocess operation is not done explicitly, it will be done when MimRearrange() is first called. Rearranging areas in your grabbed image ",
      "wordCount": 689,
      "subEntries": []
    },
    {
      "id": "UG_Specialized-image_Deinterlacing",
      "version": null,
      "title": "Deinterlacing",
      "subTitles": [
        "Discard algorithm on the entire image",
        "Averaging algorithm on the entire image",
        "Bob algorithm on the entire image",
        "Adaptive algorithms"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\Specialized-image\\Deinterlacing.htm",
      "text": " Deinterlacing Interlacing artifacts result when analyzing an image grabbed with an interlaced camera. To enhance the quality of the image, you can apply a deinterlacing algorithm using MimDeinterlace(). To understand and differentiate between deinterlacing algorithms, knowledge of interlacing artifacts and the reason they occur in an image is important. When an interlaced camera acquires images, it captures two separate fields per frame. However, these fields are not acquired at the same moment in time. They are separated by 1/60 th of a second in NTSC or by 1/50 th of a second in PAL. In the event that an object was in motion when it was being acquired, its position would be slightly shifted from one field to the next, resulting in interlacing artifacts. Taking a close look at the image, you can see a series of horizontal lines inside and around the edges of the bird. These lines are known as interlacing artifacts. The image is clearly formed by the superposition of two distinct fields, at two distinct moments in time. MIL supports three different deinterlacing algorithms to reduce or remove interlacing artifacts: the discard, averaging, and bob algorithms. Note that interlacing artifacts are only present in objects in motion. MIL can apply these algorithms to all the pixels in an image or only to the pixels in the image that are part of objects in motion. Discard algorithm on the entire image The discard algorithm, the simplest deinterlacing algorithm, removes one field from the image (the odd or even rows) and interpolates the remaining rows. For example, if you choose to discard the odd rows, the first odd row would be the average of the first and second even rows. This algorithm is advantageous because it removes all the interlacing artifacts and has a fast processing time. However, if the sequence of deinterlaced images is viewed as a video, the motion in the sequence will not be as fluid as the original sequence because each discarded field represents a unique moment in time. Also, the areas in the image that did not contain interlacing artifacts will lose sharpness in their edges. The following example depicts the result of the discard algorithm on an image containing interlacing artifacts. Averaging algorithm on the entire image The averaging deinterlacing algorithm is equivalent to performing the discard algorithm once on the even field and once on the odd field and averaging the two resulting frames. In reality, MIL proceeds in a much more efficient way, making the processing speed of this function similar to the speed of the discard algorithm. The interlacing artifacts are completely removed and this process preserves the fluidity of the video because complete fields are not discarded and therefore no individual moments in time are discarded. However, the images suffer from ghost effects (objects in motion appear in double) when looking at the stilled frames because two distinct moments in time are being merged together. These effects are barely noticeable when the images are viewed as a sequence. The loss of sharpness in background objects' edges also occurs when using this algorithm, but not as badly as with the discard algorithm. The example below illustrates the ghost effects that arise from the averaging algorithm. Bob algorithm on the entire image The bob algorithm applies the discard algorithm twice, once to each field. The two frames that result each become an output frame. Therefore, for every input frame, there are two output frames. This algorithm completely removes the interlacing artifacts and the motion in the video is fluid. The processing speed of this algorithm is also fast, although not as fast as the discard or averaging algorithms. As for the image quality, the same loss of sharpness in static objects' edges is observed as with the discard algorithm. In addition, if the sequence of deinterlaced images is viewed in motion, the edges of static objects flutter occasionally. The illustration below shows a sequence of frames, each representing the same background object. This is an example of the fluttering that can occur when viewing a sequence of deinterlacing images. If the frames are displayed very quickly (1/30 th of a second on a progressive scan display), the top and bottom edges of the rectangle appear to be moving up and down, and it is this motion that we call fluttering. Adaptive algorithms MIL also supports an adaptive version of each algorithm: the adaptive discard, adaptive average, and adaptive bob algorithms. Since interlacing artifacts are caused by differences in successive frames and are only present in the areas of the image containing objects in motion, there is no need to apply the algorithms to all the pixels in the image; this actually results in a loss in the sharpness in the edges of static objects. Therefore, prior to performing their respective deinterlacing operation, the adaptive algorithms first perform motion detection to determine the areas of the image depicting moving objects. To achieve this, each pixel in the frame to deinterlace is compared with the pixels at the same location in a group of neighboring frames. If the difference between the maximum and minimum pixel intensity is above a specified threshold value, the pixel in the frame to deinterlace is considered part of an object in motion. If the difference is below the threshold value, the pixel is considered part of a background object. After all the pixel locations have been checked, the deinterlacing algorithm is applied to the pixels in motion and the background pixels remain untouched. Note that the adaptive algorithms are more effective in preserving the sharpness and detail in background objects, however they can fail to remove some subtle interlacing artifacts. In addition, they have a longer processing time than their non-adaptive counterparts. If an adaptive algorithm is selected, each frame in the input sequence of images will undergo motion detection. Using MimControl(), you can set the number of neighboring frames used for motion detection (M_MOTION_DETECT_NUM_FRAMES), the location of the frame to be processed within this group of frames (M_MOTION_DETECT_REFERENCE_FRAME), and the threshold value used to differentiate between pixels of objects in motion and pixels of background objects (M_MOTION_DETECT_THRESHOLD). Note that in the motion detection process, MIL will automatically adjust the index of the reference frame within the group of neighboring frames for border frames, when necessary. For example, suppose you have specified a group of 3 frames to be used for motion detection, with the reference frame set to the center frame. In an input sequence of 5 images, the following image illustrates which frames are used for motion detection. The frames at the extremities of the sequence cannot be the center frame in a group of 3 frames (because they do not have an adjacent frame on both their left and right side). Therefore, MIL takes this into account and changes the reference frame to avoid these problematic situations. In the example, the first frame has no frame on its left. The reference frame will be changed to the left frame (instead of the center frame) and the motion detection will be performed using the two frames on the right. The image below is the result of the motion detection process. The white pixels represent pixels of objects in motion and the black pixels represent pixels of background objects. To generate images like this one, you use MimControl() with M_MOTION_DETECT_OUTPUT set to M_ENABLE. Note that in this case, the deinterlacing algorithm will not be performed. Deinterlacing Discard algorithm on the entire image Averaging algorithm on the entire image Bob algorithm on the entire image Adaptive algorithms ",
      "wordCount": 1262,
      "subEntries": []
    },
    {
      "id": "UG_Specialized-image_Correcting_dead_pixels",
      "version": null,
      "title": "Correcting dead pixels",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\Specialized-image\\Correcting_dead_pixels.htm",
      "text": " Correcting dead pixels If one or more pixels remains unchanged in the same place in a series of grabbed images, irresepective of the image being grabbed, the cause could be dead pixels on your camera's CCD. To replace the dead pixels on newly grabbed images with an average pixel value, taken from the neighboring pixels, use MimDeadPixelCorrection(). This function uses a dead-pixel image processing context to store processing settings. You can list the coordinates of the dead pixels, using MimPut() with M_XY_DEAD_PIXELS and two arrays (the first containing the x-coordinates, and the second containing the y-coordinates). Alternatively, create a MIL image buffer to act as a mask, where all non-zero pixels are considered dead pixels. Once specified, the coordinates of the non-zero pixels can be inquired, using MimGet() with M_XY_DEAD_PIXELS. MimDeadPixelCorrection() replaces dead pixels with an average of their non-dead pixel neighbors. Other pixels are copied from the source image buffer into the destination image buffer unchanged. The source and destination image buffer must be as large (or larger than) the dead pixel mask or equal to or larger than the largest dead pixel position of the array containing the x- and y-coordinates of the dead pixels for the source and destination, respectively. If the dead pixel mask is smaller than the source image, dead pixels on the edge of the intersection of the mask, the source image, and the destination image will only use neighboring pixels that are inside the intersection of the images when calculating the correction. Dead pixels outside the intersection are ignored. Preprocess the image processing context by calling MimDeadPixelCorrection() with M_PREPROCESS. Both the source and destination image buffers can be set to M_NULL. If, however, the source or destination image is provided, it should be a typical source or destination buffer, respectively, and it will be used in the preprocess operation, to better optimize future calls to MimDeadPixelCorrection(). If the preprocess operation was not done explictly (using M_PREPROCESS), it will be done when MimDeadPixelCorrection() is first called. Correcting dead pixels ",
      "wordCount": 337,
      "subEntries": []
    },
    {
      "id": "UG_Specialized-image_Removing_artifacts_from_grabbed_images",
      "version": null,
      "title": "Removing CCD artifacts from grabbed images",
      "subTitles": [
        "How to perform a flat-field correction",
        "Electrical bias (offset image or constant)",
        "Thermal agitation (dark image or constant)",
        "CCD sensitivity (flat image or constant)",
        "Gain"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\Specialized-image\\Removing_artifacts_from_grabbed_images.htm",
      "text": " Removing CCD artifacts from grabbed images The quality of grabbed images can be affected by electrical bias, thermal agitation, and artifacts generated by a sensitive CCD. These variations can reduce the quality of your grabbed images and cause further errors in processing. These artifacts can appear in a variety of forms, such as: a graininess in the darker areas of your image, faint horizontal or vertical lines, blotchy gradients between darker and lighter areas in your image, or even a gradient from dark to light across the entire image. Such artifacts are especially troubling in low-contrast images. One way to clean the grabbed images of these artifacts is to use MimFlatField(). This function uses a flat-field image processing context to store processing settings. You must set up the flat-field image processing context with information about the gain, thermal agitation, CCD sensitivity variations, and electrical bias, using MimControl(). You can specify each of these on a pixel-by-pixel basis using an image or, on a global level, using a constant. The flat-field image processing context is applied to the source image using the following formula. As an example of using images, this formula combines the various images of the image processing context in the following way: If you must omit one or more operands from the above formula, set the operand to a constant of either 0 or 1, depending on the operand. Note that you can also use MimFlatField() to remove non-uniform lighting from grabbed images. For more information, refer to the Removing uneven lighting from grabbed images section of Chapter 3: Fundamental image processing. How to perform a flat-field correction Before calling MimFlatField(), you must first set up the flat-field image processing context by performing the following: Allocate a flat-field image processing context, using MimAlloc() with M_FLAT_FIELD_CONTEXT. Specify the amount of electrical bias variations to remove, using MimControl() with either M_OFFSET_CONST or M_OFFSET_IMAGE. Specify the amount of thermal agitation to remove, using MimControl() with either M_DARK_CONST or M_DARK_IMAGE. Specify the amount of CCD sensitivity variations to remove, using MimControl() with either M_FLAT_CONST or M_FLAT_IMAGE. Specify the gain factor to scale the result, typically back to the full dynamic range of the destination image, using MimControl() with M_GAIN_CONST. Preprocess the context by calling MimFlatField() with M_PREPROCESS. Both the source and/or destination image buffers can be set to M_NULL. If, however, the source or destination image is provided, it should be a typical source or destination image, respectively, and it will be used in the preprocess operation, to better optimize future calls. If the preprocess operation is not done explicitly, it will be done when MimFlatField() is first called. Electrical bias (offset image or constant) Electrical bias is a level of electronic noise generated by the CCD and transferred into the grabbed image. The amount of electrical bias can differ greatly between cameras. An example of an image affected by electrical bias should resemble the following: To verify whether your image suffers from electrical bias, grab an image of a uniformly dark area (such as, grabbing with the camera's lens cap firmly in place), using a very short exposure time (compared to the standard exposure time for your application). This is referred to as an offset image. To produce a more accurate offset image, multiple offset images should be taken, and then averaged (for example, using MimStatCalculate() when MimControl() with M_STAT_MEAN is set to M_ENABLE). Each pixel of the offset image might be slightly different from the expected black value (by a small random value) and this will change from image to image; this is the electronic noise of the camera. The electrical bias will show a number of electronic problems in the camera, including: excessive noise and pattern noise. To remove the electrical bias from your image, create an offset image, as described above, and set MimControl() with M_OFFSET_IMAGE to this image. Alternatively, if you do not have an offset image or prefer to use the same value for all pixels, you can use a number instead of an image using MimControl() with M_OFFSET_CONST. Thermal agitation (dark image or constant) Thermal agitation is a level of electronic noise generated by the CCD when the free electrons in a conductor move randomly. The electronic noise is transferred into the grabbed image. The amount of thermal agitation in a grabbed image can increase as both the exposure time is increased and the CCD grows hotter. An example of an image affected by thermal agitation should resemble the following: To verify whether your image suffers from thermal agitation, grab an image of a uniformly dark area (such as, grabbing with the camera's lens cap firmly in place), using an exposure time that is standard for your application. This is referred to as a dark image. To produce a more accurate dark image, multiple dark images should be taken, and then averaged (for example, using MimStatCalculate() when MimControl() with M_STAT_MEAN is set to M_ENABLE). Dark images are a map of pixels that have a source other than light. Normally, light falls on the CCD and interacts with the silicon to free up electrons. These electrons are then moved out of the CCD and counted by the A/D converter. Besides light, there are several ways electrons can leak into the pixel without light. The dominant source of these electrons is dark current, but issues like spurious charge and residual charge can also give the pixel extra electrons. To remove the thermal agitation from your image, create a dark image, as described above, and set MimControl() with M_DARK_IMAGE to this image. Alternatively, if you do not have a dark image or prefer to use the same value for all pixels, you can use a number instead of an image using MimControl() with M_DARK_CONST. CCD sensitivity (flat image or constant) CCD sensitivity is a level of distortion exhibited by the CCD when the center of the grabbed image is more in focus (brighter, less saturated) than its outer edges, or when streaks appear from a brighter area to a nearby darker area. The distortion is transferred into the grabbed image. An example of an image affected by CCD sensitivity should resemble the following: To verify whether your image suffers from CCD sensitivity, grab an image of a uniform light gray area (such as, grabbing an image of a blank piece of paper), using a very short exposure time (compared to the standard exposure time for your application). This is referred to as a flat image. To produce a more accurate flat image, multiple flat images should be taken, and then averaged (for example, using MimStatCalculate() when MimControl() with M_STAT_MEAN is set to M_ENABLE). A flat image shows a variety of camera electronic problems, shutter problems, as well as CCD defects. Note that dust on the lens will not show up well in the flat image, but dust on the CCD or CCD defects will be very visible. To remove CCD sensitivity from your image, create a flat image, as described above, and set MimControl() with M_FLAT_IMAGE to this image. Alternatively, if you do not have a flat image or prefer to use the same value for all pixels, you can use a number instead of an image using MimControl() with M_FLAT_CONST. Gain Specifying a gain allows you to normalize (or scale) the results of the flat-field calculation, typically back to the full dynamic range of the destination image. You can only specify the gain as a single value. To establish a good gain, analyze the histogram of a grabbed image with linear luminosity. If you are uncertain as to the gain factor that would result in the best results, let MIL automatically generate the gain factor (MimControl() with M_GAIN_CONST set to M_AUTOMATIC). The automatically generated gain factor is determined by subtracting the offset image (M_OFFSET_IMAGE) from the flat image (M_FLAT_IMAGE) and then taking the average of the resulting image's pixels. If constant values are specified instead of images (by using M_FLAT_CONST and M_OFFSET_CONST), MIL returns the result of the subtraction instead. Removing CCD artifacts from grabbed images How to perform a flat-field correction Electrical bias (offset image or constant) Thermal agitation (dark image or constant) CCD sensitivity (flat image or constant) Gain ",
      "wordCount": 1366,
      "subEntries": []
    },
    {
      "id": "UG_Specialized-image_Transform_and_denoise_images_using_wavelets",
      "version": null,
      "title": "Transform and denoise images using wavelets",
      "subTitles": [
        "Wavelet context",
        "Filters",
        "Operation mode",
        "Wavelet transformations",
        "Results and drawings",
        "Wavelet denoising"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\Specialized-image\\Transform_and_denoise_images_using_wavelets.htm",
      "text": " Transform and denoise images using wavelets MimWaveletTransform() uses wavelets to decompose or recompose the specified source (typically image data). To remove noise, MimWaveletDenoise() also uses wavelets. Wavelets refer to mathematical algorithms that process data in both the space and frequency domain, offering a compromise between each. By using wavelets, MimWaveletTransform() results can therefore represent space and frequency. This makes MimWaveletTransform() especially effective at filtering images. It is typically useful for signal coding, signal denoising, data compression, and pattern recognition. Transformations conventionally represent either space or frequency data. For example, calling MimTransform() with an image can produce a precise frequency result, but it lacks spatial information. Drawing such a result looks like a signal, with no resemblance to the original source image. Drawing a wavelet transformation result resembles the original source image, since some spatial information is kept. The following illustration shows a MimWaveletTransform() and a MimTransform() (FFT) result of a source image representing several lead stripes. You can tell which is the wavelet transformation, since it resembles the source. Numerous types of wavelet results are available. The one above represents a high-frequency wavelet transformation (vertical direction). For more information about wavelet results, see the Results and drawings subsection of this section. Note that the uncertainty principal in mathematics proves that representing data with the same precision in both space and frequency impossible. Wavelets are recognized as offering an effective balance between these two domains. Wavelet context Both MimWaveletTransform() and MimWaveletDenoise() require a wavelet image processing context, which you must allocate by calling MimAlloc() with M_WAVELET_TRANSFORM_CONTEXT or M_WAVELET_TRANSFORM_CUSTOM_CONTEXT. With M_WAVELET_TRANSFORM_CONTEXT, you have access to predefined wavelet filters. With M_WAVELET_TRANSFORM_CUSTOM_CONTEXT, you must provide your own wavelet filters. Information about wavelets applies to both contexts unless otherwise specified. The wavelet context stores the filter and mode used to perform MimWaveletTransform() and MimWaveletDenoise(). To inquire about what is stored in a wavelet context, call MimInquire(). For example, you can inquire whether the context's filter uses complex numbers (M_TRANSFORMATION_DOMAIN), as well as retrieve the identifier of the internal buffer that contains the actual filter values (M_FILTER_..._ID). Before calling a wavelet operation, you should also allocate a wavelet result buffer, using MimAllocResult() with M_WAVELET_TRANSFORM_RESULT. Remember to free all image processing buffers with MimFree(). Filters To set a predefined wavelet filter (only for M_WAVELET_TRANSFORM_CONTEXT), call MimControl() with M_WAVELET_TYPE. The default is M_HAAR. This typically applies when transforming or denoising images with distinct intensity transitions. Other filter types are categorized as Daubechies (generally appropriate for intensity discontinuities) or Symmetry (a mathematically more symmetric version of Daubechies), and the number of precision points (vanishing moments) the filter employs (for example, M_DAUBECHIES_3). More vanishing moments typically result in the representation of more complex features, and longer processing time. A further distinction between filters is whether they use complex or real coefficients. The mathematical domain of complex filters (for example, M_DAUBECHIES_3_COMPLEX) consists of wavelet coefficients that have a real part (real numbers) and an imaginary part (imaginary numbers). The mathematical domain of filters that are not complex consist of wavelet coefficients that have real numbers only. Results that include imaginary numbers are generally longer to calculate, but have more information (representing the signal's amplitude and phase). To set custom wavelet filters (only for M_WAVELET_TRANSFORM_CUSTOM_CONTEXT), call MimWaveletSetFilter() and specify the filter values. The custom wavelet context must contain your filter values before calling MimWaveletTransform() or MimWaveletDenoise(). The custom filters that you specify must be separable 2D wavelets that are factorized in their 1D form. For more information, see \"Stéphane Mallat. A Wavelet Tour of Signal Processing . USA: Academic Press, 2008. \". When calling the wavelet function (MimWaveletTransform() or MimWaveletDenoise()), you must set the number of levels (iterations) with which to apply the filter, up to an internally established maximum. This is the level at which calculations at subsequent levels do not result in relevant data. The maximum level depends on the wavelet function, the source, and the wavelet image processing context. If the specified level exceeds the maximum level, MIL uses the maximum level. You can determine the actual number of levels used to produce wavelet results by calling MimGetResult() with M_NUMBER_OF_LEVELS. For more information on wavelet filters, see \"Christopher Torrence and Gilbert P. Compo. A Practical Guide to Wavelet Analysis . USA: AMS, 1998. \". Operation mode MIL uses filters in dyadic (default) or undecimated mode. In dyadic mode, processing involves sampling the wavelet coefficients by a factor of 2 at each level. For example, when drawing dyadic results, they are at different sizes, at different levels. Undecimated processing does not sample. The filter values themselves are adjusted. When drawing undecimated results, they are at the same size regardless of level. Dyadic generally applies to signal coding and data compression. Undecimated applies more to signal denoising and pattern recognition. To modify the mode, set the M_TRANSFORMATION_MODE control to M_DYADIC or M_UNDECIMATED. Performing MimWaveletDenoise() with M_DYADIC is typically faster than M_UNDECIMATED, though the quality of denoising is usually higher with M_UNDECIMATED. In general, M_DYADIC uses less resources (processing time and memory) than M_UNDECIMATED. For more information about dyadic and undecimated wavelet modes, see \"Stéphane Mallat. A Wavelet Tour of Signal Processing . USA: Academic Press, 2008. \". Wavelet transformations MimWaveletTransform() can perform a forward (M_FORWARD) or reverse (M_REVERSE) wavelet transformation on the specified source. Results are written in the specified destination. A forward transformation decomposes the source, which can be an image or wavelet result; the destination must be a wavelet result. Reverse transformations recompose the source, which must be a wavelet result; the destination can be an image or wavelet result. The basic methodology for performing wavelet transformations typically requires calling MimWaveletTransform() with an image and M_FORWARD, and then using the result with your own specialized operations, typically related to signal coding, signal denoising, data compression, and pattern recognition. The last step is to transform those results using M_REVERSE and produce the modified image. Since you must use M_REVERSE with a wavelet result (not an image), you must have at some previous point used M_FORWARD with an image to produce a result. Results and drawings To retrieve general types of wavelet results, call MimGetResult(). For example, to determine the actual number of transformation levels used, get the M_NUMBER_OF_LEVELS result. To retrieve information about a specific wavelet result, use MimGetResultSingle(). To identify the individual result, you must specify the resulting wavelet coefficient and transformation level with M_DIAGONAL_LEVEL(Level), M_HORIZONTAL_LEVEL(Level), or M_VERTICAL_LEVEL(Level). These results represent the image separated into its high-frequency components at a specific level and oriented along a specific direction. For example, if you specify M_DIAGONAL_LEVEL(3) with the M_WAVELET_COEFFICIENTS_IMAGE_ID result, you will retrieve the identifier of the internal image buffer that holds the high-frequency results oriented along the diagonal plane at the third level of transformation. You can also call MimGetResultSingle() with M_APPROXIMATION to retrieve results about the approximation (the low frequency rendition) of the wavelet transformation at the last calculated level. To perform drawing operations with wavelet results, call MimDraw(). The entire content of the wavelet result is drawn. For dyadic modes (MimControl() with M_TRANSFORMATION_MODE set to M_DYADIC), drawings are in the top-right (vertical coefficient), bottom-right (diagonal coefficient), and bottom-left (horizontal coefficient) corners of the display. This drawing pattern repeats for each level calculated (MimGetResult() with M_NUMBER_OF_LEVELS). Since dyadic transformations sample wavelet coefficients by a factor of 2 per level, drawings are resized at each level. MIL also draws the approximation (the low frequency rendition) of the wavelet transformation at the last level, in the top-left corner of the display. The following is an example of drawing dyadic results (three level transformation). For undecimated modes (M_UNDECIMATED), drawings are in one row, per level. Each row is split into three columns, representing the horizontal (left column), diagonal (middle column), and vertical (right column) wavelet coefficients for that level. Since undecimated transformations are not sampled, drawings are all the same size, regardless of level. MIL also draws the approximation (the low frequency rendition) of the wavelet transformation at the last level, in the first column of the first row. The middle and right columns in this row are blank. The following is an example of drawing undecimated results (three level transformation). Depending on the specifics of your application, such as the source and destination image, context type, transformation mode, and filter type, calculations can require MIL to internally add padding data to the image's border. MimDraw() allows you to draw without (M_DRAW_WAVELET) or with (M_DRAW_WAVELET_WITH_PADDING) this padding. You can also retrieve related results with or without padding. For example, to get the width required to draw wavelet results, you can call MimGetResult() with M_WAVELET_DRAW_SIZE_X or M_WAVELET_DRAW_SIZE_X_WITH_PADDING. The following example performs a wavelet transformation and then displays the resulting wavelet transforms. wavelettransformation.cpp Wavelet denoising MimWaveletDenoise() uses wavelet denoising techniques to remove noise from the specified source, which must be an image or wavelet result. If the source is an image, MimWaveletDenoise() produces the actual denoised image in the specified destination. Since the destination is an image (not a result), you cannot perform result type operations with it, such as drawing the calculated wavelet coefficients using MimDraw(). If the source is a result, it must come from MimWaveletTransform(). The specified destination must also be a result, allocated using MimAllocResult() with M_WAVELET_TRANSFORM_RESULT. As previously discussed, the denoising process uses the filter type and mode indicated in the specified wavelet context. Specifically, MIL decomposes the image according to the specified number of levels, performs the denoising, and recomposes the image using the same number of levels. The maximum level depends on the source image and the wavelet image processing context. When calling MimWaveletDenoise(), you must also specify a specific wavelet shrinkage process: M_BAYES_SHRINK, M_NEIGH_SHRINK, or M_SURE_SHRINK. M_BAYES_SHRINK is good at minimizing Gaussian noise. For minimizing Mean Square Errors (MSE), M_SURE_SHRINK is better. When noise is difficult to characterize, try NeighShrink. This setting mimics the general behavior of wavelet coefficients by taking neighborhood statistics and dismissing outliers. Though NeighShrink is less precise than the others, it can yield the best results under uncertain conditions. NeighShrink is usually the fastest setting. If you are having difficulty denoising with MimWaveletDenoise(), try using some of MIL's other denoising techniques, such as spatial filtering (in particular for removing salt-and-pepper noise). For more information, see the Denoise using spatial filtering and area open and close operations section of Chapter 3: Fundamental image processing. The following illustration shows a member of the Periphrastic family of near passenger birds (that is, a Toucan). The first image has no noise, while the second has white Gaussian noise. The first two images below show the removal of noise using median ranking and smoothing. The third image shows the removal of noise using MimWaveletDenoise() with M_BAYES_SHRINK. To remove Gaussian noise, a BayesShrink wavelet process proves most effective. For more information about wavelet denoising techniques, see \"Rohit Sihag, Rakesh Sharma, and Varun Setia. Wavelet Thresholding for Image DE-noising . USA: IJCA Proceedings on International Conference on VLSI, Communications and Instrumentation (ICVCI) (14):20–24, 2011. pp. 20-24.\" and \"David L. Donoho and Iain M. Johnstone. Adapting to Unknown Smoothness via Wavelet Shrinkage . USA: Journal of the American Statistical Association, Vol. 90, No. 432, 1995. pp. 1200-1224.\". The following example performs a wavelet denoising, and compares it with other conventional denoising operations, such as rank and spatial filtering. variousdenoising.cpp Transform and denoise images using wavelets Wavelet context Filters Operation mode Wavelet transformations Results and drawings Wavelet denoising ",
      "wordCount": 1889,
      "subEntries": []
    },
    {
      "id": "UG_Specialized-image_Co_occurrence_matrix_statistics",
      "version": null,
      "title": "Co-occurrence matrix statistics",
      "subTitles": [
        "Building the grayscale co-occurrence matrix",
        "Tiling your image",
        "Supported co-occurrence statistics",
        "A co-occurrence example",
        "Limiting the bit depth of your image"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\Specialized-image\\Co_occurrence_matrix_statistics.htm",
      "text": " Co-occurrence matrix statistics To establish whether two grayscale images contain a similar texture, you can compare their co-occurrence matrix statistics. You use these statistics to characterize the texture; you use them to provide a signature for a given texture. Traditionally, these second-order statistics are used to discern differences in the patterns (textures) in small images (such as tears in fabric). Taking the same statistics from a large series of images allows you to see trends that characterize the pattern/texture. With MIL, you can compute the co-occurrence matrix of a grayscale image and compute statistics from this matrix, using MimStatCalculate() with an M_STATISTICS_CONTEXT context that has some M_STAT_GLCM... enabled (MimControl()). The co-occurrence matrix is a normalized version of the frequency distribution of two grayscale values occurring at a specified pixel offset from each other. A co-occurrence matrix (and its corresponding statistics) can be calculated for the entire image or for the neighborhood of every n pixels. Building the grayscale co-occurrence matrix MIL builds the grayscale co-occurrence matrix (GLCM) for the specified pixel-pair offset, set using MimControl() with M_GLCM_PAIR_OFFSET_X and M_GLCM_PAIR_OFFSET_Y. It establishes how often each possible combination of two grayscale values occur at this offset from each other, regardless of the order of the values. For example, to evaluate the frequency that two grayscale values occur next to each other in X, set M_GLCM_PAIR_OFFSET_X to 1 and M_GLCM_PAIR_OFFSET_Y to 0. If one pixel pair is (3,0), and another pixel pair is (0,3), both are recorded in the co-occurrence matrix as (0,3). To establish the pair of a pixel on the border, mirror overscan is used. The following table shows an example of how a source image is used to build a co-occurrence matrix. Original source image Building the co-occurrence matrix for a pixel pair offset of (X + 1,Y + 0) The co-occurrence matrix showing the frequency of each pair of values with an offset of (X + 1,Y + 0) Position (X,Y) Position of paired pixel Values of paired pixels (0,0) (1,0) (3,0) 1 (1,0) (2,0) (0,1) (2,0) (3,0) (1,1) (3,0) (3,0) 2 (1,1) (0,1) (1,1) (0,4) (1,1) (2,1) (4,2) 1 (2,1) (3,1) (2,0) 1 (3,1) (3,1) (0,0) (0,2) (1,2) (2,3) (1,2) (2,2) (3,4) (2,2) (3,2) (4,1) 1 (3,2) (3,2) 2 (1,1) (0,3) (1,3) (0,3) (1,3) (2,3) (3,1) 1 (2,3) (3,3) (1,2) (3,3) (2,3) 2 (2,2) 1 In the case where the first digit is higher than the second in a pixel pair (for example, (3,0)), the frequency of this pixel pair is counted such that the first digit is always lower than the second (so, a pixel pair of (3,0) is recorded as (0,3) in the co-occurrence matrix). 2 In this case, mirror overscan is used for pixel pairs that fall outside of the source image, if using the entire image. The last pixel of each row and column is mirrored. Tiling your image Using co-occurrence statistics to discern discrepancies works best on small areas. As such, MIL allows you to calculate the co-occurrence matrix (and corresponding statistics) for the neighborhood of every n th pixel. Each of these neighborhoods would then have their own co-occurrence statistic results available for comparison. Each neighborhood is considered a tile. Set the tile size using MimControl() with M_TILE_SIZE_X and M_TILE_SIZE_Y. Set the distance between the tiles (step) using M_STEP_SIZE_X and M_STEP_SIZE_Y. If the step size and tile size force tiles to overlap, they will. If some part of these tiles falls outside the image buffer, buffer overscan is used to make up the difference. It uses transparent overscan if the buffer is a child buffer and there are underlying parent buffer pixels; otherwise it uses mirror overscan. MIL tries to equalize the number of columns or rows of overscan. For example, if 2 columns of overscan would be required, one will be placed before the first column, and one after the last column. This can result in the first pixel being an overscan pixel. If the image buffer has an overscan region, it is used. If the overscan region is too small to fit the overflow, MIL will internally extend the image buffer's overscan region. The 8 x 7 image below is divided into six 4x4 tiles that overlap in X by 1 column. To configure this example, the tile size X and Y are set to 4 (M_TILE_SIZE_X, M_TILE_SIZE_Y). The step size is set to (3,4) ( M_STEP_SIZE_X, M_STEP_SIZE_Y). Supported co-occurrence statistics MIL supports the following six different co-occurrence matrix statistics. Note that, in the following calculations, i represents the grayscale value of the first pixel, while j represents the grayscale value of the paired pixel. Pij represents the value of the normalized co-occurrence matrix of the given pixel pair. Mu i (µi ) represents the weighted average index of the row (i) and Mu j (µj ) represents the weighted average index of the column (j). Sigma i (oi ) represents the standard deviation of the row and Sigma j (oj ) represents the standard deviation of the column. To enable a co-occurrence matrix statistic, use MimControl() with M_STAT_GLCM... set to M_ENABLE. To perform the operation, call MimStatCalculate(). Contrast (M_STAT_GLCM_CONTRAST). A measure of the difference between paired pixel values, whereby the difference is squared so that the calculated value grows more rapidly the further paired pixel values are from the diagonal of the co-occurrence matrix (the more the values of paired pixels are different from each other). Dissimilarity (M_STAT_GLCM_DISSIMILARITY). A measure very similar to contrast except that the distances are not squared so that the calculated value grows linearly the further pairs of pixel values are from the diagonal of the co-occurrence matrix. Pearson's coefficient correlation (M_STAT_GLCM_CORRELATION). A measures of how close your data comes to forming a straight line in the co-occurrence matrix. The line's orientation (diagonal or not) is not specified but the result is dependent on the slope of the line. Energy (M_STAT_GLCM_ENERGY). A measure of the concentration of the same pixel pairs occurring in the target area. This value is high when the co-occurrence matrix has few entries of large magnitude. Entropy (M_STAT_GLCM_ENTROPY). A measure of the irregularity of the same pixel pairs occurring in the target area. The calculated entropy value is highest when the values of the co-occurrence matrix are quite uniformly distributed through the matrix. This occurs when the area has no pairs of pixels that occur more often than others. Homogeneity (M_STAT_GLCM_HOMOGENEITY). A measure of how similar are the values of paired pixels (how close to the diagonal are the values of paired pixels to the diagonal of the co-occurrence matrix). Homogeneity is the inverse of contrast. Homogeneity is a value between 0 and 1. A co-occurrence example The following example calculates several co-occurrence statistics for two images of similar pieces of textile. The results are returned and drawn for comparison. The two different images produce slightly different co-occurrence results that are visible, when drawn: Description Fabric without defects Fabric with defects Source images Drawn version of the co-occurrence matrix Statistical results Energy: 0.025895 Contrast: 251.493018 Entropy: 7.865064 Dissimilarity: 11.860745 Homogeneity: 0.092092 Correlation: 0.938123 Energy: 0.024421 Contrast: 305.347826 Entropy: 7.996176 Dissimilarity: 13.070599 Homogeneity: 0.084361 Correlation: 0.944868 Limiting the bit depth of your image The co-occurrence statistic operations require that your source image is at most 10 bits deep. When performing a co-occurrence statistics operation using a 32-bit buffer, MIL rescales the pixel values of the source image to fit within an unsigned integer buffer. That resulting buffer is then bit-shifted to fit within the bit-depth limitation, as set by M_GLCM_QUANTIFICATION. In the case where the source image was grabbed into a buffer with a greater bit-depth (such as, grabbing a 10-bit image and storing it in a 16-bit buffer), use MbufControl() with M_MAX and M_MIN to limit the maximum and minimum pixel values of the buffer. For example, a 10-bit image, stored in a 16-bit buffer, will be a 10-bit image with the additional bits being zero or sign-extended. Due to the positioning of the zero or sign-extended bits within the 16-bit image (specifically the 6 MSB are set to zero, for an unsigned buffer, or -1, for a signed buffer), your co-occurrence matrix would end up using these zero or the sign-extended bits in its calculations. To avoid this situation, set M_MAX to 1023 (which is 2 10 -1) and set M_MIN to 0. This results in the zero or sign-extended bits being ignored, and the preceding MSB being used to build the co-occurrence matrix. Co-occurrence matrix statistics Building the grayscale co-occurrence matrix Tiling your image Supported co-occurrence statistics A co-occurrence example Limiting the bit depth of your image ",
      "wordCount": 1430,
      "subEntries": []
    },
    {
      "id": "UG_Specialized-image_Fast_Fourier_Transform",
      "version": null,
      "title": "Fast Fourier Transform",
      "subTitles": [
        "Magnitude and phase",
        "Filtering an image"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\Specialized-image\\Fast_Fourier_Transform.htm",
      "text": " Fast Fourier Transform A Fast Fourier Transform (FFT) is used to identify any consistent spatial patterns (of frequency) in an image (which can be caused, for example, by systematic noise). You can perform one or two dimensional FFTs using MimTransform(). For 1D transforms, each row or column is treated as a 1D signal. This type of transform separates a one dimensional signal into a set of sine and cosine waves of different frequencies. For a two-dimensional signal (image), it can be interpreted as the decomposition of an image into a set of 2D patterns. The composition of these waves make up the original waveform. The forward Fourier transform is defined as: Where u and v are coordinates in the frequency domain and x and y are coordinates in the spatial domain. A forward FFT yields a real (R) and an imaginary (I) component of the image in a frequency domain (spectrum). The reverse Fourier transform is defined as: where x and y are the coordinates in the spatial domain and u and v are coordinates in the frequency domain. Magnitude and phase For a more visual understanding of the FFT results, you can calculate the phase and magnitude, using the MimTransform() function with M_MAGNITUDE. The magnitude is calculated as: where R and I are the real and imaginary components of the image, respectively. The following figures show single-frequency images and their centered magnitude. Since single-frequency images contain only one spatial frequency component, their corresponding frequency images appear as a single point of brightness with their associated negative-frequency mirrors. Note that the points in the frequency domain appear in the direction of the pattern. The distance between the points and the DC component represents the frequency of the pattern. The DC component appears at the center of the image when M_CENTER is used. The spatial shift of each pattern in the image, in degrees, is called the phase. It is calculated using the formula atan(I/R). You can obtain the phase using the M_PHASE setting. Typically, as is the case in the previous example, M_CENTER is used to center the real and imaginary components of the image in the frequency domain by repositioning their respective buffers' quadrants. The DC component is positioned at (SizeX/2-1, SizeY/2-1), where SizeX and SizeY are the width and height of the source buffer, respectively. If M_CENTER is not specified, the DC component appears in the upper left corner of the image. The following example illustrates the repositioning of a buffer's quadrants when M_CENTER is specified: Filtering an image To filter constant spatial patterns using FFTs: Perform a forward FFT (M_FORWARD), calculating the magnitude (M_MAGNITUDE) of the image. Scale the image within a displayable range using M_LOG_SCALE (this applies the formula, clog[1+|F(u,v)|]). Find the frequency components representing the noise and design a mask image to remove these components. To design the mask image, clear the mask to a non-zero value, and then set the unwanted frequencies to 0, or vice versa. Perform a simple FFT to obtain the real and imaginary components of the image, this time without calculating the magnitude. Perform an MbufCopyCond() operation or an MbufClearCond() operation on both the imaginary and real parts of the image, with the mask as the condition buffer. This will remove the unwanted frequencies. Finally, perform a reverse transform to obtain a filtered image. If you know the frequency of the noise pattern and have designed the mask, you need only perform steps 3 to 5. The following example performs an FFT on an image with a vertical noise pattern. A forward transform is performed to obtain the real and imaginary components of the image. The values of locations corresponding to the noise pattern are set to 0. Finally, a reverse transform is performed to obtain a spatial image without the noise pattern. mimfft.cpp Fast Fourier Transform Magnitude and phase Filtering an image ",
      "wordCount": 645,
      "subEntries": []
    },
    {
      "id": "UG_Specialized-image_Discrete_Cosine_Transform",
      "version": null,
      "title": "Discrete Cosine Transform",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\Specialized-image\\Discrete_Cosine_Transform.htm",
      "text": " Discrete Cosine Transform Discrete Cosine Transform (DCT) is mainly used for image JPEG lossy compression. MIL can perform DCT using MimTransform(). For a one dimensional signal, this separates the signal into a set of cosine waves of different frequency. For a two-dimensional signal (image), it can be interpreted as the decomposition of an image into a set of 2D cosine patterns. The composition of these waves make up the original waveform. The forward DCT is defined as (for an 8x8 matrix): where u and v are coordinates in the frequency domain. The reverse DCT is defined as: where x and y are coordinates in the spatial domain. Frequency 0, also called the DC component, is plotted in the top-left corner of the spectrum. All other components in the spectrum are called AC components. A DCT concentrates the low frequency components of the image in the first few coefficients (top-left corner) of the spectrum. MIL divides the image into independent blocks of 8x8 pixels and performs the transform on each individual block. Centering of the spectrum is not supported in MIL. Discrete Cosine Transform ",
      "wordCount": 185,
      "subEntries": []
    },
    {
      "id": "UG_Specialized-image_Peak_detection_and_depth_maps",
      "version": null,
      "title": "Peak intensity detection and depth maps",
      "subTitles": [
        "Setting up peak intensity detection",
        "Generating an uncorrected depth map",
        "Multiple peaks in a single lane",
        "Extraction of peaks from a multi-frame buffer"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\Specialized-image\\Peak_detection_and_depth_maps.htm",
      "text": " Peak intensity detection and depth maps It is sometimes useful to find the position of the highest intensity in every lane (row or column) of an image. MimLocatePeak1d() performs very fast and efficient peak intensity detection on each lane of pixels in a source image. The peak is the 1D neighborhood of pixels that have the highest intensity in the lane. Once the function finds the peak neighborhood of the lane, it records the sub-pixel position of the peak intensity, and the average intensity around that sub-pixel position. You can, for example, create a structured light application using MimLocatePeak1d() with images from a laser line profiling setup, to establish depth and surface information for an object in a scene. A basic laser line profiling setup consists of: a device projecting a sheet of light (usually a laser diode), a camera, and a mechanism to move the object under the laser plane. The camera is used to grab an image of the intersection of the laser plane with the object. From the way that the laser line deforms when striking the object's surfaces, depth and surface information can be established for that slice of the object. By grabbing a sequence of images as the object moves underneath the laser plane, and then processing the images with MimLocatePeak1d() and MimDraw(), you can generate an uncorrected depth map of the exposed topography of the object. An uncorrected depth map is an image where the gray value of a pixel represents its depth in the world, although the depth is not its actual world depth and its shape is not corrected for the angle of the camera. If your application requires accurate depth or shape information, see the Generating fully corrected depth and intensity maps section of Chapter 35: 3D image processing for more information on how to create a depth map. Setting up peak intensity detection MimLocatePeak1d() requires its own context (MimAlloc() with M_LOCATE_PEAK_1D_CONTEXT) and its own result buffer (MimAllocResult() with M_LOCATE_PEAK_1D_RESULT), as well as a source image from which to locate the peak intensities. To understand how to set the function to optimally find the peak in each lane, you should understand how the function works. The function scans each lane and records each lane's peak position and intensity. The peak's position is the sub-pixel distance from the origin of the lane to the sub-pixel point with the highest intensity in the peak. The peak's intensity is the average gray value of the few pixels around the sub-pixel position of the peak. Peaks are a 1D neighborhood of pixels, despite having a sub-pixel position. In a laser line image, the peak is typically the width of the laser line, in pixels. Peaks have a configurable range of acceptable widths; you can specify the average width of the laser line (nominal width) and how much that average width can vary (delta width). The range is: nominal-delta &lt;= peak width &lt;= nominal+delta. When looking for a peak in a lane, MimLocatePeak1d() focuses on the contrast between the intensity of the pixels of the local background and the intensity of the pixels of the suspected peak; you can specify the minimum contrast. For example, if the local background intensity is 80, and you want the minimum intensity in your peak neighborhood to be 230, you would specify the minimum contrast to be 150. Once a peak is suspected, the number of continuous pixels that fulfill the contrast condition are counted. Provided the total number of pixels that fulfill the contrast condition is within the range of allowable peak widths, the peak is recorded. The position of the peak is the sub-pixel position of the highest intensity within the peak width. The intensity of the peak is the average intensity of a few pixels around the peak position; you can specify how many pixels are used to calculate the peak intensity using MimControl() with M_PEAK_INTENSITY_RANGE. Note that the peak position is not always at the center of the neighborhood, as can be seen in the following zoomed image. Also note that the peak intensity is not exclusively determined by the pixels in the peak width and can include some background pixels. Typically, you set the values for the nominal width, the delta width, and the minimum contrast when you call MimLocatePeak1d(). However, you can set the parameters to M_DEFAULT and set the values for the nominal width, the delta width, and the minimum contrast using MimControl() with M_PEAK_WIDTH_NOMINAL, M_PEAK_WIDTH_DELTA, and M_MINIMUM_CONTRAST, respectively. Note that by default, MimLocatePeak1d() scans vertical lanes (columns) in the source image, storing the peak position as the Y-coordinate; this is useful for detecting horizontal laser lines. However, you can specify that MimLocatePeak1d() scans horizontal lanes (rows), using MimControl() with M_SCAN_LANE_DIRECTION set to M_HORIZONTAL, for example, to look for a vertical laser line. In this case, the peak position stored in the result buffer will be the X-coordinate, as shown in the image below: For optimal performance, preprocess the result buffer by calling MimLocatePeak1d() with M_PREPROCESS. You can choose to preprocess with or without an image. When preprocessing with an image, specify a typical image in the series of images that you will process using MimLocatePeak1d(). The preprocessed image's size and bit-depth are stored. When preprocessing without an image, you must specify the number of scan lanes in each of the images in the series. When preprocessing without an image, it is assumed that all images are 8-bit unsigned. If the preprocess operation is not done explicitly (using M_PREPROCESS) or correctly, MimLocatePeak1d() will preprocess internally when it is first called. If a new image in the series has a different size or bit-depth than the image used to preprocess, MimLocatePeak1d() will preprocess the result buffer again, given the new image's characteristics. Generating an uncorrected depth map Once you have called MimLocatePeak1d() on a laser line image, you can generate an uncorrected depth map or intensity map by calling MimDraw() with the locate peak 1D result buffer and an appropriate image buffer. To generate a depth map or intensity map of an object, you need to iterate through this process for each laser line image (slice of the object) that you grab. The following demonstrates this iterative process: Allocate an image buffer, using MbufAlloc2d(). This will be used for the depth map or intensity map. The width and height of the image buffer have constraints. The image buffer must have a width greater than or equal to the number of lanes (laser line image width or height, depending on whether the image has a horizontal or vertical laser line, respectively). This is because MimDraw() draws the results obtained for each lane in a laser line image into a single row of the depth map or intensity map image buffer, regardless of the orientation. The image buffer must have a height greater than or equal to the expected number of laser line images of the object that you will grab. Results from each grabbed image will be drawn in a new row of the depth map or intensity map image buffer. Iterate through the following steps for each image in the series of laser line images. Grab a new laser line image. Call MimLocatePeak1d() on the laser line image. Call MimDraw() with the locate peak 1D result buffer and the allocated depth map or intensity map image buffer, incrementing the row of the depth map and/or intensity map image buffer in which to write. In addition, pass M_DRAW_DEPTH_MAP_ROW or M_DRAW_INTENSITY_MAP_ROW, depending on whether you want to draw the depth map or intensity map, respectively. To draw both, you must call MimDraw() twice in each loop. The following image shows an uncorrected depth map and intensity map after one iteration: Note that instead of drawing results for each laser line after every grab, you can accumulate results in the result buffer, and then draw multiple rows in the destination image buffer by calling MimDraw() with Param2 set to M_ALL. In this case, you must set MimControl() with M_NUMBER_OF_FRAMES to the maximum number of frames (laser line images) for which results can be accumulated in the specified result buffer. When calling MimLocatePeak1d(), set M_FRAME_INDEX to the appropriate index in the result buffer which corresponds to the frame for which to store results. The following animation illustrates the uncorrected depth map of an entire series of grabbed images. Note that in some images, a peak might not be detected in all lanes. The default depth and intensity for missing peaks, M_INVALID, corresponds to the value -1 (or an unsigned buffer's maximum value). You can change the intensity value for missing peaks using MimControl() with M_PEAK_INTENSITY_INVALID_VALUE. The following image shows a single grabbed laser line image on the left, and the uncorrected depth map and intensity map that results from processing the entire series of grabbed images on the right: Note that the Y-size of the depth and intensity maps is determined by the number of laser line images in the sequence. In this example, the hand moved approximately 2 pixels per image capture so the Y-size of the depth and intensity maps is half of the source images. Multiple peaks in a single lane It is possible that there are multiple peaks in a lane which fit the specified requirements for a peak. This can occur when there is noise in the image, but frequently occurs due to reflections on the object or the background surface. To address this case, MimLocatePeak1d() can find and store peak information for several of the best peaks in each lane, using MimControl() with M_NUMBER_OF_PEAKS. After this control type is set, calling MimLocatePeak1d() establishes and stores the position and intensity for the specified number of peaks in each lane in the image. By analyzing the 1D locate peak result buffer, you can create a process that determines the most relevant peaks for the image. To retrieve this peak information from the result buffer, call MimGetResultSingle(). You can specify to retrieve the peak information for one or all peaks in one or all lanes. The most common combination is to retrieve the peak information for each peak with a given index across all lanes. Typically, this is used to get an array of the best peaks (the first peak is the peak with the highest intensity in the lane, by default) in each lane. You can specify how the multiple peaks are initially sorted in the result buffer using MimControl() with M_SORT_CRITERION. By default, the peaks are sorted by descending intensities, which means that the peak with the highest intensity, which is frequently the best peak, has an index of 0. For instance, you can retrieve the positions of the highest intensity peaks in each lane of an image, using MimGetResultSingle() with M_PEAK_POSITION. With this information, you can create a process that identifies aberrant peaks. You could then retrieve the second best peak in the lanes that have aberrant peaks to create an array with only the correct peaks for the image. Extraction of peaks from a multi-frame buffer MimLocatePeak1d() supports the extraction of peaks from a multi-frame image buffer. A multi-frame image buffer is one that is composed of multiple adjacent frames, typically grabbed using frame burst technology. For information on multi-frame image buffers, see the Specifying the dimensions of a multi-frame image buffer subsection of the Specifying the dimensions of a data buffer section of Chapter 23: Data buffers. To use a multi-frame image buffer with MimLocatePeak1d(), you must set the Y-size of each individual frame using MimControl() with M_FRAME_SIZE, and set MimControl() with M_NUMBER_OF_FRAMES to the maximum number of frames for which results can be accumulated in the result buffer. Note that the total number of frames (M_NUMBER_OF_FRAMES) can be different from the frame capacity of the source image buffer. For example, if the source image buffer has a 5-frame capacity and you have 10 total frames to process, 2 calls to MimLocatePeak1d() can be done before calling MimGetResultSingle(). Typically, you would set M_FRAME_INDEX to frame 0 on the first call to MimLocatePeak1d(); then, set it to frame 5 on the second call. Finally, it is important to set M_FRAME_INDEX such that the result buffer can hold all results produced from the call. That is, there must be enough indices available in the result buffer to accommodate results from all frames in the source image buffer. The following code snippet shows how to extract peaks from a multi-frame image buffer. /* Locate peak 1d context allocation. */ MIL_ID LocatePeakContextId = MimAlloc(MilSystemId, M_LOCATE_PEAK_1D_CONTEXT, M_DEFAULT, M_NULL); /* Locate peak 1d result allocation. */ MIL_ID LocatePeakResultId = MimAllocResult(MilSystemId, M_DEFAULT, M_LOCATE_PEAK_1D_RESULT, M_NULL); /**/ /* Set the maximum number of frames to store in the result. */ MimControl(LocatePeakContextId, M_NUMBER_OF_FRAMES, 100); /* Set the Y-size of each frame in the image. */ MimControl(LocatePeakContextId, M_FRAME_SIZE, 32); /* Set the minimum contrast for extraction. */ MimControl(LocatePeakContextId, M_MINIMUM_CONTRAST, 50); /* Set the nominal peak width for extraction. */ MimControl(LocatePeakContextId, M_PEAK_WIDTH_NOMINAL, 10); /* Set the peak width delta for extraction. */ MimControl(LocatePeakContextId, M_PEAK_WIDTH_DELTA, 10); /* Set the maximum number of peaks per scan lane to extract per frame. */ MimControl(LocatePeakContextId, M_NUMBER_OF_PEAKS, 2); /* Set the direction of the extraction process. */ MimControl(LocatePeakContextId, M_SCAN_LANE_DIRECTION, M_VERTICAL); /**/ /* Set the sort parameters for the extracted peaks to strongest peak first. */ MimControl(LocatePeakResultId, M_SORT_CRITERION, M_PEAK_INTENSITY + M_SORT_DOWN); /**/ /* Perform preprocessing. */ MimLocatePeak1d(LocatePeakContextId, SrcId, LocatePeakResultId, M_NULL, M_NULL, M_NULL, M_PREPROCESS, M_DEFAULT); while (Processing) { /**/ /* Perform peak extraction on multiple frames.*/ MimLocatePeak1d(LocatePeakContextId, SrcId, LocatePeakResultId, M_DEFAULT, M_DEFAULT, M_DEFAULT, M_DEFAULT, M_DEFAULT); /* Extract the best peaks in all scan lanes of all frames. */ MimGetResultSingle(LocatePeakResultId, M_SELECT_PEAK(M_ALL, 0), M_ALL, ResultType, UserPtr); /**/ } /**/ /* Release the locate peak 1d resources. */ MimFree(LocatePeakResultId); MimFree(LocatePeakContextId); Peak intensity detection and depth maps Setting up peak intensity detection Generating an uncorrected depth map Multiple peaks in a single lane Extraction of peaks from a multi-frame buffer ",
      "wordCount": 2308,
      "subEntries": []
    },
    {
      "id": "UG_Specialized-image_Augmentation",
      "version": null,
      "title": "Augmentation",
      "subTitles": [
        "Steps to augment an image",
        "Priority",
        "Probability",
        "Randomness and seeds",
        "Random rotation angle"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\Specialized-image\\Augmentation.htm",
      "text": " Augmentation Augmentation allows you to create a plausible variation of an image. This is done by applying a specified ordered set of image processing operations with randomized settings within a specified range. For example, to plausibly mimic the different kinds of images a camera might capture of an object passing under it, MimAugment() can randomly rotate, translate, and blur an image. This lets you generate the images that a camera can grab without the camera actually grabbing them. Augmentation can prove useful if you want to add a variety of modified images with which to perform test procedures, or if you want to increase the entries in your dataset when performing machine learning tasks, such as image classification or segmentation with the MIL Classification module. Note, although you can easily integrate MimAugment() operations from the MIL Classification module, it is not typically necessary, as you can enable preset augmentation controls from the module itself (M_PRESET...). For more information, see the Data augmentation and other data preparations section of Chapter 48: Datasets. Steps to augment an image The following steps provide a basic methodology for augmenting an image: Allocate an image processing context for augmentation, using MimAlloc() with M_AUGMENTATION_CONTEXT. Allocate an image processing buffer to hold augmentation results, using MimAllocResult() with M_AUGMENTATION_RESULT. Enable the image processing operations that the augmentation can perform, using MimControl() with M_AUG_..._OP. By default, all operations are disabled. Optionally, modify the range of values for the image processing operations that the augmentation can perform, using MimControl(). MIL randomly chooses the values to use for the operations from within the specified range. Optionally, set the priority (order) with which the augmentation can perform the image processing operations, using MimControl() with M_PRIORITY. Optionally, set the probability with which the augmentation can perform the image processing operations, using MimControl() with M_PROBABILITY. Perform the augmentation, using MimAugment(). This function performs the augmentation operation on a source image and places the results in either an image buffer or an augmentation result buffer, depending on the destination parameter setting. The resulting image buffer holds the modified version of the source image. The result buffer holds information about the modifications. If you called MimAugment() with an augmentation result buffer, retrieve the required results, using MimGetResult(). Important results include the image processing operations performed, and the values used for them, for each call to MimAugment(). To save a report of such results, call MimStream() with the M_SAVE_REPORT operation. Optionally, draw the resulting image of the augmentation, using MimDraw() with M_DRAW_AUG_IMAGE. If necessary, save your augmentation context, using MimSave() or MimStream(). Free all your allocated objects, using MimFree(), unless M_UNIQUE_ID was specified during allocation. The augmentation operation is usually run multiple times, since most augmentation related applications require multiple modified images. Typically, you should save the augmented images (MbufSave()). For more information about how to use the augmentation operation, see the following example: mimaugment.cpp Note, you can interactively configure and execute augmentation operations with MIL CoPilot. Priority By default, the image processing operations (M_AUG_..._OP) that MimAugment() can perform are prioritized (ordered) as follows: Affine operations, such as M_AUG_ROTATION_OP. Structure operations, such as M_AUG_DILATION_OP. Geometric operations, such as M_AUG_FLIP_OP. Intensity operations, such as M_AUG_INTENSITY_MULTIPLY_OP. Linear filter operations, such as M_AUG_SMOOTH_DERICHE_OP. Noise operations, such as M_AUG_NOISE_GAUSSIAN_ADDITIVE_OP. To modify this order, call MimControl() with M_PRIORITY. Changing the order of the operations can alter their final effect; for example, smoothing an image before adding noise differs from adding noise before smoothing. Probability Each time you call MimAugment(), MIL randomly selects which enabled image processing operations to perform. By default, each operation has an equal chance of being randomly selected. To modify these chances, call MimControl() with M_PROBABILITY. Note, setting the probability to 100.0 ensures that MIL performs the operation, while setting 0.0 ensures that MIL will not perform the operation (this is the same as disabling the operation). Randomness and seeds To establish the randomness required by the augmentation operation, MIL uses an internal random number generator (RNG). All the random values that MimAugment() uses trace back to a seed; that is, the first initialization value of the internal random number generator. By default, MIL randomly generates a new seed each time you call MimAugment(). To specify an explicit initialization value for the internal random number generator, call MimControl() with M_AUG_SEED_MODE set to M_RNG_INIT_VALUE, and then use M_AUG_RNG_INIT_VALUE to specify the actual value. Using the same seed multiple times lets you rerun (repeat) the same augmentation process with the same randomized settings. This seed is saved with the augmentation context. You can also specify the seed directly when calling MimAugment(), by using the SeedValue parameter. To do so, you must set M_AUG_SEED_MODE to M_USER_DEFINED_SEED. This seed is not saved with the context. Note that, you can get the seed that MimAugment() uses (regardless of how the seed was established) with the M_AUG_SEED_USED result, and then use that seed to repeat that exact augmentation operation. Random rotation angle When you enable the rotation operation (M_AUG_ROTATION_OP), a random rotation angle is generated to rotate the image. The random rotation angle can be represented with the following formula: AngleMin &lt;= (AngleRef + i * AngleStep) ± (AngleDelta / 2) &lt; AngleMax, where: AngleMin (M_AUG_ROTATION_OP_ANGLE_MIN) represents the minimum rotation angle. AngleRef (M_AUG_ROTATION_OP_ANGLE_REF) represents the reference angle. All other angles for M_AUG_ROTATION_OP will be calculated relative to this angle. i is a randomly generated integer. AngleStep (M_AUG_ROTATION_OP_ANGLE_STEP) represents the step angle. The step angle is the distance, or gap, relative to the reference angle. The random integer i determines the number of rotation steps from the reference angle to reach the angle range from which to randomly select a rotation angle. AngleDelta (M_AUG_ROTATION_OP_ANGLE_DELTA) represents the delta angle. The delta angle is the angular range, relative to (AngleRef + i * AngleStep). AngleMax (M_AUG_ROTATION_OP_ANGLE_MAX) represents the maximum rotation angle. If the generated random rotation angle is outside the range of AngleMin and AngleMax, another combination of i and RandomAngle will be generated. By default, AngleMin (M_AUG_ROTATION_OP_ANGLE_MIN) and AngleMax (M_AUG_ROTATION_OP_ANGLE_MAX) are set to 0 degrees and 360 degrees, respectively. The following example illustrates how the random rotation angle is established. Note that since i is randomly selected, an angle can be selected from any of the angular ranges around each possible step (as long as the angle is between the specified minimum and maximum angle). Augmentation Steps to augment an image Priority Probability Randomness and seeds Random rotation angle ",
      "wordCount": 1064,
      "subEntries": []
    }
  ]
}]
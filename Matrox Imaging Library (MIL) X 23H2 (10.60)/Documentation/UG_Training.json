[{
  "id": "UG_Training",
  "version": "2024020714",
  "title": "Training",
  "subTitles": null,
  "location": "MIL UG P07: Machine learning fundamentals",
  "pageURL": "content\\UserGuide\\Training\\ChapterInformation.htm",
  "text": " Chapter 49: Training This chapter explains how to perform training with the MIL Classification module. Training overview Principal trains of thought Train engine Basic concepts for training Fundamental decisions and settings Training modes Complete Transfer learning Fine tuning Summary and comparison Training mode controls Learning rate Maximum number of epochs Mini-batch size Schedule type Comparing default training mode controls Class weights Analysis, adjustment, and additional settings Results Paths and folders Recognizing a properly trained classifier Advanced analysis of your training Confusion matrix Score distribution Improving a deployed network with fine tuning Expected trends and fluctuations Examination Adjustments Bias and variance analysis High training dataset error High development dataset error Cut your losses ",
  "wordCount": 114,
  "subEntries": [
    {
      "id": "UG_Training_Training_overview",
      "version": null,
      "title": "Training overview",
      "subTitles": [
        "Principal trains of thought",
        "Train engine"
      ],
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\Training\\Training_overview.htm",
      "text": " Training overview Training is the process in which the classifier learns to predict the class to which the data belongs. Training requires a classifier context (the classifier you are training), a training context (the settings to train the classifier), and dataset contexts (and data to train the classifier). You must specify these contexts when you call MclassTrain(). Note, you have little direct control over the classifier context; your control lies in training it. That is, MclassTrain() uses the training context (which you can modify, for example, by calling MclassControl()), and the data, to train the classifier context. Training time varies considerably depending on the complexity of the application, the available hardware, and the accuracy required. To produce a properly trained classifier, MclassTrain() might have to run for an extended period, and you might have to call it several times after modifying your training settings. In some cases, you might need to modify your datasets also, and then go back to training. Once the training process returns the results you require, you can predict with it. For the most recent documentation of this topic, particularly as it relates to anomaly detection and statistical analysis (MclassStatCalculate()), check for an updated version of the MIL Help online at zebra.com/aurora-imaging-library-help. Principal trains of thought An untrained classifier context behaves like a random guess; for example, an untrained classifier would have an error rate (or accuracy) of approximately 50% when solving a 2 class (binary) problem. The goal of training a classifier is to lower its error rate (increase its accuracy) using representative data (datasets) and adjusting training settings (training context). Theoretically (and especially for CNN based classifiers), you can see training as an optimization problem that minimizes a cost (or loss) function (minimizes error). If everything is perfect, training results in a classifier that has no loss (no error) and is 100% accurate. Although this is ideal, it is unrealistic when using deep learning or machine learning technologies. Training is an iterative optimization process; at each iteration, the loss is calculated for all the images in the training dataset. The classifier's internal parameters (weights) are updated to minimize the loss value (error). Due to memory limitations, it is typically impossible to load all the data simultaneously, especially when using a CNN based classifier, since the data is images (often, thousands of images). To address this, MIL divides this data into small sets called mini-batches. MIL calculates the loss and updates the weights for each mini-batch to minimize that loss. Although training processes each mini-batch independently, the goal is to minimize the global error (the global loss). For tree ensemble classifiers, a somewhat similar process occurs with bootstrapping (bagging). Keep in mind that if you are training a classifier with a limited amount of data, you can achieve a low error rate (high accuracy) at training time, but get a high error rate (low accuracy) at prediction time. Limited data often causes training to over-fit the classifier on that data; when this happens, the classifier did not generalize the problem and does not perform well when given similar images it has not seen. Limited data can occur when, for example, you need to train with a specific defect, but you do not have many images of that defect because it happens infrequently. For more information about how to recognize a properly trained classifier, see the Analysis, adjustment, and additional settings section later in this chapter. It is often preferable to train from the ground up, when the classifier's internal weights are unset and totally based on your training settings and data. For CNN based classifiers, this is typically known as a complete training. If your data is limited, you might consider using a pretrained CNN classifier as your starting point. Ideally, the pretrained classifier that you use has already learned to generalize a similar problem to a certain degree, and its internal parameters have been adjusted accordingly. You can therefore build off those adjustments by continuing to train it with your data. This type of training, for CNN based classifiers, is considered transfer learning. For more information, see the Training mode controls subsection of the Fundamental decisions and settings section later in this chapter. Training an already trained tree ensemble classifier is considered a warm start. The following image represents a general overview of what is required to train a classifier, specifically one that is CNN based (that is, for image classification, segmentation, or object detection). The set of images with which you train the classifier must be representative of the actual application. The images should come from the final imaging setup (for example, the same camera, lens, and illumination), and should include the various aspects of the product, as well as its expected variations (for example, changes in scale, rotation, translation, illumination, color, and focus). The size of the images is determined by the application, such as the dimensions of the objects or features to classify, as well as by the specific predefined CNN classifier that you are using. Train engine The train engine refers to the hardware (CPU/GPU) with which the training is performed. By default, MIL typically uses the best available GPU (GPU training is highly recommended). You can modify this by calling MclassControl() with M_TRAIN_ENGINE or in the MILConfig utility. You must have installed the appropriate MIL X Service Pack add-on to train on your GPU. MIL add-ons are available for download through the Updates item found in MILConfig. For more information, see the MIL add-ons and updates subsection of the Requirements, recommendations, and troubleshooting section of Chapter 47: Machine learning with the MIL Classification module. Training overview Principal trains of thought Train engine ",
      "wordCount": 945,
      "subEntries": []
    },
    {
      "id": "UG_Training_Basic_concepts",
      "version": null,
      "title": "Basic concepts for training",
      "subTitles": null,
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\Training\\Basic_concepts.htm",
      "text": " Basic concepts for training The basic concepts and vocabulary conventions for training are: Bootstrap aggregating (bagging). Selecting entries to train a tree ensemble classifier. Entries are either in-the-bag or out-of-bag (per tree). MIL uses bootstrapping, bagging, randomness, and multiple learning algorithms to maximize the accuracy and performance of a tree ensemble classifier. Complete training. A training mode that resets the classifier's weights. This is for completely restarting the training of a predefined CNN or segmentation classifier, or for training an untrained CNN or segmentation classifier. Confusion matrix. A type of table, in a matrix format, that presents information about how many entries were, for each class, correctly classified, and how many were, for each class, confused with other classes, during training. A confusion matrix is also known as an error matrix. Epoch. One full cycle of the dataset during training of a predefined CNN or segmentation classifier. Fine tuning. A training mode designed for a predefined CNN or segmentation classifier that is mostly trained. You can fine tune a classifier if you want to add additional training data without restarting the entire training process. Additional data must have the same image size and the same number of classes. In-the-bag. The dataset entries that MIL randomly selects (per tree) during bootstrap aggregating, to train a tree ensemble classifier. The random selection is done with replacement; the randomly selected entries are available for reselection. Intersection over union (IOU). A metric that evaluates a segmentation classifier's performance by measuring the overlap between the ground-truth and predicted region. If the two regions are identical and completely overlap, then the IOU is equal to 1. Loss. The result of a mathematical loss (or cost) function that MIL uses to evaluate the confidence associated with the classification during training. After each epoch, MIL uses the loss value to adjust the classifier's weights to help achieve a lower loss at the end of the next epoch. Mini-batch. A subset of entries in an images dataset. Since such datasets typically contain numerous entries which require a considerable amount of memory to manage, the training process randomly splits them into groups, or mini-batches, to improve efficiency. Out-of-bag. The dataset entries that are not in-the-bag (per tree). MIL uses these to evaluate the performance of the classifier's training and regulate over-fitting. Overfitting. When a classifier is trained too precisely on the training data, that it performs poorly when generalizing to other data that is similar. To prevent overfitting and develop a properly generalized classifier, MIL uses a development dataset or out-of-bag entries. Train engine. The processing device (for example, the GPU or CPU) on which training is performed. Training mode. The process by which MIL trains a predefined CNN or segmentation classifier and the extent to which it should use previously learned information. You can train with a complete process, a transfer learning process, or a fine tuning process. Changing the training mode affects how the training process establishes and evolves the classifier's weights. Training mode controls are also known as hyperparameters. Transfer learning. A training mode where the network weights for the feature extraction layers are those from the predefined CNN or segmentation classifier. This mode is used to adapt a pretrained classifier to a new, similar classification problem. Basic concepts for training ",
      "wordCount": 546,
      "subEntries": []
    },
    {
      "id": "UG_Training_Fundamental_decisions_and_settings",
      "version": null,
      "title": "Fundamental decisions and settings",
      "subTitles": [
        "Training modes",
        "Complete",
        "Transfer learning",
        "Fine tuning",
        "Summary and comparison",
        "Training mode controls",
        "Learning rate",
        "Maximum number of epochs",
        "Mini-batch size",
        "Schedule type",
        "Comparing default training mode controls",
        "Class weights"
      ],
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\Training\\Fundamental_decisions_and_settings.htm",
      "text": " Fundamental decisions and settings Before training a classifier (predefined CNN, predefined segmentation, predefined object detection, or tree ensemble) with MclassTrain(), there are some fundamental training decisions and settings you will typically have to make. These are usually made by calling MclassControl(). The content on this page primarily applies to CNN, segmentation, and object detection classifiers. For training settings related to tree ensemble classifiers, see the Classifier and training settings for feature classification section of Chapter 55: Feature classification. During the training process, you can observe the evolution of the classifier to see if it is progressing as expected. Usually, after training, you would analyze your results, and train again to make improvements, until you consider your classifier properly trained. For more information, see the Analysis, adjustment, and additional settings section later in this chapter. Training modes Depending on the problem definition, different training modes exist, for image classification, segmentation, or object detection. Training mode controls differ in their input and output properties as well as in their initialization. To set these training modes, call MclassControl() with M_RESET_TRAINING_VALUES. When you specify a training mode, MIL resets all related training mode values accordingly. However, you can also change these values explicitly. For more information, see the Training mode controls subsection of this section. The default values for these modes should be sufficient for typical cases. You can call MclassControl() with the training context to modify the individual settings (for example, M_INITIAL_LEARNING_RATE), or you can reset them all based on the type of training you want to do (M_RESET_TRAINING_VALUES). ICNet, CSNet, and ODNet predefined classifiers usually support a complete training mode, though not necessarily other modes (transfer learning and fine tuning), when you first use them. The mode generally refers to the process by which MIL trains a predefined classifier and the extent to which it should use previously learned information. Complete In this mode (M_COMPLETE), a complete model training is performed. The classifier's source layer adapts to the size (and number of channels) of the input training images; the output layer adopts the number of target classes, and all the network weights are randomly re-initialized. This is the preferred mode when having access to a significant amount of training data. This is the default training mode. Typically, this is for completely restarting the training of a CNN, segmentation, or object detection classifier context, or for training a CNN, segmentation, or object detection classifier context that is not trained. Note, once a classifier that requires a complete training (for example, M_ICNET_M) was trained once, you can then continue the training process as transfer learning or fine tuning. This requires copying the trained classifier from the train result into a classifier context, using MclassCopyResult() with M_TRAINED_CLASSIFIER, and retraining it with MclassTrain(). Transfer learning This mode (M_TRANSFER_LEARNING) performs the transfer learning technique. Similar to complete, the source layer adapts to the size of the input training images and the output layer adapts to the number of target classes. Note, the classifier's source layer only adapts to the size of input training images (since, the number of bands will not change, the input training images should have the same bands as the CNN or segmentation classifier). Typically, this is for a CNN or segmentation classifier context that was already trained on a specific classification problem, and that you must train on a similar (but new) problem. The classifier's weights for the feature extraction layers however are those of the pretrained classifier. During a transfer learning type of training, only the classifier layers are trained to classify the images. You should use this mode when the quantity of training data is limited. For more information about the classifier's internal layers, see the Classifiers, what they are and what they do section of Chapter 47: Machine learning with the MIL Classification module. Note, Matrox has partly pretrained the M_ICNET_MONO_XL and M_ICNET_COLOR_XL classifiers to properly extract the generally important features suitable for most applications. You must typically continue the training process, with a transfer learning mode and your own labeled image, to complete the training for your specific needs. Fine tuning This mode (M_FINE_TUNING) is used to fine-tune a classifier (model). Fine tuning is used to improve a previously-trained model with the addition of new training data. Typically, this is for a CNN or segmentation classifier context that was already trained on a specific classification problem, and that you must train with additional data. In this mode, the input and output layers as well as all the network weights are those from the previously-trained model. As a consequence, the number of target classes, as well as the size and bands of the input images, cannot be changed in this mode. Typical reasons to fine-tune are: You captured more data and updated the training dataset for all or some specific classes. You must now account for new image conditions like illumination or camera perspective. Summary and comparison The following table summarizes and compares the training modes. M_COMPLETE M_TRANSFER_LEARNING M_FINE_TUNING Input images Adapts size and bands of input images Adapts size of input images Uses existing sizes Feature extraction layers Resets internal parameters (weights) Starts from existing internal parameters (weights) Uses existing internal parameters (weights) Classification layers Resets internal parameters (weights) Resets internal parameters (weights) Uses existing internal parameters (weights) Output layers Adapts number of target classes Adapts number of target classes Uses existing classes Usage New application with lots of training data New application with limited training data Improve existing application Training mode controls When you specify a specific training mode for image classification, segmentation, or object detection, MIL automatically sets the related training mode controls to the required settings. You can also modify these controls yourself, to adjust the training process. The training mode controls let you adjust the: Learning rate. Maximum number of epochs. Mini-batch size. Schedule type. Such training mode controls are also known as hyperparameters. Learning rate The learning rate controls the factor by which the optimizer updates the network weights after each iteration. The higher the learning rate, the bigger the steps the optimizer takes to adjust the network parameters, and vice versa. Training begins with an initial learning rate (M_INITIAL_LEARNING_RATE) which then decreases after each epoch according to the decay value (M_LEARNING_RATE_DECAY). As an example, if the decay is set to 0.2, the learning rate loses 20% of its amplitude value after each epoch. At the beginning, higher learning rate helps the network to converge faster to a desirable state. After several iterations, the network's weights are updated more and more carefully with lower learning rate. Maximum number of epochs The maximum number of epochs (M_MAX_EPOCH) sets the maximum number of epochs to complete the training process. An epoch refers to one complete cycle through the dataset that the classifier must learn during training. The more complex the application, the more epochs might be needed. Mini-batch size Datasets cannot usually fully reside in memory during training. Mini-batches are used to break down the dataset that each epoch cycles through. MIL loads the data and processes it one mini-batch after another for each epoch. The mini-batch size (M_MINI_BATCH_SIZE) determines the number of images that are part of a mini-batch. The larger the mini-batch size the faster the training process and potentially the more accurate the resulting network but the more memory is required. A larger mini-batch size tends to make the learning smoother and faster. The maximum batch size is limited by the available memory for calculations. In general, a bigger batch size improves the network accuracy, however, depending on data, it is not systematically the case. For image classification, batch sizes typically range from 32 to 512. Batch sizes are generally much smaller for segmentation and object detection because they use more memory. Batch size plays a key role in the consumption of resources. It is recommended to observe memory (GPU or CPU) during training, with the help of the operating system's performance monitor. Schedule type The schedule type (M_SCHEDULER_TYPE) sets the schedule with which to adjust the learning rate. You can either specify to decay the learning rate at a cyclical schedule or to decay the learning rate as the internal parameters (weights) are updated. When specifying a cyclical schedule (M_CYCLICAL_DECAY), it is expected that you have a minimum number of mini-batches (for example, a dozen) per epoch. Comparing default training mode controls The table below shows the default settings for the training mode controls and how the defaults differ depending on whether the base training mode (M_RESET_TRAINING_VALUES) is complete, transfer learning, or fine tuning. Unless otherwise specified, values apply to image classification, segmentation and object detection. Note, only M_COMPLETE is available for object detection. M_COMPLETE M_TRANSFER_LEARNING M_FINE_TUNING M_INITIAL_LEARNING_RATE 0.005 (image classification) or 0.001 (segmentation or object detection) 0.005 (image classification) or 0.001 (segmentation) 0.001 M_LEARNING_RATE_DECAY 0.1 (image classification) or 0.05 (segmentation or object detection) 0.1 (image classification) or 0.05 (segmentation) 0.1 (image classification) or 0.05 (segmentation) M_MAX_EPOCH 60 60 60 M_MINI_BATCH_SIZE 32 (image classification) or 4 (segmentation or object detection) 32 (image classification) or 4 (segmentation) 32 (image classification) or 4 (segmentation) M_SCHEDULER_TYPE M_CYCLICAL_DECAY (image classification or segmentation) or M_FACTOR_DECAY (object detection). M_CYCLICAL_DECAY M_DECAY Be careful when modifying either the base training mode (M_RESET_TRAINING_VALUES) or any of the related training mode settings, as MIL uses your last specified value. For example, if you want a specific learning rate and you set that value, but then set M_RESET_TRAINING_VALUES to a specific training mode, the learning rate, along with all other related training mode controls, will be reset to the default values listed for that training mode. Class weights Class weights allow you to influence your classifier's training by placing more or less importance (weight) on certain class definitions. Class weights are able to be adjusted for segmentation and feature classification. While class weights are not supported for image classification or object detection, image classification and object detection datasets are easily balanced with MclassPrepareData() and M_AUGMENT_BALANCING. By properly balancing a dataset to meet your classifier's needs, you can achieve a similar effect to setting class weights. In the real world, the result of some class predictions are more important than others, particularly when it relates to costs in a business application. For example, it is very important to identify a faulty part before it gets installed in a cost-sensitive application. In this case, it would be preferable to incorrectly identify a good part as being bad, as opposed to incorrectly identifying a bad part as being good. In the first scenario, you either discard the part or set it aside for further inspection. In the second scenario, a bad part will be installed, causing a failure. The costs of these two scenarios might differ drastically, and you can use class weights to train a classifier with this in mind. If your dataset only has a few entries of an important class definition (the bad part in the above example), your classifier might not be good at predicting that a target belonging to that class because there are few examples of that class during training. By setting a higher class weight to the class represented by those few entries, you can train your classifier to be more likely to predict that a target belongs to that class. In this case, you would set a higher class weight to the dataset entries representing a bad part, particularly if they are underrepresented in the dataset. Even though this might makes the classifier less accurate (by predicting that a part is bad more often than it really is), it will be more useful to this application. Fundamental decisions and settings Training modes Complete Transfer learning Fine tuning Summary and comparison Training mode controls Learning rate Maximum number of epochs Mini-batch size Schedule type Comparing default training mode controls Class weights ",
      "wordCount": 1964,
      "subEntries": []
    },
    {
      "id": "UG_Training_Analysis_adjustment_and_additional_settings",
      "version": null,
      "title": "Analysis, adjustment, and additional settings",
      "subTitles": [
        "Results",
        "Paths and folders",
        "Recognizing a properly trained classifier",
        "Advanced analysis of your training",
        "Confusion matrix",
        "Score distribution",
        "Improving a deployed network with fine tuning",
        "Expected trends and fluctuations",
        "Examination",
        "Adjustments",
        "Bias and variance analysis",
        "High training dataset error",
        "High development dataset error",
        "Cut your losses"
      ],
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\Training\\Analysis_adjustment_and_additional_settings.htm",
      "text": " Analysis, adjustment, and additional settings Ideally, after you set up your training settings, call MclassTrain(), and review your training results, you realize that your classifier is perfectly trained and you can use it with MclassPredict(). However training traditionally takes a substantial amount of time. Although this time includes analyzing training results, modifying training settings, and recalling the training function, it also includes the execution of the training function itself, and how it might be training incorrectly. In such cases, you can end up waiting a long time for the training to complete, only to find out that the training is completely wrong. This can be caused by several factors, such as inappropriate architecture, hyperparameters, and datasets. For example, a very small initial learning rate (such as, 10^-6) can make a complete training process extremely slow. To help mitigate this, you can call two hook functions (also known as callbacks), one at the end of each mini-batch, and a second at the end of each epoch. This hooking mechanism helps ensure that training is developing in the correct direction, while it is happening. Specifically, you can use MclassHookFunction() to hook a function to a training event, and then call MclassGetHookInfo() to get information about the event that caused the hook-handler function to be called. Hooking lets you retrieve critical training information, such as the current loss value, training dataset accuracy, and development dataset accuracy. Collecting this information helps you analyze the success of the training and to assess the classifier's performance. The training process can also be interrupted while in the hook (callback) function. Inherently, training can take a lot of time; you should therefore typically expect to monitor the training process for proper convergence, and to make modifications to the process, or even to abort and restart it, if required. Proper convergence refers to increasing accuracy and minimizing error; in this way, you are converging to a classifier that properly identifies the class to which the data belongs. Note, not all results and settings are available for every possible classification task, and all different type of configurations. When necessary, the corresponding functions indicate availability (for example, MclassGetResult() and MclassControl()). Results To retrieve training results, call MclassGetResult() with the training result buffer that MclassTrain() produced. Typically, you are not only interested in the accuracy of the classifier, but also in the robustness of that accuracy. A properly trained classifier has both a high accuracy (or, conversely, a low error or loss), and has also been trained in such a way that, if given similar input that it has not seen, will continue to behave as accurately as you expect. As discussed in the following sections, recognizing a properly trained classifier can sometimes be tricky business. Paths and folders Training and data preparation, for image classification and segmentation, can cause MIL to write images to local folders. The paths to these folders can vary, depending on whether you are performing image classification or segmentation, or if you are using default, absolute, or relative paths. To retrieve such information, call MclassInquire(), and specify the path or folder related setting, such as: M_ROOT_PATH. M_SEGMENTATION_FOLDER. M_SEGMENTATION_FOLDER_ABS. M_REGION_MASKS_FOLDER. M_REGION_MASKS_FOLDER_ABS. M_PREPARED_DATA_FOLDER. If the inquired information is an empty string, MIL is using the default path; to inquire it, add M_DEFAULT_PATH (for example, M_PREPARED_DATA_FOLDER + M_DEFAULT_PATH). For information about the default path, or to change it, go to the DefaultDestinationFolder item under the Classification item in the MILConfig utility. Most settings that you can inquire are settable using MclassControl(). Recognizing a properly trained classifier A properly trained classifier is ready for prediction. But what does properly trained mean? A properly trained classifier captures the general problem and successfully generalizes it so that it has a good accuracy on the trained data, and also on future data that is similar. This generalization requirement represents the main reason to organize data in separate datasets (for example, the training dataset and the development dataset); you must ensure that the classifier does not under-fit, nor over-fit, the problem. Typical issues to suggest that a classifier is not properly trained include: The classifier over-fitting the training data, which usually means the classifier is not general enough to perform properly in the field (during prediction). Improperly augmented data, which can cause the classifier to learn how to solve a problem other than the one that was originally presented. The following images illustrate two kinds of data represented by diamonds and circles, and the kinds of issues a classifier might have trying to differentiate between them. This differentiation, which is also known as the classifier's solution or data split, is indicated by a green line. The classifier separates the data badly, given the numerous errors. Do not consider this classifier properly trained; its solution is too general as it under-fits the problem. The classifier too precisely separates every detail of the data and disregards any underlying trend. This solution is unlikely to work on new data (prediction). Do not consider this classifier properly trained; it is not general enough as it over-fits the problem. The classifier almost perfectly separates the data by capturing an underlying trend. You should consider this classifier properly trained; it has effectively generalized a solution. A properly trained classifier might also have to meet the following requirements: Minimum accuracy or IOU. Maximum false positive and false negative rates. Maximum advanced error metrics. Execution speed on the target platform (CPU). As previously discussed, MIL uses the development dataset, which is required for training a CNN, to help regulate training issues such as overfitting (to a certain degree, tree ensemble classifiers use bagging for this). Nevertheless, such issues can still occur and might be a sign that there is an underlying problem with how the training data is represented as a whole (and not how it was split). Advanced analysis of your training An advanced analysis of your training refers to a more in-depth and granular investigation into what was properly and improperly classified. Such techniques can shed light on why your training is proving unsuccessful and how to fix it. Confusion matrix The confusion matrix is a type of table, in a matrix format, that presents information about how many entries were correctly and incorrectly classified during training. A correctly classified entry means that its predicted class is the same as the ground truth for that class (the expected class). For example, the following confusion matrix was calculated after the training was done using a development dataset with 3 classes (A, B, and C) and 500 entries (images) representing each one. The cells on the diagonal, where the classes in the ground truth row on the left intersect with the corresponding classes in the predicted column at the top indicate the number of development dataset entries that were properly classified as that class. If the classifier is perfect, the number of predicted classes would always equal the number of ground truth (expected) classes; this would result in a value of 0 in every other cell. In this example, the perfect result would be to have 500 in every intersecting cell. By examining the values in the intersecting cells, and in the other cells, you can understand the proportions between true positives, false positives, true negatives, and false negatives. The terminology for these values applies to binary classifiers that predict a target as either belonging to a class, or not belonging to a class. The values, however, can also be calculated from a confusion matrix for a classifier with more than 2 classes (for example, the 3x3 confusion matrix above). True positive (TP): A correct prediction that a target belongs to a specific class. False positive (FP): An incorrect prediction that a target belongs to a specific class. True negative (TN): A correct prediction that a target does not belong to a specific class. False negative (FN): An incorrect prediction that a target does not belong to a specific class. For multiclass classification problems, there are more than just positive and negative classes. In these cases, you must calculate the values independently for each class by summing over the corresponding cells in the confusion matrix. The TP for a reference class is the cell at the intersection of the ground truth and the prediction for that class. The FP is the sum of the remaining cells in the prediction column for the class. The FN is the sum of the remaining cells in the ground truth row for the class. The TN is the sum of any remaining cells. In the first confusion matrix above, the cells in matrix represent the following, for each class: Using class B as an example, summing over the respective cell gives the following values: TP = 400. FP = (6+1) = 7. TN = (491+3+0+499). FN = (10+90) = 100. You can use these values to calculate the following metrics for your classifier. Again, for multi-class classification problems, the metrics must be calculated independently for each class. Accuracy = (TP+TN) / (TP+FP+TN+FN). This tells you: Of all the predictions, how many are correct? Precision = TP / (TP+FP). This tells you: Of the items predicted to belong to a certain class, how many truly belong to that class? Recall (also known as sensitivity) = TP / (TP+FN). This tells you: Of the items that truly belong to a certain class, how many are predicted to belong to that class? This is a particularly important metric when you are working with unbalanced datasets, as a classifier is less likely to predict a class if it is underrepresented in the dataset. F1 score = 2*(precision*recall) / (precision+recall). This tells you: The overall performance of a classifier, equally weighting precision and recall. Do not rely too heavily on this metric if precision and recall are not of equal importance to your application. IOU = TP / (TP+FN+FP). This tells you: A score between 0 and 1 indicating the performance of a segmentation classifier. This is only a meaningful metric for a segmentation confusion matrix, where each cell represents a number of pixels. For more information about IOU, see the Training analysis subsection of the Training and analysis for segmentation section of Chapter 52: Segmentation. The table below shows the metrics for all classes in the confusion matrix, using the same process as shown before with class B. You can also take the average for each class to get metrics for your classifier as a whole. Reference class TP FP TN FN Accuracy Precision Recall F1 score IOU (segmentation only) A 491 10 990 9 0.987 0.980 0.982 0.981 0.936 B 400 7 993 100 0.929 0.983 0.800 0.880 0.789 C 499 93 907 1 0.937 0.843 0.998 0.914 0.841 Average - - - - 0.951 0.935 0.927 0.925 0.855 Depending on your dataset balancing and your classifier's application, a weighted average in proportion to the number of entries for each class might be more useful as a metric. For example, if you have class A with only 10 entries, then an average will weigh that class equally to class B with 1000 entries. If correctly predicting class A is critical to your application then this might be okay, but otherwise it will skew your results. Since training a classifier is an iterative process, you need a way to evaluate each iteration. There is no singular best metric to use when evaluating a classifier, and you should not trust a metric unless you know how to interpret it. Carefully select the metrics that most accurately reflect the problem you are trying to solve with your classifier. Although counter intuitive, it can be preferable for certain applications to actually have some false classifications. Generally, if a classification is incorrect, it is either a false positive or false negative. The classifier is either wrong about what it thought was right (false positive classification) or it is wrong about what it thought was wrong (false negative classification). This information can prove invaluable, not only to help you adjust your training, but also to develop a better classifier for your specific application. For example, you might want to err on the side of false positives for a defective class, or you might want to err on the side of false negatives for a good class to maintain yields and reduce waste. Score distribution The classifier returns a score for each class. The image that you are classifying is, as expected, associated with the class with the highest score. The highest score is always greater than 100/N , where N is the number of classes. It is possible that false positives are more important than false negatives, and vice versa. Other than using the score to identify the best class, you can use it to take further decisions about the classified image. For example, a weak highest score, such as 51% in a binary classification problem, can result in rejecting the part to minimize the false positive rate. Analyzing the distributions of the scores, per expected class, can facilitate setting up a threshold decision to establish what is the lowest acceptable highest score before the classification result is rejected. An example of this is shown in the following image. You can construct score distribution information like this using results that you can retrieve by calling MclassGetResult(). Improving a deployed network with fine tuning It might be difficult to collect enough data to fully cover all variations of the final application. The development dataset accuracy might be overestimating the real performance of the application. In such cases, the deployed system can also collect additional data of interest to add to the training dataset. The increased dataset can be further used to improve the network through fine-tuning. A threshold on the score can be used to select the images to collect, typically the ones classified with a low confidence. These images will need to be manually verified and labeled later by an expert and added to the existing training and development datasets. If, after fine-tuning, the performance of the network is improved, the new network can be re-deployed in the field. To improve your classifier's training, it is recommended to: Plan for mechanisms to efficiently collect more data from a deployed system. Archive the datasets to potentially further improve a network and assess the performance improvement. Expected trends and fluctuations Despite fluctuating values, you should expect the following while the training is occurring: The loss value, which is updated after each epoch, generally decreases on average. The accuracy or IOU of the training and development datasets, after each epoch, generally increases on average. These overall trends indicate that training is proceeding successfully. The amplitude of the fluctuations typically decreases with larger batch sizes. The loss value decreases very fast over the first iterations, thus a logarithmic scale is commonly used to display the loss so you can observe its long-term evolution. After many epochs, if the error rates reach steady states and their values are far from expectations, the training process can be canceled before it ends, by calling MclassControl() with M_STOP_TRAIN. Examination At the end of the training, the overall evolution of the error rate and the loss is the first area of analysis. The error trends of both the training dataset and the development dataset should decrease over time. Given enough time (a large number of epochs), the training dataset error should eventually reach a minimum, and the classifier should perform as well as the expert in the domain. The development dataset error is also expected to decrease to a minimum, but the error might be unacceptably greater than the training dataset error. Also, if at some point in time, the development dataset error, after reaching a minimum, starts increasing (gets worse), the classifier is probably over-fitting the data. These are some examples of typical results you can get after training. The proper evolution of loss: Under-fitting symptoms: Over-fitting symptoms: The following graphs show a proper evolution of the error rate (the graph on the left) and the loss (the graph on the right) of the training dataset (in green) and the development dataset (in purple) during the execution of MclassTrain(). The error rate eventually converges to 0% and the loss gets down to 0.0015872. Note, the error rate can be seen as a kind of compliment to accuracy; the lower the error, the higher you can consider the accuracy. You should abort the training process if the values level off prematurely or at an unacceptable level. Adjustments Observing issues in training, such as over- or under-fitting, is known as bias and variance analysis. This is done so you can understand how your classifier was trained and figure out the adjustments you can make so you can retrain in a better way. Bias and variance analysis The bias and variance analysis is typically based on observing errors related to the training dataset and development dataset. This analysis can help guide what you should adjust to improve the training when you recall MclassTrain(). The following image summarize the issues and what you can do; the subsequent subsections elaborates on this. High training dataset error The training dataset error measures the number of misclassified training dataset entries relative to the total number of training dataset entries. Ideally, the training dataset error converges to the Bayes error rate, which is the lowest possible error for any classifier. Typically, human and Bayes errors are close, and it is expected that the difference between the training dataset error and the human error, the avoidable bias, can be reduced to its minimum. A high training dataset error often indicates that the classifier is under-fit. Given no major mislabeling error in the dataset, such as a systematic labeling error, if the training dataset error is high, try taking the following bias reduction actions: Train longer by increasing the maximum number of epochs. Use a predefined CNN classifier with a larger capacity. Smaller classifiers can fail to handle complex problem. Try to use inputs (for example, images) with more features, such as color. A lack of features can hinder robust classifications. Adjust training mode related settings (hyperparameters), such as increasing the initial learning rate. High development dataset error The development dataset error measures the number of misclassified development dataset entries relative to the total number of development dataset entries. Ideally, the development dataset error should converge as close as possible to the training dataset error. A high development dataset error often indicates that the classifier is over-fit. Given no major mislabeling error in the dataset, such as a systematic labeling error, if the development dataset error is high, try taking the following variance reduction actions: Collect and label more data to increase the training dataset. You can also add augmented data to the training dataset to regularize the training process. Adjust training mode related settings (hyperparameters), such as increasing the initial learning rate. Note, investigating the development dataset errors together with the training dataset errors helps determine if the classifier was trained more than it should have been. Cut your losses On occasion, despite repeatedly training and adjusting and retraining, it can seem impossible to draw any more improvements from your classifier. Some of the most difficult cases can occur when your classifier is able to split your data to a fairly good degree, but has stubbornly plateaued at a level that just isn't good enough, as certain key indicators, such as loss and error rates, are no longer improving. In these cases, it is best to stop training, and try implementing more advanced techniques and examinations, such as analyzing the confusion matrix, scrutinizing the score distribution, and having a deployed application intended to collect data that you can then use to improve training and update the application. It is possible that such examinations indicate problems with your data, which could mean improving or restructuring your datasets. Analysis, adjustment, and additional settings Results Paths and folders Recognizing a properly trained classifier Advanced analysis of your training Confusion matrix Score distribution Improving a deployed network with fine tuning Expected trends and fluctuations Examination Adjustments Bias and variance analysis High training dataset error High development dataset error Cut your losses ",
      "wordCount": 3356,
      "subEntries": []
    }
  ]
}]
[{
  "id": "UG_Common_tasks_with_3D_data",
  "version": "2024020714",
  "title": "Common tasks with 3D data",
  "subTitles": null,
  "location": "MIL UG P05: 3D processing and analysis",
  "pageURL": "content\\UserGuide\\Common_tasks_with_3D_data\\ChapterInformation.htm",
  "text": " Chapter 33: Common tasks with 3D data This chapter describes how to perform common tasks with 3D data. Common tasks with 3D data overview Implementation considerations Removing the background Fixturing Sampling Normals Meshes Using 2D image processing techniques with 3D data Retrieving the orientation and center point or position of a 3D object Decision tree: which 3D module to use Dealing with a very large or complicated point cloud Remarks Module-specific values for retrieving an object's pose Item picking robot Establishing the relationship between the camera and the robot Determining the pose of the object(s) Retrieving the matrix Getting the transformation in the format required by the robot Locating defects in a point cloud Segmenting using 2D blob analysis ",
  "wordCount": 121,
  "subEntries": [
    {
      "id": "UG_Common_tasks_with_3D_data_Common_tasks_with_3D_data_overview",
      "version": null,
      "title": "Common tasks with 3D data overview",
      "subTitles": null,
      "location": "MIL UG P05: 3D processing and analysis",
      "pageURL": "content\\UserGuide\\Common_tasks_with_3D_data\\Common_tasks_with_3D_data_overview.htm",
      "text": " Common tasks with 3D data overview You can perform a variety of tasks by processing and analyzing 3D data using the MIL 3D modules. Note that different 3D modules can be used to perform the same task. For example, you can find an object using the 3D Blob Analysis module, the 3D Model Finder module, the 3D Metrology module, or the 3D Registration module. However, to obtain optimal results, you should choose the 3D module that best suits your specific needs. For example, you should use the 3D Model Finder module when you want to determine the location and orientation of multiple rectangular planes. This chapter discusses typical 3D use cases, including the suggested steps to take and 3D modules to use to in an attempt to obtain optimal results, depending on the requirements of your application. Common tasks with 3D data overview ",
      "wordCount": 145,
      "subEntries": []
    },
    {
      "id": "UG_Common_tasks_with_3D_data_Implementation_considerations",
      "version": null,
      "title": "Implementation considerations",
      "subTitles": [
        "Removing the background",
        "Fixturing",
        "Sampling",
        "Normals",
        "Meshes",
        "Using 2D image processing techniques with 3D data"
      ],
      "location": "MIL UG P05: 3D processing and analysis",
      "pageURL": "content\\UserGuide\\Common_tasks_with_3D_data\\Implementation_considerations.htm",
      "text": " Implementation considerations Several considerations come into play when working with 3D data. For example, you will often want to remove the background from your scene before proceeding with most 3D operations. This section briefly describes some typical requirements or tasks when working in 3D. Removing the background If there is a large background plane (such as a table or conveyer) in the scene, you can remove it using one of the following methods (note that the first method is more accurate): Obtain a point cloud of the background without the objects, fit a plane to this point cloud using M3dmetFit(), and then crop the background points in the original point cloud using M3dimCrop() with the fitted plane. For more information, see the Cropping or masking points subsection of the Working with points in a point cloud section of Chapter 35: 3D image processing. Fit a plane to the background using M3dmetFit(), copy the mask constructed from the points considered outliers during the fit using M3dmetCopyResult() with M_OUTLIER_MASK (foreground points will be considered outliers and will have a non-zero value), and then pass the resulting mask image to M3dimCrop() to crop out the background plane. For more information, see the Using a fitted geometry to mask points from a point cloud subsection of the Performing 3D fitting section of Chapter 39: 3D metrology. Fixturing Retrieving a fixturing matrix is often helpful (for example, you can use this matrix to fixture an object to the origin for comparison purposes with a reference model). You can typically obtain a fixturing matrix using an M...CopyResult() function (for instance, M3dmodCopyResult() with M_FIXTURING_MATRIX). Get the inverse of this matrix (M3dgeoMatrixSetTransform() with M_INVERSE) if your goal is to perform an action at the location at which an object was found or detected. For example, when drawing graphics, you can pass the inverse of the fixturing matrix to M3dgraText() to annotate an object at its found location in the point cloud. Another use case is when you need to send a robot to pick up an object. See the Item picking robot section later in this chapter. Sampling The resolution between point clouds should be similar for some 3D operations (for example, when using the 3D Model Finder module with a surface model or when using the 3D Registration module to register multiple point clouds). You can use M3dimSample() to subsample or upsample a point cloud to ensure it has a suitable density. Note that upsampling is referred to as surface sampling, since it is typically applied to a meshed point cloud. Subsampling is useful, for example, to maintain an approximately similar distance between points after a merge operation. You can also use M3dimSample() to subsample an unorganized point cloud to obtain an organized point cloud, or to generate a point cloud from a 3D geometry. See the Sampling a point cloud or a 3D geometry subsection of the Working with points in a point cloud section of Chapter 35: 3D image processing for more information. Surface sampling is useful when using 3D registration to register a point cloud to a reference CAD model. In this scenario, since a meshed point cloud restored from a CAD file is often sparse, with planar surfaces consisting of much larger surface triangles compared to other areas, surface sampling of the CAD-sourced point cloud makes its point density compatible with that of the acquired point cloud. For more information, see the Surface sampling a meshed point cloud subsection of the Working with points in a point cloud section of Chapter 35: 3D image processing. For details about sampling related to 3D model finder operations, see the Model and target point cloud resolution subsection of the Search settings specific to surface models section of Chapter 37: 3D model finder. Normals Many 3D functions require a normals component (M_COMPONENT_NORMALS_MIL), in which calculated point normal values are stored. Some 3D cameras and sensors provide normals; others do not. If yours does not, you can calculate a point cloud's unit normal vectors using M3dimNormals(). Note that some functions will automatically generate a normals component if it is required for the operation (for example, when surface sampling using M3dimSample()). Normals are required, for example, when using: M3dmodFind() to find any model. M3dimFilter() to filter using the source point cloud's normals information (M3dimControl() with M_USE_SOURCE_NORMALS set to M_TRUE). M3dimMesh() to calculate a mesh using the local plane or smoothed surface reconstruction mode (M3dimControl() with M_MESH_LOCAL_PLANE or M_MESH_SMOOTHED, respectively). M3dimSample() to subsample using certain subsample modes (M3dimControl() with M_SUBSAMPLE_MODE set to M_SUBSAMPLE_NORMAL or M_SUBSAMPLE_GEOMETRIC). M3dimStat() to calculate statistics on the normals component ( M3dimControl() with M_COMPONENT_OF_INTEREST set to M_COMPONENT_NORMALS_MIL). M3dblobSegment() to calculate a blob segmentation that uses normal vector angles (M3dblobControl() with M_NORMAL_DISTANCE_MAX or M_GLOBAL_NORMAL_DISTANCE_MAX set to a non-infinite angle, or M_GLOBAL_PLANE_DISTANCE_MAX set to a non-infinite distance). M3dgraControl() to color a point cloud graphic using the source container's normals component (M3dgraControl() with M_COLOR_COMPONENT set to M_COMPONENT_NORMALS_MIL). Note, you must ensure that the normal vectors are normalized to 1 before calling a function that requires a normals component. These functions assume that non-zero normal vectors are unit normal vectors, and they do not normalize them. You can use M3dimFix() with M_NORMALS_NORMALIZED to validate whether each point with a non-zero confidence has a normal with a norm of exactly 1 or 0, and normalize them to have a norm of 1 if not. Meshes Generating a triangular mesh structure for a point cloud is known as surface reconstruction, and is useful for certain 3D operations, such as calculating volumes (see the Calculating a volume section of Chapter 39: 3D metrology). A meshed point cloud can also help improve the performance of an M3dimProject() operation. Note also that a meshed point cloud typically shows surface texture better than non-meshed point clouds when displaying 3D data. For more information, see the Surface reconstruction subsection of the Working with points in a point cloud section of Chapter 35: 3D image processing. Using 2D image processing techniques with 3D data While 3D data and 2D image processing techniques are fundamentally different, there are situations where they can be used together in point cloud processing. Doing so is often advantageous in terms of time, since 3D operations are typically time-expensive. One example of using 2D image processing techniques with 3D data is to use a 2D view of the 3D data to find the location of an object, and then to use this information to analyze the object in 3D. The M3dimProject() function projects point cloud data into a depth map, which you can then use with a 2D finder module, such as Model Finder or Pattern Matching. Since the depth map is calibrated to give results in world units, you can map your 2D finder results back onto the 3D data to extract 3D properties of a found object of interest. For example, after using a 2D finder module, you can use 3D registration to obtain an object's exact pose in 3 dimensions. Many 3D modules allow you to create various 2D images, which are useful for further processing or diagnostic analysis. For example, you can use M3dregCopyResult() to create a distance image, an overlap mask, or a pair index image. These images allow you to examine the results of specific iterations of the registration process. See 3D Registration for more information. Occasionally, you might want to use an individual component of a point cloud. Since a component is allocated as a MIL buffer, you can use the component with any 2D function that takes a buffer. For example, you can threshold the Z-coordinates stored in a point cloud's range component using MimBinarize(). Doing so allows you to isolate data from the background based on Z-values. This is a quicker operation than using M3dimProject() to project the point cloud into a depth map. When using MimBinarize(), the resulting destination image buffer becomes a binarized mask that you can pass to M3dimCrop(), which will crop out points that did not meet the binary threshold condition. You can then process the resulting data with a 3D module using, for example, statistical analysis (using M3dimStat()) to obtain point distribution data, or with the 3D blob analysis module to segment the points or calculate features for clusters of points. Another component that is sometimes useful to manipulate directly is the confidence component. You could manipulate it to limit a processing operation to only certain points, rather than the whole point cloud. For more information, see the Using components individually section of Chapter 41: 3D Containers. You could also pass the confidence component to the 2D Blob Analysis module to generate a segmented label image. You can then use the label image to calculate 3D features for the blobs using 3D blob analysis (M3dblobSegment() and then M3dblobCalculate()). For more information, see the Segmenting using 2D blob analysis subsection of the Locating defects in a point cloud section later in this chapter. Implementation considerations Removing the background Fixturing Sampling Normals Meshes Using 2D image processing techniques with 3D data ",
      "wordCount": 1500,
      "subEntries": []
    },
    {
      "id": "UG_Common_tasks_with_3D_data_Retrieving_the_orientation_and_center_point",
      "version": null,
      "title": "Retrieving the orientation and center point or position of a 3D object",
      "subTitles": [
        "Decision tree: which 3D module to use",
        "Dealing with a very large or complicated point cloud",
        "Remarks",
        "Module-specific values for retrieving an object's pose"
      ],
      "location": "MIL UG P05: 3D processing and analysis",
      "pageURL": "content\\UserGuide\\Common_tasks_with_3D_data\\Retrieving_the_orientation_and_center_point.htm",
      "text": " Retrieving the orientation and center point or position of a 3D object You can retrieve the orientation and center point or position of a point cloud, depth map, or 3D geometry object. If you are unsure of where to start or which module to use, consult the following for suggestions. Decision tree: which 3D module to use Do you know exactly the object that you are looking for? Yes. Is the object a simple geometry (cylinder, line, plane, sphere) or a complex object? Simple geometry (does not include rectangular planes and boxes). Is the geometry the main item in the scene (like a background plane or a big sphere in the point cloud)? Is the size of the geometry unconstrained? If all answers are yes, use 3D Metrology with M3dmetFit(). Else (the geometry is small in a complex scene, the geometry's size needs to be constrained, or you are looking at multiple instances), use 3D Model Finder. Complex object (includes rectangular planes and boxes). Do you need to find the precise position and orientation of the object and/or you are looking at multiple instances? Yes. Use 3D Model Finder. No. Can you segment the scene into one or more clusters of points (blobs) and then select based on some geometrical properties? Can you segment part of the scene based on a planar surface of approximate dimensions? If any is yes, use 3D Blob Analysis. If none is yes, use 3D Model Finder. Do you have a model and scene available, with little to no background and minimal rotation of the object within the scene? If yes, use 3D Registration. Do you have a very large and complicated scene? If yes, refer to the Dealing with a very large or complicated point cloud subsection of this section. No (you don't know exactly the object you're looking for). Can you segment the scene into one or more clusters of points (blobs) and then select based on some geometrical properties? Yes. Use 3D Blob Analysis. No. Consider using 3D image processing statistics analysis. You can use M3dimStat() to calculate, for example, centroid, distance-to-nearest-neighbor, or principle component analysis (PCA) statistics, which provide data about the overall shape and point distribution of your scene. Before using M3dimStat(), you might consider cropping the point cloud. If the scene is complicated and you cannot use most 3D functions directly, use M3dimCrop() with hardcoded parameters. For example, if a region of interest is known, hardcode a bounding box region and use M3dimCrop() to isolate the area of interest. Does the required object have a constant pose? For example, the object lies consistently on a floor or conveyor? Yes. Consider using M3dimProject() to project the point cloud into a depth map. You can then use a 2D finder module, such as Model Finder or Pattern Matching, to identify your object. The Blob Analysis module might also be useful. No. Refer to the solutions presented above using the 3D modules. Once detected, further processing and analysis is available for the object. Dealing with a very large or complicated point cloud In the case of a very large and complex scene, start with the 3D Blob Analysis module to remove extraneous information and isolate areas of interest. Then, use the 3D Model Finder, 3D Metrology, or 3D Registration module. If the scene is primarily composed of boxes, an initial pass with 3D blob analysis might not be needed and you can use 3D model finder directly. Note that the 3D Model Finder module is slow when the complexity of the scene is high, but is similar to the 3D Blob Analysis module when searching for boxes. If the objects are boxes, you can use the 3D Model Finder module to avoid additional steps without increasing the search time. Once the larger point cloud is segmented into blobs, use the 3D Model Finder module to find the pose of all blobs or use the 3D Metrology or 3D Registration module to find the pose of specific blobs. For example, if the object in a blob is a simple geometry (like a sphere), use 3D metrology. If 3D metrology is not suitable, but you are confident that an isolated blob represents your target, use 3D registration. Remarks Typically, a 3D module's main computation function (for example, M3dmodFind(), M3dmetFit()) performs the initial object detection (such as finding a model occurrence). Once calculated, you can retrieve relevant results such as M_CENTER_.... For a module-specific listing of ways to retrieve a 3D object's orientation and center point or position, see the Module-specific values for retrieving an object's pose subsection of this section. Your target and/or reference point clouds might require sampling to bring their respective resolutions to the same density. See the Sampling subsection of the Implementation considerations section earlier in this chapter. Using 2D finder modules on your data can save computation time, since some 3D operations are costly in this respect. For example, it could be more efficient to use the 2D Model Finder module to find an object before deploying the 3D Registration module to establish the object's precise pose in 3 dimensions. To make 3D data available in 2D, you can project a point cloud into a depth map using M3dimProject(). To project points onto a fitted plane, you might require a fixturing matrix that provides the transformation required to fixture the points such that points are projected to the plane. See the Fixturing to a plane subsection of the How to fixture in 3D section of Chapter 45: Fixturing in 3D for more information. When using the 3D Model Finder module, if there is a lot of occlusion you should set the maximum coverage (M3dmodControl() with M_COVERAGE_MAX). Otherwise, the default settings are typically sufficient. For a discussion of other considerations when working with 3D data, see the Implementation considerations section earlier in this chapter. Module-specific values for retrieving an object's pose Typically, a 3D module's M...GetResult() function can return the orientation and center point of a point cloud, depth map, or 3D geometry object. In general for a 3D geometry, once you have found the required geometry, fitted one into your scene, or copied results into a 3D geometry object (for example, from a 3D blob analysis calculation), its pose is available. Note that some geometries are available only once you copy them out of a result buffer. For example, when searching for a box model using M3dmodFind(), a successful occurrence is stored in a result buffer. To access the box, you must copy it into a 3D geometry object (in this case, using M3dmodCopyResult()) before you can retrieve its orientation. The following table lists the ways to retrieve a 3D object's orientation and center point or position, depending on the module used. Module Orientation Center point or position 3dmod For a found occurrence, obtained using M3dmodFind(). Use M3dmodGetResult() with: M_AXIS_... to get a cylinder's orientation. M_NORMAL_... to get a plane's orientation. For a box model occurrence, you can use M3dmodCopyResult() with M_GEOMETRY to establish a box with the same orientation. Then, use M3dgeoCopy() with M_BOX_ORIENTATION to copy, into a transformation matrix object, the transformation values that determine the box's orientation. To find the tilt angle of a box with respect to the XY-plane, use M3dgeoConstruct() with M_PLANE and M_FACE. You can specify the face that is located furthest from the XY-plane (maximum Z) to construct a plane at that face. Then, call M3dmetFeature() with M_ANGLE to calculate the tilt angle. Other operations available using M3dmodCopyResult() include: M_BOUNDING_BOX to copy the oriented bounding box of any found model occurrence. M_FIXTURING_MATRIX to copy the transformation that can move the working coordinate system to the found model occurrence. M_OCCURRENCE_MATRIX to copy the transformation that transforms the surface model to the same location as the found occurrence. This is the inverse of M_FIXTURING_MATRIX and is applicable to surface results only. Use M3dmodGetResult() with: M_CENTER_.... M_POSITION_... to get a surface's position relative to the model's origin. 3dmet For a fitted 3D geometry, obtained using M3dmetFit(). Use M3dmetGetResult() with: M_AXIS_... to get a cylinder or line's orientation. M_NORMAL_... to get a plane's orientation. You can also use M3dmetCopyResult() with: M_FIXTURING_MATRIX to establish a transformation matrix that can move the working coordinate system to the fitted 3D geometry. Use M3dmetGetResult() with: M_CENTER_.... 3dreg For point clouds for which a registration was computed using M3dregCalculate(). Use M3dregCopyResult() with: M_REGISTRATION_MATRIX to find the transformation required to align the two point clouds. — 3dblob For a blob, obtained using M3dblobSegment(), and for which the appropriate features have been calculated using M3dblobCalculate(). Use M3dblobGetResult() with: M_AVERAGE_NORMAL_... to get the average normal orientation of the blob's points M_PRINCIPAL_AXIS_... to get the components of the blob's three principal axes. M_SEMI_ORIENTED_BOX_ANGLE to get the angle of the blob's semi-oriented bounding box. Note that this operation cannot distinguish if the required blob is flipped with respect to Z. Use M3dblobGetResult() with: M_CENTROID_... to get the blob's centroid (center of mass). M_BOX_CENTER_... to get the center of the blob's bounding box. 3dim For statistics results about a point cloud, depth map, or 3D geometry, obtained using M3dimStat(). Use M3dimGetResult() with: M_PRINCIPAL_AXIS_... to get the components of the object's principal axes. M_SEMI_ORIENTED_BOX_ANGLE to get the angle of the object's semi-oriented bounding box. Use M3dimGetResult() with: M_CENTROID_... to get the object's centroid (center of mass). M_BOX_CENTER_... to get the center of the object's bounding box. 3dgeo For a 3D geometry: Defined using a dedicated 3D geometry function (for example, M3dgeoCylinder()). Constructed using M3dgeoConstruct(). Obtained from an operation in another module that results in a defined 3D geometry (for example, using M3dmetFeatureEx()). For a 3D box geometry, use M3dgeoCopy() with: M_BOX_ORIENTATION to copy, into a transformation matrix object, the transformation values that determine the box's orientation. For a 3D plane geometry, use M3dgeoConstruct() with: M_LINE and M_NORMAL to construct a unit line parallel to the plane's normal. To construct a point at the center of a 3D geometry object, use M3dgeoConstruct() with: M_POINT and M_CENTER (available for box, finite cylinder, finite line, point, or sphere geometries). Retrieving the orientation and center point or position of a 3D object Decision tree: which 3D module to use Dealing with a very large or complicated point cloud Remarks Module-specific values for retrieving an object's pose ",
      "wordCount": 1705,
      "subEntries": []
    },
    {
      "id": "UG_Common_tasks_with_3D_data_Item_picking_robot",
      "version": null,
      "title": "Item picking robot",
      "subTitles": [
        "Establishing the relationship between the camera and the robot ",
        "Determining the pose of the object(s) ",
        "Retrieving the matrix",
        "Getting the transformation in the format required by the robot"
      ],
      "location": "MIL UG P05: 3D processing and analysis",
      "pageURL": "content\\UserGuide\\Common_tasks_with_3D_data\\Item_picking_robot.htm",
      "text": " Item picking robot If you need a robot to pick up objects, you must establish the translation and rotation needed to reach the objects. Establishing the relationship between the camera and the robot To pass positional results to the robot, the coordinates should be in the robot's coordinate system. To do so, you need to perform hand-eye calibration. This involves establishing the transformation between the working coordinate system and the robot coordinate system, and then applying this transformation to the point cloud before analysis or to the positional results after analysis. Use McalCalculateHandEye() to calculate the required transformations between coordinate systems in a robotics setup. For more information, see the Hand-eye calibration section of Chapter 42: Grabbing from 3D sensors. Determining the pose of the object(s) You can use many of the 3D modules to determine the pose of an object. To decide which module to use, refer to the Retrieving the orientation and center point or position of a 3D object section earlier in this chapter. For a discussion of other considerations when working with 3D data, see the Implementation considerations section earlier in this chapter. Your robot will require the location at which to pick up the object. This involves retrieving the matrix that defines the required transformation and then getting the transformation in the format required by the robot. The following subsections describe this procedure. Retrieving the matrix You can obtain the translation and rotation needed to move the robot to the object in the form of a transformation matrix, which, when applied, transforms coordinates appropriately. Typically, for a robot picker application, you must obtain the inverse of a fixturing matrix. When not inverted, the fixturing matrix provides the transformation values that can move the working coordinate system to the object, whereas the inverse gives values for moving the robot to the object's found location in the point cloud. In most cases, obtaining the inverse fixturing matrix is a two-step process. First, use an M...CopyResult() function to obtain a fixturing matrix (for instance, M3dmodCopyResult() with M_FIXTURING_MATRIX.) Then, use M3dgeoMatrixSetTransform() with M_INVERSE to reverse the direction of the transformation. Note that for surface results from a 3D model finder operation, M_OCCURRENCE_MATRIX is equivalent to the inverse of M_FIXTURING_MATRIX. Note that if you want to pick up objects that are rotated along a certain axis, you might need to add an additional transformation, depending on how the fixturing matrix is defined. For example, the fixturing matrix of a cylinder model occurrence defines the transformation that aligns the positive Z-axis with the occurrence's central axis. If you want to pick up cylinders that are also rotated along the X- or Y-axis, set up a transformation matrix that contains the rotation to the required axis (for example, M3dgeoMatrixSetTransform() with M_ROTATION_...). Then, use M3dgeoMatrixSetTransform() with M_COMPOSE_TWO_MATRICES to add it. Also note that you should compose the resulting matrix with the matrix that defines the transformation between the working coordinate system and the robot coordinate system (if not applied to the point cloud before analysis). Getting the transformation in the format required by the robot Once you have a matrix that represents the required transformations, retrieve the translations and rotations to pass to the robot controller. These are displacements in X, Y, and Z and rotation angles in degrees, respectively. Use M3dgeoMatrixGetTransform() with M_TRANSLATION and M_ROTATION_... to get the required values from the transformation matrix. A robot rotates by performing a sequence of rotations about the axes of the robot coordinate system (also known as Roll-Pitch-Yaw rotation). When using M3dgeoMatrixGetTransform() to retrieve the required set of rotation angles, you must specify the correct transformation type that corresponds to the order in which the robot will perform them, since the rotations are successive. You can determine the required rotation order by checking the convention used by your robot's manufacturer. The following table shows the notation used for each rotational axis by the robot controllers of various robot manufacturers. Robot type Robot controller representation Rotation about the X-axis Rotation about the Y-axis Rotation about the Z-axis ABB The orientation is converted by the robot controller to a quaternion. DENSO Roll Pitch Yaw Epson W V U Fanuc W P R KUKA C B A Staubli Rx Ry Rz Once you have retrieved the translation and/or rotation values from the transformation matrix, you can then pass the values to the robot (using McomSendPosition()). Item picking robot Establishing the relationship between the camera and the robot Determining the pose of the object(s) Retrieving the matrix Getting the transformation in the format required by the robot ",
      "wordCount": 760,
      "subEntries": []
    },
    {
      "id": "UG_Common_tasks_with_3D_data_Locating_defects_in_a_point_cloud",
      "version": null,
      "title": "Locating defects in a point cloud",
      "subTitles": [
        "Segmenting using 2D blob analysis"
      ],
      "location": "MIL UG P05: 3D processing and analysis",
      "pageURL": "content\\UserGuide\\Common_tasks_with_3D_data\\Locating_defects_in_a_point_cloud.htm",
      "text": " Locating defects in a point cloud When working in 3D, you will often want to identify differences between a target point cloud and a reference model, and to locate those differences. For example, if you have a scanned object and a CAD model, any scratches, bumps, or other irregularities should be identified and located. The following is a general outline for this process: Use the 3D Registration or 3D Model Finder module to align the target and reference point clouds. These modules allow you find the transformation required to align the two point clouds (M3dregCopyResult() with M_REGISTRATION_MATRIX or M3dmodCopyResult() with M_FIXTURING_MATRIX, respectively). You can then apply the transformation to the target point cloud using M3dimMatrixTransform() to fixture it to the same working coordinate system. Whether to use 3D registration or 3D model finder depends on your application. Both modules can register a target to a reference point cloud. With 3D registration, the reference object must be a point cloud. With 3D model finder, you should use a surface model (M3dmodDefine() with M_SURFACE), unless your object can be approximated by a 3D geometry (such as a cylinder). Note that for geometric models, you can use M3dmodDefine() to define the model parametrically. You cannot parametrically define an object with the 3D Registration module. For objects for which you have a model and scene available, with little to no background and minimal rotation of the object within the scene, use 3D registration. Use M3dmetDistance() to compute the distance between target and reference points. This results in a distance image. Threshold the distance image using MimBinarize() to identify the target points whose distances are too far from the reference (and therefore indicate a potential flaw or defect). Pass the binarized image to M3dimCrop() to crop the target point cloud. Once cropped, the points that are kept in the target point cloud are those whose distances from the reference point cloud are considered unacceptable. Note that you can specify to store results in an unorganized point cloud (M_UNORGANIZED); this typically uses less memory. However, if the target point cloud was initially organized, keeping the organization could prove useful. See step 5 below. Segment your point cloud. If your data is organized, use the binarized image with the 2D Blob Analysis module to define a segmentation. This approach is typically faster than calculating a segmentation from the point cloud. See the Segmenting using 2D blob analysis subsection of this section. If your data is unorganized or the above method didn't meet your needs, use the 3D Blob Analysis module to segment the cropped point cloud into separately identified blobs. Use M3dblobSegment() to perform the segmentation. The resulting blobs should be your flaws. Keep in mind the following: Carefully set the M_MAX_DISTANCE control type. The specified distance should ensure that points belonging to a defect are considered neighbors and therefore grouped together. To get a good initial estimate for M_MAX_DISTANCE, you can perform a distance-to-nearest-neighbor statistics calculation (using M3dimStat() with M_STAT_CONTEXT_DISTANCE_TO_NEAREST_NEIGHBOR) on the original point cloud. You might want to use a multiplication factor that is greater than 1 on the retrieved statistic (M3dimGetResult() with M_DISTANCE_TO_NEAREST_NEIGHBOR_MAX). Set the M_NUMBER_OF_POINTS_MIN control type to an appropriate value to create blobs with enough points to be considered a defect; groups of points with less than this minimum will not be segmented into blobs and are ignored. The KD tree search mode (M_NEIGHBOR_SEARCH_MODE set to M_TREE) is typically more precise than the other modes when determining which points are neighbors for the M3dblobSegment() operation. Once segmented into blobs, you can retrieve the orientation and center point of each blob using M3dblobGetResult(). See the Retrieving the orientation and center point or position of a 3D object section earlier in this chapter. Once you have performed the segmentation, you can do the following with the resulting blobs: Use M3dblobExtract() to copy the blobs (flaws) to their own container. Compute and retrieve blob features such as the centroid and bounding box, using M3dblobCalculate() and M3dblobGetResult(). Perform additional operations on the blobs using M3dblobSelect() M3dblobCombine(), and/or M3dblobSort(). Copy blob results into an index or label image, using M3dblobCopyResult() with M_INDEX_IMAGE or M_LABEL_IMAGE, respectively. You can use these images to identify specific blobs, since each pixel in an index or label image is set to the index or label of its corresponding blob. You can then pass a specific index or label to M3dblobCalculate() to target an individual blob for feature calculation. For a discussion of other considerations when working with 3D data, see the Implementation considerations section earlier in this chapter. Segmenting using 2D blob analysis To define a segmentation from a binarized image, perform the following: Use MblobControl() to configure a 2D blob analysis context with appropriate settings, including M_BLOB_IDENTIFICATION_MODE set to M_INDIVIDUAL and M_IDENTIFIER_TYPE set to M_BINARY. Pass the binarized image to MblobCalculate() to compute labels for the binarized pixel groupings. Call MblobLabel() to generate a label image. Pass the label image to M3dblobSegment() while also specifying the M_SEGMENTATION_CONTEXT_LABEL_IMAGE predefined context. The resulting 3D blob analysis result buffer will hold the segmentation in which flaws are separately identified into blobs. With this approach, points are grouped into blobs based on the 2D pixel lattice of the range component, rather than the world distance between points. Locating defects in a point cloud Segmenting using 2D blob analysis ",
      "wordCount": 885,
      "subEntries": []
    }
  ]
}]
[{
  "id": "UG_ML_Classification_and_classifier_overview",
  "version": "2024020714",
  "title": "Machine learning with the MIL Classification module",
  "subTitles": null,
  "location": "MIL UG P07: Machine learning fundamentals",
  "pageURL": "content\\UserGuide\\ML_Classification_and_classifier_overview\\ChapterInformation.htm",
  "text": " Chapter 47: Machine learning with the MIL Classification module This chapter presents an overview of how to use machine learning technologies provided by the MIL Classification module. MIL Classification module overview Steps to perform machine learning Choose a task and classifier Collect data and build a dataset Train Predict Task and object summary Work-flow summary Basic concepts for machine learning Classifiers, what they are and what they do CNN, segmentation, and object detection classifiers ICNet, CSNet, and ODNet Small ICNet and CSNet Medium ICNet and CSNet Extra large ICNet and CSNet Sizing up your scenario for ICNet and CSNet Tree ensemble classifier Which task is right for you Image classification (predefined CNN classifier) Segmentation (predefined segmentation classifier) Object detection (predefined object detection classifier) Object detection versus segmentation Anomaly detection (predefined anomaly detection classifier) Feature classification (tree ensemble classifier) Requirements, recommendations, and troubleshooting MIL add-ons and updates Computer requirements for a predefined classifiers Troubleshooting for training a predefined classifier GPU and related installations ",
  "wordCount": 164,
  "subEntries": [
    {
      "id": "UG_ML_Classification_and_classifier_overview_MIL_Classification_module",
      "version": null,
      "title": "MIL Classification module overview",
      "subTitles": null,
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\ML_Classification_and_classifier_overview\\MIL_Classification_module.htm",
      "text": " MIL Classification module overview The MIL Classification module lets you apply machine learning technologies such as deep learning and decision trees to perform image classification, segmentation, object detection, anomaly detection, and feature classification. To carry out these tasks, the module uses algorithmic architectures known as classifiers. These technologies, and the tasks you can perform with them, help you solve identification and categorization issues that are not easily addressed with traditional image processing techniques. For example, consider trying to identify a specific type of pasta, among similarly shaped pasta, in images filled with pasta that are randomly positioned, overlapping, and pointing in every direction. Categorizing the kind of pasta in such images by defining and distinguishing the groups of features to extract, as is required by traditional image processing techniques, would likely prove problematic. The MIL Classification module makes this problem more easily solvable by taking a different approach. First, you would collect and label multiple images of each kind of pasta and add them to a dataset context (the MIL object that holds the data). Then, you would use the dataset to train a classifier context (the MIL object that holds the classification technology). During training, the classifier learns to recognize the different kinds of pasta. Once training is done, you can use the trained classifier to properly predict the kind of pasta in your target images. The Classification module provides the tools to use critical machine learning processes, such as building and labeling a dataset, augmenting and preparing data to expand a dataset, controlling train settings, analyzing train and predict (inference) results, and importing a trained ONNX format classifier for prediction. Many of these processes can be performed interactively, using MIL CoPilot. The Classification module also supports the statistical analysis of datasets (MclassStatCalculate()). That is, you can analyze a classifier's performance by computing metrics on predicted datasets. This part of the documentation (Machine learning fundamentals) covers what you need to know about classifiers, datasets, training, and prediction, to perform machine learning with MIL. It is recommended that you read the fundamentals before starting to build your application. Specific instructions vary according to the task you want to perform, and are covered in the Machine learning tasks part. For the most recent documentation of these fundamentals, particularly as they relate to anomaly detection and statistical analysis (MclassStatCalculate()), check for an updated version of the MIL Help online at zebra.com/aurora-imaging-library-help. MIL Classification module overview ",
      "wordCount": 405,
      "subEntries": []
    },
    {
      "id": "UG_ML_Classification_and_classifier_overview_Steps_to_using_the_MIL_Classification_module",
      "version": null,
      "title": "Steps to perform machine learning",
      "subTitles": [
        "Choose a task and classifier",
        "Collect data and build a dataset",
        "Train",
        "Predict",
        "Task and object summary",
        "Work-flow summary"
      ],
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\ML_Classification_and_classifier_overview\\Steps_to_using_the_MIL_Classification_module.htm",
      "text": " Steps to perform machine learning The following steps provide a basic methodology to perform machine learning with the MIL Classification module: Choose a task and classifier. Collect data and build a dataset. Train. Predict. To take advantage of all available resources, such GPU training and prediction (using OpenVINO or CUDA), you should install the required MIL X Service Pack add-on. For more information, see the MIL add-ons and updates subsection of the Requirements, recommendations, and troubleshooting section later in this chapter. Choose a task and classifier The machine learning task you choose is a vital initial decision that guides your application's development. Once you know the task (image classification, segmentation, object detection, feature classification), you can select the classifier and start building your data and training. In general: Image classification is for classifying entire images. Segmentation is for classifying pixels in an image (typically, in image regions). Object detection is for classifying instances of objects (regions) in images. Feature classification is for classifying numerical data (typically features extracted from image). For more information about available tasks, and why to choose one over the other, see the Which task is right for you section later in this chapter. Additional alternatives to performing machine learning with the Classification module include importing a trained ONNX format classifier (a machine learning model) and using it for prediction (MclassPredict()). For more information, see the ONNX section of Chapter 50: Prediction. You can also have Matrox construct and train a classifier for you, using the labeled data that you provide (such classifiers are considered pretrained). For more information, contact customer support. Collect data and build a dataset The data that you collect and use to train your classifier must be correctly labeled and representative of the task you want to perform. This is critical to building a good dataset, and developing a properly trained classifier. Although the kind of data you need is different from task to task, the requirement of having proper data is essential for all. To use your data with MIL, you must add it to a dataset context. Some important recommendations regarding image data (for image classification, segmentation, and object detection): Use MIL CoPilot to build and manage your images datasets. For example, MIL CoPilot lets you interactively create, label, modify, import, and export datasets. When using MIL CoPilot, ensure that you have installed all related MIL updates. For more information, see the Requirements, recommendations, and troubleshooting section later in this chapter. Decouple dataset creation from usage. Since constructing a dataset involves disk storage, it should be done first, properly, and on its own. Once the dataset is ready, you can use it (for example, restore it) for training or prediction. For more information, see the Guidelines for managing an images dataset section of Chapter 48: Datasets. Increase and improve your images dataset, using MclassPrepareData(). For example, you can resize and crop your image data, and you can perform numerous augmentations on it. For more information, see the Data augmentation and other data preparations section of Chapter 48: Datasets. Train Once you have built your dataset, you can train your classifier with it, using MclassTrain(). This requires allocating a training context and a training result buffer. Training for image classification, segmentation, and object detection requires two datasets: a training dataset and a development dataset. Although you can explicitly create these from your single dataset (for example, by using MclassSplitDataset()) and specify them at training time, MIL can automatically split your single dataset into the ones required when you call MclassTrain(). At this time, MIL can automatically augment your data with an internally defined augmentation context. MclassTrain() can also automatically allocate a classifier context based on the specified training context. Training is typically an involved process that includes analyzing training results, modifying settings, modifying data, and re-training the trained classifier context until it is ready for prediction. Predict Once your classifier is trained, you can predict with it, using MclassPredict(). To do so, you must allocate the required result buffers. You can also use prediction to try and label data (assisted labeling), or as part of the training phase; that is, prediction results can cause you to implement more advanced training settings, re-train, and then predict again. Task and object summary The MIL Classification module requires allocating several objects (for example, contexts and buffers) whose type relates to the task and classifier chosen. The details of what you must allocate are discussed in the task specific chapters; the following table represents a summary. Task Image classification Segmentation Feature classification Object detection Classifier context M_CLASSIFIER_CNN_PREDEFINED M_CLASSIFIER_SEG_PREDEFINED M_CLASSIFIER_TREE_ENSEMBLE M_CLASSIFIER_DET_PREDEFINED Specific predefined classifier context M_ICNET_... M_CSNET_... None. M_ODNET Dataset context M_DATASET_IMAGES M_DATASET_IMAGES M_DATASET_FEATURES M_DATASET_IMAGES Data preparation context M_PREPARE_IMAGES_CNN M_PREPARE_IMAGES_SEG None. M_PREPARE_IMAGES_DET Training context M_TRAIN_CNN M_TRAIN_SEG M_TRAIN_TREE_ENSEMBLE M_TRAIN_DET Training result buffer M_TRAIN_CNN_RESULT M_TRAIN_SEG_RESULT M_TRAIN_TREE_ENSEMBLE_RESULT M_TRAIN_DET_RESULT Prediction result buffer M_PREDICT_CNN_RESULT M_PREDICT_SEG_RESULT M_PREDICT_TREE_ENSEMBLE_RESULT M_PREDICT_DET_RESULT Work-flow summary Regardless of the task and classifier you choose, the basic work-flow for using the MIL Classification module does not generally change. This work-flow involves building your data, training the classifier with it, and then predicting with the trained classifier. The time and resources required for each phase, particularly building and training, can vary greatly and is highly contingent on your application. Steps to perform machine learning Choose a task and classifier Collect data and build a dataset Train Predict Task and object summary Work-flow summary ",
      "wordCount": 894,
      "subEntries": []
    },
    {
      "id": "UG_ML_Classification_and_classifier_overview_Basic_concepts",
      "version": null,
      "title": "Basic concepts for machine learning",
      "subTitles": null,
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\ML_Classification_and_classifier_overview\\Basic_concepts.htm",
      "text": " Basic concepts for machine learning The basic concepts and vocabulary conventions for the MIL Classification module are: Artificial Intelligence (AI). The simulation of intelligence in machines. Classes. The unique categories in a finite number of categories that represent the conclusions to a classification problem. Fusilli and macaroni, for example, represent 2 classes for categorizing pasta. Classes can hold supplementary data that is typically used as a visual aid, such as an identifying icon image or color. You can add classes to a dataset context. Classes are also known as class definitions, labels, or outputs. Classification. Applying mathematical learning processes, such as machine learning or deep learning, to solve identification and categorization issues known as classification problems. Typically, such problems are not easily solvable using traditional image processing techniques. Classifier. The mathematical architecture that must be trained to perform classification (predict the class to which a target belongs). You can have a predefined CNN classifier, a predefined segmentation classifier, a predefined object detection classifier, or a tree ensemble classifier. A classifier is also known as a network or a model. Classifier context. A MIL object that stores a classifier and its settings. The terms classifier context and classifier are often used interchangeably. Convolutional neural network (CNN). A type of neural network typically used to learn from, and predict for, image data. Dataset. A type of database containing a labeled series of entries (images or features) with which to train a classifier. Dataset context. A MIL object that stores a dataset and its settings. The terms dataset context and dataset are often used interchangeably. Decision tree. A part of ML that tries to process (separate) data as a collection of decision-based nodes connected by branches. Deep learning (DL). A part of ML focusing on learning techniques related to neural networks. Image classification network (ICNet). Internal Matrox classifiers (networks) that you can specify when you allocate a predefined CNN classifier context to perform image classification; for example, M_ICNET_M. Machine learning (ML). A part of AI focusing on systems that can learn from data to make decisions with minimal human assistance or explicitly programmed procedures. Neural network. Algorithms based on biological neuron network systems that attempt to recognize and distinguish patterns and relationships in the provided data. Object detection network (ODNet). Internal Matrox classifier (network) that you can specify when you allocate a predefined object detection classifier context to perform object detection; for example, M_ODNET. Predefined CNN. A classifier that uses a CNN that was predefined by Matrox, and that must be trained with an images dataset. Once trained, you can use the predefined CNN classifier to perform image classification (classify an entire image). CNN and image classification are used interchangeably unless otherwise specified. Predefined object detection. A classifier that uses an object detection specialized network that was predefined by Matrox, and that must be trained with an images dataset. Once trained, you can use the predefined object detection classifier to segment an image. Predefined segmentation. A classifier that uses a segmentation specialized network that was predefined by Matrox, and that must be trained with an images dataset. Once trained, you can use the predefined segmentation classifier to segment an image. Predicting. Using a trained classifier to identify the class to which the target (image, feature list, or dataset) belongs. Predicting, predict, and prediction are used interchangeably and are also known as classification or inference. Such terms are sometimes used even when the actual MIL prediction function is not. For example, training can be seen as a type of prediction, since the classifier is essentially learning how to predict. Receptive field. The portion of the input image that is visible to the classifier to make a prediction. The size of the receptive field is equivalent to the source layer's image size. Segmentation network (CSNet). Internal Matrox classifiers (networks) that you can specify when you allocate a predefined segmentation classifier context to perform segmentation; for example, M_CSNET_M. Source layer. The input (initial) layer of a classifier (for example, CNN, segmentation, object detection, or ONNX). Training. The process in which the classifier learns to predict the class to which the data belongs. Training context. A MIL object that stores the training settings with which to train a classifier. You can have a CNN training context, a segmentation training context, or a tree ensemble training context. Tree ensemble. A classifier that uses multiple decision trees and bootstrap aggregating. After training this classifier with a features dataset, you can use it to perform feature classification. Weights. Internal and hidden parameters within the classifier that transform data. Classifiers can have millions of weights. During most types of training, modifications to the training mode controls (hyperparameters) can affect how weights are established. Once a classifier is trained, its weights no longer change. Weights are sometimes referred to as learnable parameters. Additional basic concepts and vocabulary conventions for the MIL Classification module, more specifically related to datasets, training and prediction, are listed in their respective chapters. Basic concepts for machine learning ",
      "wordCount": 832,
      "subEntries": []
    },
    {
      "id": "UG_ML_Classification_and_classifier_overview_Classifiers_in_general",
      "version": null,
      "title": "Classifiers, what they are and what they do",
      "subTitles": [
        "CNN, segmentation, and object detection classifiers",
        "ICNet, CSNet, and ODNet",
        "Small ICNet and CSNet",
        "Medium ICNet and CSNet",
        "Extra large ICNet and CSNet",
        "Sizing up your scenario for ICNet and CSNet",
        "Tree ensemble classifier"
      ],
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\ML_Classification_and_classifier_overview\\Classifiers_in_general.htm",
      "text": " Classifiers, what they are and what they do Classifiers are the underlying mathematical architectures that require training with labeled data. Once trained, you can use the classifier to properly predict the class to which new data belongs. The classifier you choose depends on the task you want to perform. Matrox has predefined several classifiers for image classification (M_ICNET_...), segmentation (M_CSNET_...), and object detection (M_ODNET) tasks. Although the underlying structure of these ICNets, CSNets, and ODNets is similar, the specifics of the problems they are designed to handle, and types of the images recommended for them, can differ. Note, there are no predefined tree ensemble classifiers for feature classification. This section discusses classifiers in general. Training a classifier with a dataset and predicting with it is discussed in the later chapters (Chapter 48: Datasets, Chapter 49: Training, and Chapter 50: Prediction). For information about importing a trained ONNX format classifier (a machine learning model) and using it for prediction with MIL, see the ONNX section of Chapter 50: Prediction. CNN, segmentation, and object detection classifiers A CNN is a complex function with millions of weights that, when properly set (successfully trained), can classify an image. Structurally, a CNN is basically implemented as a cascade of layers, including a source layer, one or more hidden layers, and an output layer. The source layer defines the input size of the network, while the output layer establishes the classes. The hidden layers learn how to process the image for the defined classes. Whether you are performing image classification, segmentation, or object detection, the underlying technology is a CNN that is specifically designed for the required machine learning task. Since MIL predefines this classifier architecture, you cannot explicitly control it. Your control lies in training the classifier. During training, MIL establishes the values of the classifier's weights based on your labeled images (training dataset context) and training settings (training context). In this way, the classifier determines the boundaries between classes within a complex internal feature space. The label of the image identifies the class to which it belongs; this class label is known as the ground truth and is usually established by a human (c'est-à-dire, un être humain). For example, a human will label numerous images as either GoodApple or BadApple. The training analyzes these labeled images and iteratively adjusts the classifier's weights so it can identify and differentiate the different images with, ideally, a very low error rate (high accuracy). The goal is to establish a stable set of weights such that the trained classifier can classify similar images as well as a human. Several settings, which you can specify for the training context, can affect how MIL establishes the classifier's weights. These settings are sometimes referred to as training mode settings. Adjusting these settings (such as the learning rate and the number of training cycles or epochs) can help improve how the classifier gets trained on the labeled data. ICNet, CSNet, and ODNet The predefined image classification (ICNet) and segmentation (CSNet) classifiers that you can train can be small, medium, or extra large, and you can choose a specific one. More complicated problems and larger images can call for larger classifiers, which can result in longer training and prediction times. The specific classifier you choose can affect training settings, in particular, the training mode (complete, transfer learning, or fine tuning). The predefined object detection classifier (ODNet) does not have different sizes; that is, you must use M_ODNET. Predefined ICNet, CSNet, and ODNet classifiers do not impose any restriction on the minimum size of training images; however, all training images must be the same size. When predicting with ICNets (image classification) and ODNet (object detection), the size of the target images must be the same as the images used for training. When predicting with CSNets (segmentation), the target image size can be smaller or bigger than the images used to train these classifiers (this requires calling MclassControl() with M_TARGET_IMAGE_SIZE_X and M_TARGET_IMAGE_SIZE_Y). Small ICNet and CSNet The small ICNet (M_ICNET_S) and CSNet (M_CSNET_S) classifiers are compact and typically used for smaller images. The capacity of the network is low, so it might not be suitable for highly complex problems. You can use grayscale (1-band) or color (3-band) images with these classifiers. They are for a complete training. Medium ICNet and CSNet The medium ICNet (M_ICNET_M) and CSNet (M_CSNET_M) classifiers are designed to solve the majority of problems. The capacity of these networks is high enough to cover many problems with a fairly fast prediction speed. You can use grayscale (1-band) or color (3-band) images with these classifiers. They are for a complete training. Extra large ICNet and CSNet The extra large ICNets (M_ICNET_XL, M_ICNET_MONO_XL, and M_ICNET_COLOR_XL) and CSNets (M_CSNET_XL, M_CSNET_MONO_XL, and M_CSNET_COLOR_XL) classifiers have a high capacity, but prediction is slower than the other classifiers. With the general version of these classifiers (M_ICNET_XL and M_CSNET_XL), you can use grayscale (1-band) or color (3-band) images. These classifiers are for a complete training. With a specific version of these classifiers (M_ICNET_MONO_XL, M_ICNET_COLOR_XL, M_CSNET_MONO_XL, and M_CSNET_COLOR_XL), you can use either grayscale (1-band) or color (3-band) images, depending on the one you choose. These classifiers are intended for transfer learning (to build on training done by Matrox). Sizing up your scenario for ICNet and CSNet Many factors can theoretically affect which ICNet or CSNet works best; for example, the complexity of the problem (that is, how challenging is it to perform the image classification or segmentation task), the amount of labeled data that you have (training images), the accuracy that you need, and the required speed of your training and prediction. As a rule of thumb, the harder it is for a human to perform the task (such as identifying an object in an image), the more complex it is for a classifier. Note, some image characteristics that seem innocuous to humans, such as minor variances in rotation and lighting, can prove complex for a classifier. In practice, medium predefined classifiers work best, regardless of your scenario, or the problem's complexity. You should use a medium classifier first, and only try another if results are not what you expect. The following summarizes the typical ICNet or CSNet classifier to choose, based on the given scenario. The given scenario ICNet or CSNet Small Medium Extra Large XL color¹ XL mono¹ I want to explore and experiment. - ? - - - I want to use what is recommended to work best in the vast majority of cases. - ? - - - I have a problem that does not seem very complex, a lot of good - although small - training images, and the medium (recommended) classifier does not work as well as expected. ? - - - - I have a complex problem, a lot of good training images, and the medium (recommended) classifier does not work as well as expected. - - ? - - I do not have enough training images (my problem is either typical or complex), and the medium (recommended) classifier does not work as well as expected. - - - ? ? I cannot pin point the reason, but I have tried the other classifiers and they do not worked as expected. - - - ? ? ¹Matrox has trained the XL color/monochrome classifiers on a large generic dataset. Selecting them requires you to perform transfer learning (that is, you must set M_RESET_TRAINING_VALUES to M_TRANSFER_LEARNING). Note, you can perform transfer learning on any trained classifier. Some additional points to consider, when the medium (recommended) classifier is not working as expected: Do not immediately use another classifier. Instead, try augmenting your data, adjusting the labeling, and tweaking your training settings. You can also add more data to your dataset. If performing segmentation, and the foreground (area to detect) is very small and/or thin, try the small CSNET classifier as it tends to give finer segmentation for less complex cases. If some images are too small, you can resize them. You can also down-sample (shrink) your images to help reduce memory requirements and decrease prediction time. Do not down-sample if it noticeably affects important features, such as causing a defect to disappear or become faint. Although there is no minimum image size requirement, if you have opted for a small classifier and are still not getting the results you need, try using images that are at least 43 pixels. Tree ensemble classifier A tree ensemble classifier is a set of decision trees. A decision tree is a collection of nodes connected by branches. Each node takes in data and either makes a decision on how to best split (branch) the data into 2 other nodes, or makes a final decision on that data (known as a leaf node). Starting with a single node, the data gets processed down the tree until it is no longer splittable and all branches end with a leaf. Those leaves can be considered the classes. To make the classification making process more robust, tree ensembles use many decision trees, rather than using just one. Each tree takes in and classifies the same set of features; the final classification decision is based on the decision of the majority of the trees. To process the features, node by node, tree ensembles use bagging, randomness and multiple learning algorithms (bootstrap aggregating) during the training process. For more information on tree ensemble classifiers, see the Setting up a classifier subsection of the Classifier and training settings for feature classification section of Chapter 55: Feature classification. Classifiers, what they are and what they do CNN, segmentation, and object detection classifiers ICNet, CSNet, and ODNet Small ICNet and CSNet Medium ICNet and CSNet Extra large ICNet and CSNet Sizing up your scenario for ICNet and CSNet Tree ensemble classifier ",
      "wordCount": 1624,
      "subEntries": []
    },
    {
      "id": "UG_ML_Classification_and_classifier_overview_Which_task_is_right_for_you",
      "version": null,
      "title": "Which task is right for you",
      "subTitles": [
        "Image classification (predefined CNN classifier)",
        "Segmentation (predefined segmentation classifier)",
        "Object detection (predefined object detection classifier)",
        "Object detection versus segmentation",
        "Anomaly detection (predefined anomaly detection classifier)",
        "Feature classification (tree ensemble classifier)"
      ],
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\ML_Classification_and_classifier_overview\\Which_task_is_right_for_you.htm",
      "text": " Which task is right for you The problem you want to solve will indicate the machine learning task you should perform. Once you know the task, you will know the type of classifier you should use. Typical problems Task (classifier) to use Image classification (ICNet) Segmentation (CSNet) Object detection (ODNet) Anomaly detection (ADNet) Feature classification (tree ensemble) ONNX (imported model) ¹ Classify entire images (for example, image identification or boolean validation) ? Classify image regions on a pixel level ? Detect instances of regions (objects) in an image ? Detect anomalous images or anomalous pixels in images ? Classify numerical data (typically representing image features) ? Predict with an imported machine learning model ? ¹ The Classification module lets you import a trained ONNX machine learning model into an ONNX classifier context and use it with MclassPredict(). Any machine learning task supported with ONNX is supported for prediction with MIL. For more information, see the ONNX section of Chapter 50: Prediction. Image classification (predefined CNN classifier) Image classification refers to classifying an entire image; it typically requires a predefined CNN classifier context that was defined by Matrox (an ICNet), and that must be trained with an images dataset context. An example of image classification is identifying different types of images that have complex and similar features, such as different types of fabrics or different types of pasta. You can use image classification to predict whether an entire image is one of several classes (such as Pasta1, Pasta2, or Pasta3), or to perform a boolean type of prediction; that is, predict if an image belongs to a bad or invalid type of class (such as NotEnoughPasta) or if it belongs to a good or valid type of class (such as EnoughPasta). The classifier gives each image a score, per defined class, to indicate the degree to which that class represents the image. The class with the highest score is the one that best represents the image. The following example shows an image and its class results. The best class is highlighted. Segmentation (predefined segmentation classifier) Segmentation refers to a somewhat coarse pixel level classification of regions in an image; it typically requires a predefined segmentation classifier context that was defined by Matrox (a CSNet), and that must be trained with an images dataset context. Training such a classifier can let you predict the class of similar regions within similar images on a pixel level. An example of segmentation is identifying the pixels representing the presence of defects that are difficult to distinguish from the surfaces on which they occur, such as scratches or pits on steel, as shown here. Segmentation is often performed when the classification is based on a small area (for example, a defect like a small scratch), relative to the size of the entire scene, and you cannot otherwise delimit the area (for example, with fixturing). In such cases, a local view of the data can improve robustness. That is, rather than being interested in the image as a whole (for example, establishing whether an image is good or bad), you are interested in the pixel data of regions that can potentially occur in that image (for example, identifying the pits or scratches that make the image invalid). Segmentation can also prove useful for other purposes, in particular because you can use the class of each pixel to perform post-processing operations, such as carrying out a type of blob extraction. For example, in the steel defect example, you can see the scratch and pit as two types of blobs extracted from the steel surface background (you can also use this information to calculate a position). Ultimately, the classifier gives each pixel a score, per defined class, to indicate the degree to which that class represents the pixel. The defined class with the highest score is the one that best represents the pixel. With segmentation, the background is itself a class, and it is always processed. If, for example, none of the defect classes are found, the predicted class will be the background. If you want to classify image regions and do not need the class of each pixel within, you might consider performing object detection. For more information, see Object detection versus segmentation. Object detection (predefined object detection classifier) Object detection refers to classifying instances of objects (regions) in an image; it typically requires a predefined object detection classifier context that was defined by Matrox (an ODNet), and that must be trained with an images dataset context. Training such a classifier lets you predict the class of similar objects (regions) within similar images. An example of object detection is finding instances of defects that are difficult to distinguish from the surfaces on which they occur, such as knots on wood, as shown here. Once an instance of a region (object) is found, object detection identifies the class to which the entire region belongs. This distinguishes it from segmentation, which identifies the class of every pixel in the region. Object detection versus segmentation Both object detection and segmentation can be seen as classifying regions in an image. Also, use cases for object detection, such as finding various type of defects, are often like those for segmentation. In general, object detection is preferred when you want to simplify locating instances of objects within an image (that is, you want to identify regions as a whole), while segmentation is performed when you require the high level of precision that it provides (that is, you want to identify the region's pixels). More specifically: Object detection Segmentation The class and location of each instance is returned. Instances are found (predicted) when a rectangular region in the target represents one of the defined classes. For example, the rectangular regions enclosing the SmallKnot and LargeKnot classes. Each rectangular region found represents one class (you have no information about the pixels within the region). The class of each pixel is returned. Finding the class of every pixel can prove critical, if this high level of detail is needed. If not needed, it can prove unnecessarily challenging to identify multiple occurrences of a class. For example, if 2 large knots are side by side and some pixels overlap, object detection can find these as 2 instances of large knots, while segmentation will identify all the pixels as large knots and differentiating them might prove more difficult (there is no notion of instances or occurrences in segmentation). The location of instances returned by object detection can prove useful; for example, you can ignore results that do not occur in a predefined part of the target. The location of segmentation results is not inherently provided (you would have to create some post processing operations to get their general location). Regions are defined by rectangles; this can be seen as a quick way to label your data. Regions can be defined by brushing over the required areas; this can be more time consuming than enclosing regions with rectangles. Anomaly detection (predefined anomaly detection classifier) Anomaly detection refers to finding anomalous images; it requires a predefined anomaly detection classifier context that was defined by Matrox (an ADNet), and that must be trained with an images dataset context that contains only good (non-anomalous images). This can be particularly useful when valid images are plentiful and easy to acquire, while invalid images are rare and can have a variety of abnormalities that make them invalid, such as scratches, chips, dents, and foreign objects. In addition to identifying anomalous images, anomaly detection can also localize the anomalies by identifying their coarse pixel location. Since only valid (non-anomlaous) images are used in training, there is no labeling required for your datasets, which can significantly reduce development time and costs. Although effective as a stand-alone machine learning task, anomaly detection can also be useful as a preliminary training step for other machine learning tasks, such as image classification, segmentation, and object detection. For example, if you ultimately want to identify a variety of different defects in an image, you can use anomaly detection to find all defective images first. This lets you know which images should be sent for manual labeling so the specific defective class description can be assigned, and which images do not need any manual labeling since you have already determined they are defect free. Reliably reducing the amount of image data that requires manual labeling can often cut costs and development time significantly. Feature classification (tree ensemble classifier) Feature classification refers to classifying numerical data; it requires a tree ensemble classifier context that must be trained with a features dataset. In the following example, the classifier identifies the shapes in the image after extracting numerical data from each shape, such as rectangularity, compactness, and elongation. Numerical data for feature classification does not have to originate from an image. For training purposes, you must feed the classifier numerous sets of feature values. Each set is an entry in a features dataset and must be labeled with the class that represents the data. For example, you could have 3 classes called Disk, Square, and Cross. In your dataset, you would provide hundreds of entries that contain feature values (such as rectangularity, compactness, and elongation) and each of those entries would have the label of the class (shape) those features represent. By taking in the labeled sets of feature values, the classifier learns the criteria with which to split the data, node by node, until it is able to properly identify the class that best represents that data (the feature values). Once such a classifier is trained, it should be able to properly identify any similar data that you give it. Which task is right for you Image classification (predefined CNN classifier) Segmentation (predefined segmentation classifier) Object detection (predefined object detection classifier) Object detection versus segmentation Anomaly detection (predefined anomaly detection classifier) Feature classification (tree ensemble classifier) ",
      "wordCount": 1644,
      "subEntries": []
    },
    {
      "id": "UG_ML_Classification_and_classifier_overview_Requirements_recommendations_and_troubleshooting",
      "version": null,
      "title": "Requirements, recommendations, and troubleshooting",
      "subTitles": [
        "MIL add-ons and updates",
        "Computer requirements for a predefined classifiers",
        "Troubleshooting for training a predefined classifier",
        "GPU and related installations"
      ],
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\ML_Classification_and_classifier_overview\\Requirements_recommendations_and_troubleshooting.htm",
      "text": " Requirements, recommendations, and troubleshooting The Classification module has computer related requirements, such as memory and GPU, that are additional to what is already required to run MIL. For example, datasets can have many images, and you must ensure that you have enough RAM to use them for training. There are also several recommendations that you should follow, particularly related to training a predefined CNN, segmentation, object detection, or anomaly detection classifier. To take advantage of available hardware and maximize performance, such as training and predicting on a GPU, you should install the required MIL add-ons and updates. Other than ensuring that you have enough RAM for your datasets, tree ensemble classifiers do not have any additional computer related requirements. To validate that your system can optimally perform image classification (CNN), segmentation, object detection, or anomaly detection, a CNN Train Engine Test is available in the MILConfig utility under the Classification item. MIL add-ons and updates To maximize the capabilities of the Classification module, you should install the latest MIL add-ons and updates, typically related to classification, training, prediction (inference), and MIL CoPilot. MIL add-ons and updates are available for download through the Updates item found in the MILConfig utility. In particular, check for MIL add-ons related to GPU training, OpenVINO predict engines, and CUDA predict engines. For more information, refer to the whatsnew release note. Computer requirements for a predefined classifiers To use a predefined CNN, segmentation, object detection, or anomaly detection classifier, note the requirements and recommendations for the following: GPU and CPU. It is highly recommended to use a GPU for training (CPU training can take significantly longer). Some GPUs only function when connected to a monitor. GPUs are also known as graphic cards or display adapters. Prediction might be faster on a GPU. MIL provides an example, PredictEngineSelection.cpp, to help you pick the fastest prediction engine available to you. GPU related requirements and recommendations can depend on whether you have installed the latest MIL add-ons /updates in MILConfig. The GPU must be from NVIDIA; for example, Quadro P1000, RTX 3060, and RTX 4070. The NVIDIA GPU driver version must be at least 452.39. The supported GPU compute capabilities range from 3.5 to 8.9; the recommended minimum is 6.0. For more information on compute capability, refer to the CUDA GPUs | NVIDIA Developer page at https://developer.nvidia.com/cuda-gpus (support for CUDA on NVIDIA GPUs is provided with a MIL X Service Pack Add-on / update). The recommended graphics driver for Intel Integrated Graphics when using the Intel OpenVINO provider for predicting is 27.20.100.9079 (support for OpenVINO prediction is provided with a MIL X Service Pack Add-on / update). When using a GPU, one CPU core is still needed. Using a CPU with a higher clock rate, rather than a higher number of cores, is recommended. If using the CPU for training, a high-end multi-core CPU is recommended. It is recommended to have at least 4 gigabytes of GPU memory. Having more memory is preferable, as it allows for a larger mini-batch size during training. Although it is possible to use eGPUs (external GPUs) for training, they are designed for gaming and have not been tested. To use an eGPU, try training with it on a laptop with a Thunderbolt 3 port. eGPUs are not supported on the desktop. While training on the GPU, avoid running any other application on it. Operating system and main memory. You must use a 64-bit Windows 10 or above operating system. It is typically required that the main memory is at least twice the size of the GPU's memory. As previously discussed, you must have enough memory to process datasets, which can be large (in particular, for segmentation, object detection, and anomaly detection). SSD. It is recommended to store your datasets on an SSD to reduce the time required for operations such as training and prediction. Consider the size requirements of your dataset before choosing an SSD. Disable features and processes that can severely reduce disk performance, such as file indexing and anti-virus processes. It is recommended to use the latest update of MIL CoPilot for managing your images datasets. For additional examples and utilities, refer to Matrox Imaging in GitHub. Troubleshooting for training a predefined classifier If you are having trouble using a predefined CNN, segmentation, object detection, or anomaly detection classifier on your computer, you can call MclassInquire() to retrieve information that can help diagnose the problem (for example, training errors or training very slowly). For example, you can inquire: The general engine (GPU or CPU) that MIL is using to train (M_TRAIN_ENGINE_USED and M_TRAIN_ENGINE), or a description of the specific engine (M_TRAIN_ENGINE_USED_DESCRIPTION), for example, \"GeForce RTX (tm) 2060 6GB\" and \"Intel(R) Core(TM) i7-8700 CPU 3.20GHZ\". To train properly, MIL should use the engine you expect (preferably, the best compatible GPU). The status of the GPU (M_GPU_TRAIN_ENGINE_LOAD_STATUS) or CPU (M_CPU_TRAIN_ENGINE_LOAD_STATUS) training engine. To train properly, MIL must successfully load the train engine. Whether a training DLL is installed (M_TRAIN_ENGINE_IS_INSTALLED). To train properly, MIL must successfully install the training DLL. Whether your training images adhere to the requirements of the classifier. Image requirements can include data type and depth (M_TYPE), as well as the number and order of bands (M_SIZE_BAND and M_BAND_ORDER). Invalid training images lead to invalid training; it is recommended to check these requirements after training to ensure all images are valid. Whether the selected training mode (complete, fine tuning, or transfer learning) is allowed, given your classifier's current configurations (M_TRAINABLE_COMPLETE, M_TRAINABLE_FINE_TUNING or M_TRAINABLE_TRANSFER_LEARNING). Not all training modes support all training settings; for example, you cannot change the number of classes if you are using a fine tuning training mode. To retrieve information about the general state of the training, call MclassGetResult() with M_STATUS. The status can give you critical information, such as whether the training (or prediction) was successful, or whether it encountered an issue (for example, not enough memory or a non-finite computational value). Similar information is also available for prediction (for example, M_PREDICT_ENGINE_PROVIDER, M_PREDICT_ENGINE_DESCRIPTION, and M_PREDICT_ENGINE_PRECISION, and M_STATUS). GPU and related installations You should install the most appropriate drivers and updates for any machine learning component you are using, such as GPUs and iGPUs. These drivers and updates are not specific to MIL and are typically available for download by right-clicking on the device name (such as the specific GPU you are using) from the Display adapters item found in Microsoft Window's Device Manager. If necessary, you can follow these steps to verify whether your GPU is installed correctly: Open the Windows Command Prompt (press the Windows key + R and type \"cmd\"). Type \"devmgmt.msc\". Ensure that the GPU is working correctly; for example, there should not be an exclamation point. The following is an example of how a properly functioning GPU is listed: Ensure that you are not using a GPU that should be connected to a monitor and is not. The following is an example of how an improperly connected GPU is listed: If the problem persists, install the latest display driver for your GPU. Requirements, recommendations, and troubleshooting MIL add-ons and updates Computer requirements for a predefined classifiers Troubleshooting for training a predefined classifier GPU and related installations ",
      "wordCount": 1194,
      "subEntries": []
    }
  ]
}]
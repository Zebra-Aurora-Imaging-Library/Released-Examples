[{
  "id": "UG_color",
  "version": "2024020714",
  "title": "Color processing and analysis",
  "subTitles": null,
  "location": "MIL UG P03: 2D processing and analysis",
  "pageURL": "content\\UserGuide\\color\\ChapterInformation.htm",
  "text": " Chapter 22: Color processing and analysis This chapter explains how to use the MIL Color Analysis module to perform color-based procedures such as color transformation using relative color calibration, color matching, projection, and distance. This chapter also explains how to process color images and convert colors to different color spaces. Color processing and analysis overview Basic concepts for the MIL Color Analysis module Color spaces and converting between them Color spaces RGB HSL HSV CIELAB Source color space Converting between color spaces Normalizing the RGB color space Reference color space Gamma correction Relative color calibration Steps to performing relative color calibration Color-samples and color elements Reference color-sample Operation mode Histogram-based Color-to-color Global mean variance Color calibration intent Computation option Selecting strategy Converting to grayscale Extracting the luminance Principal component projection Using a principal component projection for converting to grayscale Distance between colors Differences in distances Color distance types Euclidean distance Manhattan distance Mahalanobis distance Delta-E distance Advanced CIE distance types Choosing a distance type Color matching Steps to performing color matching Basics of color matching Color identification Supervised color segmentation Defining and adding color-samples and color elements Area identifier image Acceptance Color-sample acceptance (for the color-sample's match score) Relevance acceptance (for the target area's relevance score) Distance tolerance Tolerance mode Operation mode and distance type M_STAT_MIN_DIST operation mode M_MIN_DIST_VOTE operation mode M_HISTOGRAM_MATCHING and M_HISTOGRAM_VOTE operation modes Basic results Best-matched color-sample Match status Match score and relevance score Outlier coverage and color-sample coverage Color distance Image results Target areas Color-samples and color elements Background and outliers Inverting colors Advanced color matching settings and concepts Color band specification settings Distance normalization settings Color space encoding Performing the encoding Histogram matching concepts Color separation Separation operation Color statistics Covariance Principal components Source label Color processing and analysis examples Example of relative color calibration operations Example of color processing operations ",
  "wordCount": 308,
  "subEntries": [
    {
      "id": "UG_color_Color_processing_and_analysis_overview",
      "version": null,
      "title": "Color processing and analysis overview",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\color\\Color_processing_and_analysis_overview.htm",
      "text": " Color processing and analysis overview Although most cameras can acquire color images, most analysis modules only operate on monochrome images. In these cases, you must do one of the following: convert the color image to grayscale, create a child buffer from one of the three color bands, or copy one of the three color bands to a 1-band buffer. Using MIL-Lite, you can perform these operations and store the transformed data in a monochrome buffer. This data can then be passed to an analysis module, which typically requires the full version of MIL. You can, however, implement advanced color processing and analysis on color images directly, with the MIL Color Analysis module. You can use this module to perform the following color-based procedures: Transformation (relative color calibration). The transformation operation allows you to adjust color data according to a specific color reference, using relative color calibration. This can be useful to, for example, homogenize colors that can appear unalike when taken with different cameras or illuminants. Distance. The distance operation allows you to calculate the difference in color between two images or to calculate the distance between a color image and a color constant. This can be useful to, for example, visualize the difference in color between images, or detect color defects. Matching. The matching operation allows you to define color-samples composed of individual color elements, and use them for color identification or supervised color segmentation. This can be useful to, for example, identify the color of an object found with a pattern recognition module (such as MIL Model Finder), or segment colors to determine the percentage of color in an image. Projection. The projection operation allows you to separate a selected color from a rejected one, convert color images to grayscale, or calculate an image's covariance matrix or its principal color components. This can be useful to, for example, discard an unwanted color from an image, or perform a mathematically optimal color-to-grayscale transformation. The full version of MIL is required to use the MIL Color Analysis module. Color processing and analysis overview ",
      "wordCount": 345,
      "subEntries": []
    },
    {
      "id": "UG_color_Basic_concepts",
      "version": null,
      "title": "Basic concepts for the MIL Color Analysis module",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\color\\Basic_concepts.htm",
      "text": " Basic concepts for the MIL Color Analysis module The basic concepts and vocabulary conventions for the MIL Color Analysis module are: Area identifier image. An image, provided with the color matching operation, that specifies the target areas. Background pixels. Pixels outside the target areas. Best-matched color-sample. The color-sample that most matches a target area's color, with respect to all color matching constraints. Color distance. The numerical difference between two colors. Color-sample element. The individual color data entities that define a color-sample. Color identification. Matching the color of each target area with the best color-sample. Color matching. Calling McolMatch() to perform color identification or supervised color segmentation. Color matching context. A MIL object that stores all color-samples and color matching settings. Color-sample. The information, in the context, that defines the color to be processed using relative color calibration or color matching. For relative color calibration, color-samples have an associated color-mapping between it and the reference color-sample. Color space. A mathematical model, that typically has 3 components (for example, RGB, HSL, LAB), with which to describe colors. Color space encoding. Color transformation from the range represented in an image buffer (for example, 0 to 255) to its native (theoretical) range (for example, all real numbers between 0 and 1). First principal component. The strongest principal component vector computed from a PCA. If no principal component is the strongest, one is arbitrarily considered the first principal component. IEC. Refers to the International Electrotechnical Commission, a globally-recognized standards organization in the field of electrotechnology. Match score. A measure of similarity between the color of the target area and the color of the color-sample, when performing color matching. Outliers. Pixels, within a target area, that do not relate to any color-sample. Principal component analysis (PCA). A mathematical analysis which, when applied to a color image's data, results in vectors pointing towards the direction of maximum color variance. Each vector is considered a principal component. There are as many principal components as there are color bands. Reference color-sample. The information, in the relative color calibration context, with which to establish the color mapping used in the relative color calibration. Reference color space. The standard used to interpret the color space data (for example, sRGB). Relative color calibration. Calling McolTransform() to adjust an image's color data according to the mapping associated with a color-sample in a relative color calibration context. Relative color calibration context. A MIL object that stores the reference color-sample, the color-samples, and the color mapping with which to perform relative color calibration. Relevance score. A measure of confidence associated with the color match score. Source color space. The color space with which the MIL Color Analysis module interprets color. sRGB. Standard RGB specifications, as defined by the IEC Project Team 61966-2-1. Supervised color segmentation. Matching the color of each target area pixel with the best color-sample. Used to determine the proportion of color in a target area, based on the color-samples. Target. The image with which to perform the color matching. Target area. A section of the target with which to match a color-sample. Triplet. A color-sample defined with three explicit color component values. Basic concepts for the MIL Color Analysis module ",
      "wordCount": 530,
      "subEntries": []
    },
    {
      "id": "UG_color_Color_spaces_and_converting_between_them",
      "version": null,
      "title": "Color spaces and converting between them",
      "subTitles": [
        "Color spaces",
        "RGB",
        "HSL",
        "HSV",
        "CIELAB",
        "Source color space",
        "Converting between color spaces",
        "Normalizing the RGB color space",
        "Reference color space",
        "Gamma correction"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\color\\Color_spaces_and_converting_between_them.htm",
      "text": " Color spaces and converting between them A color space is a mathematical model that typically has three components (bands) with which to describe color data, such as RGB (red, green, blue). Regardless of whether you are using the MIL Color Analysis module to perform color transformation using relative color calibration (McolTransform()), matching (McolMatch()), projection (McolProject()), or distance (McolDistance()), color data is always interpreted according to a specific color space. Color spaces The major color spaces used today are RGB, HSL, HSV, and CIELAB, each of which has unique qualities with which to represent color. The color space that you use throughout an application must be consistent otherwise results can be skewed. For example, when calculating the distance between two color images or when performing a color match, all your color data should be in the same color space. For McolTransform() and McolMatch(), you must also consider the context's source color space. For more information, see the Source color space subsection of this section. RGB RGB is based on red, green, and blue color component (band) values. Typically, these components are directly used for acquiring and displaying color. For example, when displaying a color image buffer, the first band is routed to the monitor's first output channel (usually red), the second band to the second channel (usually green), and the third band to the third channel (usually blue). The RGB color space can be seen as a cube with a red, green, and blue axis. Colors located at the origin, [0, 0, 0], are considered to be black, while colors located at [255, 255, 255] (for an 8-bit buffer) are considered to be white. All other colors can be represented as a combination of red, green, and blue values within this range. Acquisition and display devices can render RGB data differently. Since RGB maps to such devices, it is a device-dependent color space. Theoretically, there are as many RGB color spaces as there are color devices. Although there will always be some variance, color devices typically adhere to certain internationally accepted standards. To interpret color space data, MIL uses standard RGB specifications (sRGB), as defined by the International Electrotechnical Commission (IEC) Project Team 61966-2-1. HSL HSL is established from an RGB color model, but is based on hue, saturation, and luminance color component (band) values. Since such components are generally more intuitive, HSL can be seen as a color space designed to mimic the human way of describing colors. Like RGB, HSL is device-dependent. In RGB, every color is a mixture of red, green, and blue, which can make it difficult to ascertain the exact component values of a particular color. However, in HSL, the color's hue is stored as a separate component (H), which is represented as an angular position on a circular color disk. The other components control only the color's purity (S) and lightness (or luminance, L), which can be used to alter the color's quality, but not the color's basic hue. In the diagram below, the lightness (or luminance) component extends from one point of a bicone to the other, from black (0% lightness) to white (100% lightness). The colors at the center disk are situated at 50% lightness. With the HSL model, band independence makes color manipulation much simpler. Most applications that allow for an interactive manipulation of colors represent the color with HSL. You can, for example, perform color matching with the hue (color) band only, using McolControl() with M_BAND_MODE set to M_COLOR_BAND_0. This can solve certain problems, such as matching dark orange and bright orange, which can be difficult in RGB. Also, matching the hue independently of the luminance can be useful if your image has non-uniform lighting, shadows, or highlights. HSV HSV resembles HSL, except the third component is called value, and represents brightness as if the white point of the HSL model has been compressed to the central disk. In the diagram below, the model is shown as a simple cone, with hue (H) positioned around a circular disk, and saturation (S) increasing from the center out. The value (V) component ranges from black (the far point of the cone below the disk) to white (the center of the disk). In this model, 100% brightness (or value) corresponds to maximum color richness, offering an intuitive advantage over HSL, which represents the richest colors at only 50% lightness. The HSV color space reflects how paint colors change when creating tints (adding white) or shades (adding black), which is intuitively easy to understand. As with HSL, the separate bands allow for interactive manipulation. CIELAB CIELAB (or LAB) is based on the color's luminance (L), its position between red and green (A), and its position between yellow and blue (B). Unlike RGB and HSL, LAB is intended to be device-independent, and was developed as a distinct color model intended to represent a completely human interpretation of color using statistical data taken from visual experiments. Color differences in LAB vary proportionally with human perception. For example, if two colors are at a distance of 5, they will appear roughly 5 times as different as two colors at a distance of 1. LAB was designed to be perceptually uniform, making it a good space to measure color difference. Since LAB is based on color perception, its mathematical model better represents how humans distinguish color, and color differences are more meaningful. This can be seen in the following example, where you have to choose which color, A or B, is closer to color X. Mathematically, color A is the closest color, in an RGB color space. However, color B is intuitively closer, which is also what the distance in the LAB color space mathematically represents. It can be preferable to use CIELAB over RGB since, like HSL, you can use CIELAB to discard the luminance (band L), and perform color matching with only band A and band B. Also, CIELAB can be more robust with colors that are visually alike, especially for minor color differences. For example, when matching a red target among color-samples with similar shades of red, CIELAB can outperform RGB and HSL. With CIELAB, distances have been standardized by the International Commission on Illumination (CIE). A color distance of 1 with CIELAB corresponds to the smallest possible color difference a human can perceive. Source color space When allocating a color matching or a relative color calibration context using McolAlloc(), you must set the context's source color space to one of the following color spaces: M_RGB, M_HSL, or M_CIELAB. Relative color calibration contexts must be M_RGB. Note that you should not confuse \"color space\", which is the mathematical model used to describe color, and \"source color space,\" which is the specific color space in which the color matching context is defined. Advanced users should also be aware of the \"reference color space,\" which refers to how MIL interprets the color data of the source color space when performing an internal conversion before the color operation. For more information, see the Converting between color spaces subsection of this section. MIL considers all color data required for color matching or relative color calibration to be in the source color space specified for the context. For example, if you set an M_HSL source color space, MIL assumes that the target image (McolMatch()), as well as the color-sample elements (whether they be defined by an image or by triplet values using McolDefine()), are also HSL. If they are not, MIL still interprets and matches the color data as if it was HSL and does not produce an error. Therefore, unless all the color data is consistent with the source color space, you can receive misleading results. Since color data is interpreted according to the context's source color space, be careful when restoring color images into an automatically allocated data buffer using MbufRestore(), when the color's type is not explicitly known. For example, if you are working in an M_RGB color space, and you use an image restored from a file that contained YUV data, the Y, U, and V bands will be incorrectly read as R, G, and B bands, respectively. Instead, you should use MbufAllocColor() to allocate a color data buffer with an explicit data type (for example, M_RGB24), and then use MbufLoad() to load the data from the color image into the specified color data buffer. In this case, MbufLoad() will convert the image's color data to the proper type, as defined by the buffer (for example, M_RGB24). When matching, you must set the encoding of the source color space according to the actual dynamic range of your color data, using McolControl() with M_ENCODING. By default, an 8-bit source color space encoding is assumed, regardless of the depth of your buffers. Since MIL processes color data as is (even if it is inconsistent), you must remember to modify the encoding control according to your data, otherwise you can get incorrect results. For example, you will not get an error if you match a 16-bit target area with an 8-bit color-sample. Therefore if your color data was acquired with a 16-bit camera, you should set M_ENCODING to M_16BIT. For more information, see the Color space encoding subsection of the Advanced color matching settings and concepts section later in this chapter. Converting between color spaces You can convert color data between HSL and RGB color spaces using MimConvert() with M_HSL_TO_RGB or M_RGB_TO_HSL. For efficiency when converting from a 3-band (RGB) buffer, you can calculate just the hue component of the HSL color space and save it in a 1-band buffer (M_RGB_TO_H). For more information on the algorithm used to convert between HSL and RGB color spaces, see the Extracting the luminance subsection of the Converting to grayscale section later in this chapter. When using an RGB source color space (McolAlloc()), you can use McolSetMethod() with the ConversionModeOrComputeOption parameter to convert your RGB data to CIELAB or HSL before performing McolMatch(). This type of conversion can be useful if, for example, you have allocated a color matching context in an RGB color space, but you want to calculate with CIELAB or HSL colors. Note that when performing multiple match operations on RGB data in CIELAB or HSL, you might want to consider providing images that have already been converted to CIELAB or HSL, which might be faster. When converting with McolSetMethod(), the converted color is essentially the same as the source color, except that it is represented in a different color space, and therefore has different (converted) color band values. For example, pure red has an RGB value of (255, 0, 0), while in HSL the same red has a value of (0, 255, 128). As shown in the following illustration, if you are using an RGB source color space, all color data (such as the color-sample elements) is assumed to be in RGB. However, if you use McolSetMethod() to convert from RGB to HSL, the match operation is performed with the HSL version of the color. Note that in the following example, the target image is already in HSL. You can also use MimConvert() to convert sRGB to LAB (M_SRGB_TO_LAB), and to convert LAB to sRGB (M_LAB_TO_SRGB). In these cases, your RGB data must adhere to the IEC standards for sRGB (previously discussed); otherwise, unexpected results can occur. To obtain sRGB color, you can grab images using an sRGB camera and illuminant in a favorable environment. You can also perform a relative color calibration on an RGB image to transform it to sRGB, using McolTransform() and an sRGB ColorChecker. For more information, see the Relative color calibration section later in this chapter. Instead of LAB, you can use MimConvert() to convert sRGB to and from LCH (M_SRGB_TO_LCH or M_LCH_TO_SRGB). The LCH color space is a reinterpretation of LAB that emphasizes polar coordinates (absolute black and absolute white), which can make LAB colors easier to deal with (similar to HSL being easier to deal with than RGB). LCH is based on luminance (L), chroma (C), and hue (H). If you are converting to and from linear sRGB data (not gamma corrected), you must use the linear version of the sRGB conversion values (M_SRGB_LINEAR_TO_LAB, M_SRGB_LINEAR_TO_LCH, M_LAB_TO_SRGB_LINEAR, or M_LCH_TO_SRGB_LINEAR). For more information, see the Gamma correction subsection of this section. MimConvert() is available with MIL-Lite, while McolSetMethod() requires the full version of MIL. Normalizing the RGB color space Normalizing a color space can help remove unwanted variations in shades from an image. For example, using blob analysis on images with shadows or sharp variations of the same color might find false borders, and return inaccurate results. You can fix the unwanted variations in color shades by normalizing the colorspace. Using MimConvert() with M_RGB_NORMALIZE on an RGB image will normalize the color of each pixel to smooth variations in shade. The components of each individual pixel are divided by the sum of that pixel's component values. This division results in a number between 0 and 1 (inclusive) for every component, and has the effect of reducing variations over similar colors. If the destination buffer is an integer buffer, the normalized components are then multiplied by the depth of the buffer; this scales the values, but retains the relative difference between the other colors. After converting the RGB color data to normalized RGB data, you can convert it to any other color space with MimConvert(), and keep the normalized separations between colors in your image. Reference color space To interpret the RGB source color space data when using McolSetMethod() with the CIELAB conversion mode, the color matching context uses standard RGB specifications (sRGB) as its reference color space, which you can specify using McolAlloc() with the ColorSpaceProfileId parameter. sRGB specifications are defined in the International Electrotechnical Commission (IEC) 61966-2-1. Note that when converting RGB using the HSL conversion mode, the reference color space is not required since HSL is based on the RGB color model. Gamma correction Depending on the display device, the colors of grabbed images can appear over-saturated or too dark. This is typically caused by a non-linear relationship between a pixel's value and its displayed intensity. To compensate for this discrepancy, the acquisition process (for example, the camera doing the grab) can apply a gamma correction, thereby leaving your color data in a corrected (non-linear) state. When converting RGB source colors before the match using McolSetMethod() with the CIELAB conversion mode, MIL requires uncorrected (linear) color data. Therefore, if gamma correction has been applied on the RGB source color data, you must remove it using McolControl() with M_CONVERSION_GAMMA set to M_ENABLE. By default, MIL assumes the RGB source color data is in a corrected (non-linear) state, and removes the correction. MIL also assumes that the data has been corrected according to the standards of the context's reference color space, as specified using McolAlloc() with the ColorSpaceProfileId parameter. MIL uses these standards (for example, sRGB) to remove the gamma correction. If your RGB source color data is in an uncorrected (linear) state, you must set M_CONVERSION_GAMMA to M_DISABLE. Regardless of whether your RGB source color data is in a corrected or uncorrected state, MIL removes or keeps the correction according to M_CONVERSION_GAMMA and, in the case of removing the correction, applies the standards of the reference color space (sRGB). Therefore, you will not receive an error if, for example, your color data is not corrected (linear) and you set M_CONVERSION_GAMMA to M_ENABLE (to remove the correction). Such a scenario would, in all likelihood, render inaccurate results. Unless you are using the CIELAB conversion mode in an RGB source color space, M_CONVERSION_GAMMA is ignored. Color spaces and converting between them Color spaces RGB HSL HSV CIELAB Source color space Converting between color spaces Normalizing the RGB color space Reference color space Gamma correction ",
      "wordCount": 2621,
      "subEntries": []
    },
    {
      "id": "UG_color_Relative_color_calibration",
      "version": null,
      "title": "Relative color calibration ",
      "subTitles": [
        "Steps to performing relative color calibration",
        "Color-samples and color elements ",
        "Reference color-sample",
        "Operation mode",
        "Histogram-based",
        "Color-to-color",
        "Global mean variance",
        "Color calibration intent",
        "Computation option",
        "Selecting strategy"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\color\\Relative_color_calibration.htm",
      "text": " Relative color calibration Various conditions, such as different cameras and illuminants, can cause colors to seem dissimilar. With McolTransform(), you can perform a relative color calibration. This transforms an image's color data to better resemble the color data of a specified reference. Transforming colors to be consistent can increase the accuracy of subsequent color operations, such as distance and matching. For example, you have a particular camera in an assembly line that interprets color images of food as you expect. You must then compare these colors, using a color distance operation, with images of food taken in other assembly lines with other cameras that interpret the same colors differently. Flagging problems due to color inconsistencies in the food across the multiple lines can prove difficult since colors are already inconsistent based on the cameras taking the images. To manage this, you must establish one or more color-samples, each of which represents a particular camera's interpretation of color. You must also establish the reference color-sample. This is a unique type of color-sample that represents the color you expect. For example, the color data taken by camera 1. For each color-sample, MIL calculates a mapping (McolPreprocess()) to transform its color data to resemble the color data of the reference color-sample (such as having a similar color temperature and distribution). The context holds the color-samples, the reference color-sample, and the operation settings (strategy) with which to establish the color mapping. A preprocessed context holds the actual color mapping. By specifying the color-sample that correlates to the camera capturing the image to transform (McolTransform()), MIL applies the associated mapping to correct the image's color to better resemble the reference color-sample. All colors from all cameras can now be coherent (relative to the reference color), and subsequent color operations can be more accurate. MIL assumes all color data that a relative color calibration context uses is 8-bit RGB (3-band). If color data is inconsistent (for example, between the source image and the color-sample), MIL still processes it all as 8-bit RGB. Steps to performing relative color calibration The following steps provide a basic methodology for using relative color calibration in MIL: Allocate a relative color calibration context, using McolAlloc() with M_COLOR_CALIBRATION_RELATIVE. The context holds the color-samples with their associated mapping and the reference color-sample. Specify the processing strategy, using McolSetMethod(). The strategy sets how to calculate the color-sample mappings. Define the color-samples using McolDefine(). You must define one or more color-samples. You must define one, and only one, reference color-sample. Preprocess the context, using McolPreprocess(). Preprocessing calculates and associates a mapping with each color-sample in the context. There is no mapping for the reference color-sample. Optionally, perform drawing operations, using McolDraw(). Transform color data with relative color calibration, using McolTransform(). You do not specify a result buffer in which transformation results are written. MIL produces the transformed image directly in the destination image buffer specified with McolTransform(). Typically, you use the transformed image in subsequent color based operations, such as color matching or distance. Free all your allocated objects, using McolFree(), unless M_UNIQUE_ID was specified during allocation. Context modifications are part of the preprocessing (or training) phase. For such changes to take effect, you must typically call McolPreprocess() before McolTransform(). To inquire if this is necessary, use McolInquire() with M_PREPROCESSED. You can also use McolInquire() to retrieve other information about the context in general, as well as its color-samples and the reference color-sample. McolTransform() corrects an image's color data according to the preestablished mapping of the specified color-sample. No consideration is made to other specifics of the application, such as the particular camera or illuminant that you are using. Color-samples and color elements Color-samples are unified sets of color data that you define using McolDefine(). As discussed, operations such as McolPreprocess() and McolTransform() require color-samples. MIL stores them with the context. Color-samples are made up of one or more color elements. When you define a color-sample, you automatically add the first color element to the color-sample. You can define a color-sample (and consequently the first color-sample element) from: A region of an image (M_IMAGE). Three explicit color component values (M_TRIPLET). By default, when defining image type color-samples for relative color calibration, MIL uses the color data of each individual pixel in the color-sample to perform its calculations. For certain operational strategies, you can specify to use color statistics instead (such as the mean color of the color-sample). For more information, see the Selecting strategy subsection of this section. Note that when using color-samples for color matching, MIL always uses an estimation of the color (such as the mean). To add color elements to color-samples that already exist in the context, specify M_ADD_COLOR_TO_SAMPLE. MIL now considers this the color of the color-sample. Subsequent operations use this color data as if it was originally part of the color-sample. If the type of the existing color-sample is M_IMAGE, the type of any additional color element must be M_IMAGE. Also for M_IMAGE, you must ensure that the source image from which to define the additional color-sample elements has the same format as the original source image used to define the initial color-sample element. If the color-sample is M_TRIPLET, additional color elements must be M_TRIPLET. When adding a color-sample or color element with M_IMAGE, you must specify a source image, and optionally specify a rectangular region within it. For good results, use the best source image possible. You can apply a mask to the color-sample or color element, using McolMask(), to ignore unwanted data, or specify a non-rectangular region. If the source image buffer has a region of interest (ROI), which can be set using MbufSetRegion(), MIL ignores all pixels outside the ROI when defining the color-sample. MIL ignores these pixels as if you had explicitly masked them with McolMask(). The color-sample index starts at 0. Each subsequent color-sample you add to the context is given a sequential index number. If you delete a color-sample, MIL shifts all entries with higher indices down by one. This also applies when adding or deleting color elements from a color-sample. To consistently access a color-sample using the same value, you can assign a label to a color-sample. If you don't, MIL assigns one automatically. To change it afterward, use McolControl() with M_SAMPLE_LABEL_VALUE. You can delete color elements from color-samples, or color-samples from the context, using McolDefine(). You can delete all color-samples at once (you must add color-samples one at a time). When you add, modify, or delete a color-sample, you must preprocess the context (McolPreprocess()) before any subsequent call to McolTransform(). Reference color-sample As previously discussed, the reference color-sample is a unique type of color-sample. It defines the reference color of the relative color calibration context. After preprocessing the context, all its color-samples have an associated mapping to transform them according to the reference color-sample. Each relative color calibration context must have one reference color-sample. Managing the reference color-sample is nearly identical to managing any other color-sample. Use M_REFERENCE_SAMPLE to add the reference color-sample to the context, delete the reference color-sample from the context, or add a color element to the reference color-sample. To delete a color element from the reference color-sample, use M_REFERENCE_SAMPLE_ITEM(). Operation mode With McolSetMethod(), you can set the operation with which to establish the color mappings that MIL associates to the color-samples. This can affect the precision, speed, and flexibility of your relative color calibration application. The following operations can be set: M_HISTOGRAM_BASED (default), M_COLOR_TO_COLOR, or M_GLOBAL_MEAN_VARIANCE. M_HISTOGRAM_BASED and M_GLOBAL_MEAN_VARIANCE are global type operations. MIL establishes the color mapping using images as a whole. M_COLOR_TO_COLOR is a local type operation. MIL establishes the color mapping point-to-point. Global operations are more flexible. In certain cases, M_GLOBAL_MEAN_VARIANCE can also be faster. Point-to-point operations are more precise, though they have more requirements, such as data alignment or pairing (transform the color at this exact point to the color at that exact point). You can further affect how MIL calculates the color mapping by using the color calibration intent settings in McolSetMethod(). These settings enhance or curtail the inherent precision of the operation. MIL considers the operation and quality settings to be the strategy with which to establish the color mappings. For more information, see the Color calibration intent subsection of this section. Keep in mind that strategies calling for strict adherence to color data can cause overfitting. This refers to misleading transformations resulting from over emphasizing minor color fluctuations or outlier data. To avoid overfitting, select the proper strategy given the precision requirements of your application and provide realistic color data when establishing the mappings between color-samples and the reference color-sample. For more information, see the Selecting strategy subsection of this section. Histogram-based Histogram-based transformations are well known in the field of color image processing. Such transformations create a color-sample's mapping based on transforming its color distribution to resemble the color distribution of the reference color-sample. Histogram-based transformations are typically effective when dealing with similar scenes (similar images and color data). Previous examples showing the color correction of foodstuff use M_HISTOGRAM_BASED. These plates of food look generally the same; their colors do not differ radically and outlier color data is minimal. The animation below shows how histogram-based color calibration is performed. Unlike M_COLOR_TO_COLOR, M_HISTOGRAM_BASED does not require a point-to-point correspondence between the color-sample and the reference color-sample. Color fluctuations, misalignment, and outliers have less effect on histogram-based transformations and overfitting issues are less likely. For example, the foodstuff images are not aligned, and the histogram-based relative color calibration works well. To reduce processing time, consider resizing very large images. This also reduces memory allocation. Color-to-color For M_COLOR_TO_COLOR, MIL creates a color-sample's mapping based on transforming its color to the color of the reference color-sample on a point-to-point basis. This makes M_COLOR_TO_COLOR the most precise operation. It is most effective when your application requires an exact reproduction of color. Since the color-to-color operation depends on a strict point-to-point correspondence to establish a color-sample's mapping, consider using a ColorChecker image to define and preprocess your color-samples. ColorCheckers are well known in the color management field. They hold the color information you intend to process in a square grid. Although not mandatory (you can use any image), ColorCheckers can prove convenient. You can grab them with the required camera and illuminant to create image type color-samples (M_IMAGE). Since M_COLOR_TO_COLOR deals with the exact reproduction of color, using ColorCheckers with explicit color values (M_TRIPLET) can also prove effective. The following is an example of the type of color issues that the color-to-color operation can correct. These images show the same 2 posters with different colors caused by 2 different cameras and illuminants. The first poster has the color you want. The second poster has the color to correct. Using the same 2 cameras and illuminants, you can grab a ColorChecker image. You can now use M_COLOR_TO_COLOR to create a mapping to transform the color data of the second ColorChecker (color-sample) to the color data of the first ColorChecker (reference color-sample). You can then use the mapping to correct the color of the poster. You can use any image with a color-to-color operation, as long as you can pair the colors in the color-sample with the colors in the reference color-sample. ColorCheckers simplify this. When using M_COLOR_TO_COLOR, it is recommended to have a true pixel-wise correspondence between images. For example, the ColorChecker image for the color-sample should be exactly the same as the ColorChecker image for the reference color-sample (the images should be perfectly aligned, have the same dimensions, the same angle of rotation, and be at the same scale). Of course, the actual colors in the ColorCheckers should be different. In such cases, it is also useful to make a correspondence between the colors in a ColorChecker image (the color-sample) and M_TRIPLET color values (the reference color-sample). Global mean variance For M_GLOBAL_MEAN_VARIANCE, MIL creates a color-sample's mapping based on transforming its mean and variance (standard deviation) to resemble the mean and variance of the reference color-sample. Such transformations can correct shifts in white-balance. They can also correct general color distortion that cameras, illuminants, and contrast dynamics in scenes can cause. The following is an example of the type of color issues with which you can use M_GLOBAL_MEAN_VARIANCE. These images show an electronic board with 3 different color casting effects caused by 3 different illuminants and cameras. By using any one of the images as the reference color, the M_GLOBAL_MEAN_VARIANCE operation can correct the other two colors accordingly. When using M_GLOBAL_MEAN_VARIANCE, you need not have color data or content that is aligned or similar. The following is an example of how M_GLOBAL_MEAN_VARIANCE can use the global color distribution of the mapping, established with a board image, to transform a color image that is completely different. You should typically use M_GLOBAL_MEAN_VARIANCE when you require a global impression of color, rather than the dynamics of color variation. Color calibration intent You can specify the following color calibration intent (quality) settings, using McolSetMethod(): M_BALANCE, M_GENERALIZATION, or M_PRECISION. These settings can increase or decrease the accuracy inherent in the operation by limiting or expanding the amount of color information the operation internally processes. For M_COLOR_TO_COLOR and M_HISTOGRAM_BASED, the default is M_BALANCE. For M_GLOBAL_MEAN_VARIANCE, the default is M_GENERALIZATION (no other color calibration intent is available). With M_BALANCE, the operation considers a moderate amount of color information to perform its calculations. This is a standard setting that represents a compromise between accuracy and robustness. There is a reduced risk of overfitting the color data, compared with M_PRECISION. With M_GENERALIZATION, the operation considers a minimal amount of color information to perform its calculations. This setting represents the lowest possible accuracy, and the lowest risk of overfitting the color data. It also represents the fastest setting, which can be useful for very large images. With M_PRECISION, the operation considers a high amount of color information to perform its calculations. This setting represents the highest possible accuracy, and the highest risk of overfitting the color data. For example, if you perform the relative color calibration on grabbed images containing outlier colors that you did not consider during preprocessing, incorrect color effects can result. Computation option By default, MIL uses all pixel values between the reference color-sample and the color-samples to generate the color information that the operation requires. For an M_COLOR_TO_COLOR operation, MIL can use color statistics to generate the color information. To specify how to generate the color information, call McolSetMethod() and set the computation option to M_COMPUTE_ITEM_PIXELS (the default) or M_COMPUTE_ITEM_STAT (for color-to-color only). Based on the particulars of your application, you must evaluate if color statistics are appropriate. Insufficient color information can lead to misleading transformations (such as overfitting). Selecting strategy There are numerous options with which to base your strategy (operation and quality settings). The following is an overview of what to consider: Precision. This refers to the accuracy requirements of your application. M_COLOR_TO_COLOR (with M_PRECISION) is the most precise, while M_GLOBAL_MEAN_VARIANCE is the least. M_HISTOGRAM_BASED attempts to reconcile both. The precision that M_HISTOGRAM_BASED obtains is often comparable to M_COLOR_TO_COLOR. Speed. This refers to the time requirements of your application. M_GLOBAL_MEAN_VARIANCE (with M_GENERALIZATION) offers the fastest processing, while M_HISTOGRAM_BASED takes the longest to process. Strategy typically affects the speed of preprocessing (McolPreprocess()) more than the relative color calibration itself (McolTransform()). For example, while preprocessing with M_HISTOGRAM_BASED is longer than M_COLOR_TO_COLOR, the speed of the transformation is the same for both. Flexibility. This refers to the adaptability requirements of your application. M_GLOBAL_MEAN_VARIANCE (with M_GENERALIZATION) is the most flexible. It usually proves useful when other operations fail, since its generality makes it less affected by the misalignment of color data or a strong dissimilarity between color data. M_GLOBAL_MEAN_VARIANCE can also prove less prone to error if you must transform unknown images that contain many outlier colors. M_COLOR_TO_COLOR is the least flexible. Since this is a point-to-point operation, minor misalignments can heavily influence transformations. M_HISTOGRAM_BASED offers a good compromise in flexibility. Although color-data should be basically similar, you need not have a point-to-point correspondence. Set up. This refers to the preprocessing requirements of your application. M_GLOBAL_MEAN_VARIANCE (with M_GENERALIZATION) is the simplest to set up. Since this is a very general type of operation, the color-samples you provide need not be especially accurate nor complete. M_COLOR_TO_COLOR is the most difficult to set up. Since this is a point-to-point operation, it can fail if you are using misaligned images. You can also provide a ColorChecker image, but you must externally define it. ColorCheckers are industry standards, and not native to MIL. As in most cases, M_HISTOGRAM_BASED offers a middle ground, allowing you to set up using real-world images. Relative color calibration Steps to performing relative color calibration Color-samples and color elements Reference color-sample Operation mode Histogram-based Color-to-color Global mean variance Color calibration intent Computation option Selecting strategy ",
      "wordCount": 2792,
      "subEntries": []
    },
    {
      "id": "UG_color_Converting_to_grayscale",
      "version": null,
      "title": "Converting to grayscale",
      "subTitles": [
        "Extracting the luminance",
        "Principal component projection",
        "Using a principal component projection for converting to grayscale"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\color\\Converting_to_grayscale.htm",
      "text": " Converting to grayscale Converting color images to grayscale can be useful since many processing and analysis modules operate on grayscale images. For example, a picture of a license plate can be taken in color, but before it can be used in an Automatic Number Plate Recognition (ANPR) application written with the MIL String Reader module, it must be converted to grayscale. To convert color images to grayscale, you can either use MimConvert() to extract the luminance, or McolProject() to perform a principal component projection. To enhance image quality and improve subsequent operations, you can also process your images by, for example, performing binary thresholding. Various conditions, such as different cameras and illuminants, can cause color from identical images to appear dissimilar. If this occurs, you can call McolTransform() to perform a relative color calibration before converting color images to grayscale. Relative color calibration ensures all colors are consistent according to a specified reference color. For more information, see the Relative color calibration section earlier in this chapter. MimConvert() is available with MIL-Lite. McolProject() and McolTransform() require the full version of MIL. Extracting the luminance You can use MimConvert() to convert color images to grayscale by extracting the luminance (intensity) from an RGB image (M_RGB_TO_L), or copying the luminance component of an image into a 3-band RGB buffer (M_L_TO_RGB), to create a monochromatic (gray) RGB buffer. For the YUV color space, you can use M_RGB_TO_Y to extract the Y-component from an RGB image. You can also use MimConvert() to convert from RGB to HSL, or vice versa (M_RGB_TO_HSL and M_HSL_TO_RGB, respectively). This can be useful if you want to perform color analysis in a color space other than the one in which your color data is stored. To convert between RGB and HSL color spaces, MimConvert() uses the following algorithm: #define min(a, b) (((a) &lt; (b)) ? (a) : (b)) #define max(a, b) (((a) &gt; (b)) ? (a) : (b)) // Input and output between 0 and 1. void RGBToHSL(float R, float G, float B, float* H, float* S, float* L) { float MinVal, MaxVal, Delta; // Min and max values. MaxVal = max(R, max(G, B)); MinVal = min(R, min(G, B)); Delta = MaxVal - MinVal; // Compute the luminance. *L = 0.5f * (MaxVal + MinVal); if (Delta == 0.0f) { *S = *H = 0.0f; } else { // Compute the saturation. if (*L &lt;= 0.5f) { *S = Delta / (MaxVal + MinVal); } else { *S = Delta / (2.0f - MaxVal - MinVal); } // Compute the hue. if (MaxVal == R) { *H = 60.0f * ((G - B) / Delta); } else if (MaxVal == G) { *H = 60.0f * (2.0f + ((B - R) / Delta)); } else { *H = 60.0f * (4.0f + ((R - G) / Delta)); } if (*H &lt; 0.0f) { *H += 360.0f; } } // Remap the angle between 0 and 1. *H = *H / 360.0f; } float HueToRGB(float Temp1, float Temp2, float Hue) { if (Hue &gt; 360.0f) { Hue = Hue - 360.0f; } else if (Hue &lt; 0.0f) { Hue = Hue + 360.0f; } if (Hue &lt; 60.0f) { return (Temp1 + (Temp2 - Temp1) * Hue / 60.0f); } else if (Hue &lt; 180.0f) { return Temp2; } else if (Hue &lt; 240.0f) { return (Temp1 + (Temp2 - Temp1) * (240.0f - Hue) / 60.0f); } return Temp1; } // Input and output between 0 and 1. void HSLToRGB(float H, float S, float L, float* R, float* G, float* B) { float Temp1, Temp2; // Remap the angle between 0 and 360. H = H * 360.0f; // Achromatic case. if (S == 0.0f) { *R = *G = *B = L; } else { if (L &lt;= 0.5f) { Temp2 = L * (1.0f + S); } else { Temp2 = L + S - (L * S); } Temp1 = 2.0f * L - Temp2; *R = HueToRGB(Temp1, Temp2, H + 120.0f); *G = HueToRGB(Temp1, Temp2, H); *B = HueToRGB(Temp1, Temp2, H - 120.0f); } } For more technical information, refer to Computer Graphics: Principles and Practice in C, James D. Foley et al., Addison-Wesley Publishing Company, United States, 1995. Principal component projection You can convert a color image to grayscale using McolProject() with M_PRINCIPAL_COMPONENT_PROJECTION which, unlike MimConvert(), does not simply extract a single component of the color space. Instead, it uses the distribution of the image's color data to calculate the best grayscale conversion possible, minimizing the loss of information. This results in a grayscale image that can better differentiate the color in the original source image. As the example above illustrates, extracting just the luminance, as is done with MimConvert(), can cause different colors to look the same in grayscale, which can make analysis problematic in certain cases. However, with M_PRINCIPAL_COMPONENT_PROJECTION, MIL performs a principal component analysis (PCA) to calculate the color image's strongest vector, referred to as the first principal component, which represents the greatest degree of color variation within the color space. Each color pixel is then projected to a point on this vector, resulting in a grayscale image that conveys more information than a luminance extraction. If the first principal component vector is the same as the luminance vector (black-to-white), the principal component projection will be very similar to extracting the luminance band. This can happen if, for example, the greatest variation in color in the image is roughly from black to white. In this case, it can still be advantageous to use the first principal component, since this vector was calculated from the data distribution itself and is optimally oriented (black-to-white/white-to-black) for the projection calculation. Using a principal component projection for converting to grayscale For converting to grayscale, McolProject() calculates the first principal component vector using the entire source image or specific areas of the source image. Typically using the entire source image is sufficient. To identify specific areas in the source image with which to calculate the first principal component, you must specify another image, referred to as a data identification image. In this image, you must set the corresponding pixels to M_SOURCE_LABEL. By specifying the exact colors (pixels) in the source image to use, you can increase the difference between grayscale values in some cases. When using a principal component projection to convert to grayscale, colors are automatically projected between the brightest (white) and darkest (black) side of the grayscale palette. If this is performed properly, the resulting status of McolProject() is M_SUCCESS. If MIL cannot confidently determine which pixels to project to the bright side of the grayscale palette and which to project to the dark side, the direction of the projection is chosen arbitrarily and the resulting status is M_UNSTABLE_POLARITY. When this occurs, you can use the data identification image to identify the corresponding pixels in the source that you want to project to the bright side of the grayscale palette and which to project to the dark side, using McolProject() with M_BRIGHT_LABEL and M_DARK_LABEL. By default, MIL projects the result according to the dynamic range (minimum and maximum pixel values) established from all the pixels in the source image, even if you have set specific source pixels to M_SOURCE_LABEL. If necessary, you can use M_MASK_CONTRAST_ENHANCEMENT to improve the conversion to grayscale by performing the projection according to the dynamic range established from only the specified source image pixels. The result is similar to increasing the contrast. When using M_MASK_CONTRAST_ENHANCEMENT, MIL saturates the projected unidentified source image colors that are outside the dynamic range of the identified source image pixels' color (M_SOURCE_LABEL); that is, MIL maps those colors to either the minimum or maximum allowable grayscale value. In this case, the projection result can contain sections that have lost a certain degree of contrast information. This example illustrates how, given the dynamic range MIL establishes from the identified source image pixels, MIL projects to 0 any unidentified pixel value below 100, and projects to 255 any value above 200. Dynamic range of the identified source image pixels: 100 to 130 Projected unidentified pixel values Resulting grayscale pixel values 5 100 10 100 20 100 40 100 195 200 200 200 220 200 240 200 When using McolProject(), you can also specify a destination mask. The mask determines where to write the result of the projection operation and does not affect calculations. Results are only written to unmasked destination pixels. Do not confuse this mask with the data identification image containing M_SOURCE_LABEL, which is used to identify the source image pixels to use for the calculation. McolProject() can either produce the resulting grayscale image, or the transformation matrix to convert the image to grayscale. You can use this matrix with MimConvert() to perform an optimal color-to-grayscale transformation of any 3-band color image. Only use this matrix to convert images that have a similar color distribution, otherwise unpredictable results can occur. Converting to grayscale Extracting the luminance Principal component projection Using a principal component projection for converting to grayscale ",
      "wordCount": 1503,
      "subEntries": []
    },
    {
      "id": "UG_color_Distance_between_colors",
      "version": null,
      "title": "Distance between colors",
      "subTitles": [
        "Differences in distances",
        "Color distance types",
        "Euclidean distance",
        "Manhattan distance",
        "Mahalanobis distance",
        "Delta-E distance",
        "Advanced CIE distance types",
        "Choosing a distance type"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\color\\Distance_between_colors.htm",
      "text": " Distance between colors It might be useful to calculate the distance between colors, for example, to find flaws in an image by comparing it to a perfect version of the image (golden template), or to a color constant. Distances between colors can either be calculated using McolDistance(), or when matching colors with McolMatch(). If required, you can normalize distance results. For more information, see the Distance normalization settings subsection of the Advanced color matching settings and concepts section later in this chapter. Using McolDistance(), you can calculate the point-to-point distance between colors in two sources. The first source must be an image, while the second source can be: an image, a color constant, a covariance matrix, or the covariance of a specified image. Results are written to the destination image. To ignore unwanted pixels in the distance calculation, you can use McolDistance() with a mask image, within which you must identify the masked (non-zero) pixels. Set unmasked pixels to 0 in the mask. The color distance is calculated only for the masked (non-zero) pixels within the intersection of the source, destination, and mask images, with the assumption of a common origin at the top-left corner. Color distances are also calculated when determining which color-sample best matches an image area. Therefore, you can retrieve distance results, after calling McolMatch(). The following image shows how distances between the target areas and multiple color-samples can be drawn. The distances drawn correspond to the distances between the target areas and their best-matched color-samples. Note that this is unlike McolDistance(), which can only calculate the distance between 2 sources. For more information on retrieving color distances when matching colors, see the Image results subsection of the Color matching section later in this chapter. When calculating the color distance, make sure the color data that you provide is compatible. For example, you will not receive an error if you try to calculate the distance between an RGB and an HSL image, or a 16-bit and an 8-bit image. If the color data is not compatible, it is still processed as is (no error), which can produce misleading results. Note that when calculating color distance using McolMatch(), you must also consider the context's source color space. For more information, see the Source color space subsection of the Color spaces and converting between them section earlier in this chapter. Various conditions, such as different cameras and illuminants, can cause color from identical images to appear dissimilar. If this occurs, you can call McolTransform() to perform a relative color calibration before calculating the color distance. Relative color calibration ensures all colors are consistent according to a specified reference color. For more information, see the Relative color calibration section earlier in this chapter. Differences in distances The primary differences between distances calculated with McolMatch() and McolDistance() are: When the sources are images, McolDistance() calculates a point-to-point distance, while McolMatch() uses color statistics (average color). McolMatch() calculates the distance between each color-sample's statistic and each target area's statistic, or between each color-sample's statistic and each pixel in each target area, depending on the operation mode specified with McolSetMethod(). McolDistance() results are from two specified sources. However, McolMatch() results can come from several color-samples, or even from background and outlying regions. If you are only interested in distance values, McolDistance() can be more convenient to use than McolMatch(), since McolDistance() requires less setup, does not perform a matching operation, and results are returned directly to the function. Unlike McolDistance(), McolMatch() takes the specified context's color space into account; it also offers more options than McolDistance(), such as converting to the CIELAB color space and operating on specific color bands. Color distance types Whether you are matching colors (McolMatch()) or using McolDistance(), the color distance can be calculated using one of the following distance types (unless otherwise specified): Euclidean (M_EUCLIDEAN). Mahalanobis (M_MAHALANOBIS). Manhattan (M_MANHATTAN). Delta-E (M_DELTA_E). Advanced distance types, as established by the standards of the International Commission on Illumination (CIE). These distance types are only available for color matching (McolMatch()). When calculating the distance between colors, set the distance type with McolDistance(). When matching colors with McolMatch(), set the distance type with McolSetMethod(). Euclidean distance A Euclidean distance is the square root of the sum of the squared differences between the color of the first source and the color of the second source. A Euclidean distance is generally regarded as a well-known standard distance calculation. The following example illustrates how the distance between a green point, indicated by a circle, and two other green points, indicated by a triangle and a square, is measured with a Euclidean calculation. A Euclidean distance can be represented with the following formula, where: r1 and r2 represent the first color component of the first and second source color. g1 and g2 represent the second color component of the first and second source color. b1 and b2 represent the third color component of the first and second source color. Manhattan distance A Manhattan distance (also known as a City Block distance) is the sum of the absolute value of the differences between the color of the first source and the color of the second source. A Manhattan distance is generally considered the simplest distance calculation and is typically appropriate for calculating color distances between hue (H) bands in HSL. For example, using McolMatch() with an HSL color space, and calculating the distance between the zero (H) bands (McolControl() with M_BAND_MODE set to M_COLOR_BAND_0). The following example illustrates how the distance between a green point, indicated by a circle, and two other green points, indicated by a triangle and a square, is measured with a Manhattan calculation. A Manhattan distance can be represented with the following formula, where: r1 and r2 represent the first color component of the first and second source color. g1 and g2 represent the second color component of the first and second source color. b1 and b2 represent the third color component of the first and second source color. Note that in HSL the color's hue is stored as a separate component (H) represented as an angular position on a circular color disk. Therefore the distance between colors is equal to the smallest angular difference, rather than the absolute value of the difference. Mahalanobis distance A Mahalanobis distance is calculated between the color of the first source and the covariance of the second source. If you are using McolDistance() and the second source is an image, the covariance of that image, rather than the color of each pixel, is used to calculate the distance. If you are using McolSetMethod(), the covariance of the color-sample is used. A Mahalanobis distance is generally regarded as a slower, though more robust distance calculation for elongated color-samples. The following example illustrates how the distance between a green point, indicated by a circle, and two other green points, indicated by a triangle and a square, is measured with a Mahalanobis calculation. A Mahalanobis distance can be represented with the following formula, where: x represents the first source color. u represents the average of the second source color. sigma is for the covariance matrix of the second source color. The distance calculated for Mahalanobis, between a color and a distribution of colors (covariance), is similar to a Euclidean distance between the mean of the two colors, but weighted by the inverse of the covariance of the distribution. This implies that the more a color distribution varies in a direction within the color space, the less significant the distance is in that direction. Since the covariance matrix of the second source is used, the second source should typically be a distribution of colors, such as an image, and not a single solid color (a color constant). However, if you provide a color constant as the second source, Mahalanobis behaves very much like Euclidean and will yield similar results. Delta-E distance A Delta-E color distance is similar to a Euclidean color distance, but has been generally adjusted for the LAB (CIELAB) color space. This function assumes that the data is in the LAB color space; it does not convert the data before computing the distance. To convert the data, use MimConvert() with M_SRGB_LINEAR_TO_LAB or M_SRGB_TO_LAB. Advanced CIE distance types In addition to M_DELTA_E, MIL offers more specialized types of CIE color distances, which you can specify using McolSetMethod(). These types are recommended for advanced users dealing with minor color variances in industrial color difference evaluation. CMC (M_CMC_ACCEPTABILITY and M_CMC_PERCEPTIBILITY). These distance types are generally intended for the textile industry and allow for lightness and chroma factors based on either acceptability or perceptibility requirements. CIE94 (M_CIE94_GRAPHIC_ARTS and M_CIE94_TEXTILE). These distance types are similar to CMC but allow for weighting factors based on color tolerances for either the graphic arts industry or the textile industry. CIEDE2000 (M_CIEDE2000). This distance type is similar to CIE94, but is generally more robust regarding the effect of lightness on color. If M_DELTA_E is proving ineffective, you might want to try M_CIEDE2000 as a first alternative. These distance types follow the standards of the CIE, as specified in their technical report on Colorimetry (CIE 15:2004). Refer to this document for more information. Choosing a distance type Choosing the most appropriate distance type with which to calculate color distances depends on many factors, including the color space of your data, the background, and the particularities of your application. Typically, a Euclidean distance should be used for RGB and CIELAB color spaces, while a Manhattan distance should be used for HSL. A Mahalanobis color distance should be used when dealing with closely-related colors that are not expressed in HSL. The following example illustrates an RGB source image of a grapefruit, which is to be used in a matching operation. Although for RGB colors a Euclidean distance is typically sufficient, in this case a Mahalanobis distance is preferable. In the source image, the color of the background and some parts of the grapefruit are similar; this makes Mahalanobis yield better results, since the covariance of the image is used. The pixels of the grapefruit correspond roughly to a distribution of shades of yellow, therefore, with a Mahalanobis distance, shades of yellow are considered to be closer to the grapefruit than other colors. To illustrate this point, the following image (a plotting of the color histogram) shows two separate groups of pixels displayed in RGB; one group is from the image's background, and the other is from the grapefruit. For each group of pixels (background and grapefruit), this image shows: The first principal component, indicated by the red lines. Each first principal component represents the direction of greatest standard deviation for its group of pixels. The mean color, indicated by the intersection of the blue line with the first principal component. Encircled in black, on the left, are the background pixels that will match the grapefruit, being closer by Euclidean distance to the grapefruit's mean color. Encircled in black, on the right, are the grapefruit's pixels that will match the background, being closer by Euclidean distance to the background's mean color. However, with a Mahalanobis distance, any distance oriented parallel to the principal component (the red lines) will be scaled by the inverse of the standard deviation. Therefore, the encircled pixels will match with the correct group (background or grapefruit), yielding a better matching result. Distance between colors Differences in distances Color distance types Euclidean distance Manhattan distance Mahalanobis distance Delta-E distance Advanced CIE distance types Choosing a distance type ",
      "wordCount": 1909,
      "subEntries": []
    },
    {
      "id": "UG_color_Color_matching",
      "version": null,
      "title": "Color matching",
      "subTitles": [
        "Steps to performing color matching",
        "Basics of color matching",
        "Color identification",
        "Supervised color segmentation",
        "Defining and adding color-samples and color elements",
        "Area identifier image",
        "Acceptance",
        " Color-sample acceptance (for the color-sample's match score)",
        "Relevance acceptance (for the target area's relevance score)",
        "Distance tolerance",
        "Tolerance mode",
        "Operation mode and distance type",
        "M_STAT_MIN_DIST operation mode ",
        "M_MIN_DIST_VOTE operation mode ",
        "M_HISTOGRAM_MATCHING and M_HISTOGRAM_VOTE operation modes",
        "Basic results",
        "Best-matched color-sample",
        "Match status",
        "Match score and relevance score",
        "Outlier coverage and color-sample coverage",
        "Color distance",
        "Image results",
        "Target areas",
        "Color-samples and color elements",
        "Background and outliers",
        "Inverting colors"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\color\\Color_matching.htm",
      "text": " Color matching Color matching is the process of finding a match between the color of a target area and one or more predefined color-samples. Color-samples are composed of color elements. These color elements can be added to or removed from the color-sample to control the color-sample's color properties during the matching process. When you first define a color-sample using McolDefine(), you are also automatically adding the first color element to the sample. The match is performed, in part, by calculating the color distance between the target area and the color-sample. This information can be retrieved after calling McolMatch(). However, if you are only interested in color distances, you can use McolDistance(). For more information, including the differences in distances between McolMatch() and McolDistance(), see the Distance between colors section earlier in this chapter. Various conditions, such as different cameras and illuminants, can cause color from identical images to appear dissimilar. If this occurs, you can call McolTransform() to perform a relative color calibration before matching colors. Relative color calibration ensures all colors are consistent according to a specified reference color. For more information, see the Relative color calibration section earlier in this chapter. Steps to performing color matching The following steps provide a basic methodology for using the MIL Color Analysis module to match colors: Allocate a color matching context to hold your color-samples and color matching settings, using McolAlloc() with M_COLOR_MATCHING. Specify the source color space, using McolAlloc() with M_RGB, M_HSL, or M_CIELAB. For more information, see the Source color space subsection of the Color spaces and converting between them section earlier in this chapter. Allocate a color matching result buffer to hold the color matching results, using McolAllocResult() with M_COLOR_MATCHING_RESULT. This step is not required if you are calculating an image result directly, using McolMatch() with M_DRAW_.... Optionally, convert your colors to the appropriate color space, using MimConvert(). For example, you can convert RGB color-samples to HSL. When using an RGB source color space, you can also use McolSetMethod() to convert your RGB data to CIELAB or HSL before the match. For more information, see the Converting between color spaces subsection of the Color spaces and converting between them section earlier in this chapter. Define the color-samples and the first color element of the sample using McolDefine(). Optionally, add or remove color elements from the sample using McolDefine() with M_ADD_COLOR_TO_SAMPLE or M_DELETE. Optionally, change the operation mode and distance type used to perform the match operation, using McolSetMethod(). Optionally, control the settings of a color matching context or the color-samples contained therein, using McolControl(). When controlling the settings of color-samples, you can specify a specific color-sample or M_ALL, which applies the control type setting to all the color-samples contained within the color matching context (when supported). Preprocess the context, using McolPreprocess(). Perform the color matching operation, using McolMatch(). Retrieve the required results from the result buffer, using McolGetResult(). Draw image results, using McolDraw(). Free all your allocated objects, using McolFree(), unless M_UNIQUE_ID was specified during allocation. Basics of color matching Color matching can be used to perform one of two basic tasks: color identification or supervised color segmentation. In either case, the match is based on the defined color-samples, whose color elements can come from a source image or from explicit color values. For more information, see the Defining and adding color-samples and color elements subsection of this section. Color identification Color identification refers to matching the general color of each target area with the best color-sample within a group of predefined color-samples. When performing color identification, you might find it useful to locate the target areas with the MIL Geometric Model Finder module before performing the match. To produce image results for a target area, use McolMatch() or McolDraw() with M_DRAW_AREA_MATCH_USING_LABEL or M_DRAW_AREA_MATCH_USING_COLOR. Supervised color segmentation Supervised color segmentation refers to matching the color value of each pixel in the target image (or target area) with the best predefined (supervising) color-sample. For supervised color segmentation, you must use McolSetMethod() with M_MIN_DIST_VOTE, which matches colors on a pixel-by-pixel basis, and produce image results using M_DRAW_PIXEL_MATCH_USING_LABEL or M_DRAW_PIXEL_MATCH_USING_COLOR. By matching the color value of each pixel with the best predefined color-sample, you can use the Color Analysis module to separate objects by their color information. This type of matching is useful to, for example, retrieve the relative presence of each color in an image (this is also referred to as the spatial coverage for each of the colors). Defining and adding color-samples and color elements As previously discussed, color-samples are composed of color elements. When you first define a color-sample, it is automatically considered to have a single color element. Therefore, when you add a color-sample to the color matching context, you are by default adding the first color element to the color-sample. You can define a color-sample (and consequently the first color-sample element) from either a region of an image (M_IMAGE), or from three explicit color component values (M_TRIPLET), using McolDefine(). For image type color-samples, MIL performs an estimation of the color based on color statistics, such as the mean. MIL considers this the color of the color-sample and uses it whenever required (for example, McolTransform()). The size of the color-sample does not typically affect speed or effectiveness. To add color elements to color-samples that already exist in the context, use M_ADD_COLOR_TO_SAMPLE. The color estimation that MIL performs for the color-sample uses the new color element(s) that you specify for the color-sample, as well as the existing element(s) of the color-sample. MIL now considers the resulting estimate the color of the color-sample. You can also use McolControl() to control settings related to color-samples. When you add, modify, or delete a color-sample, you must preprocess the color matching context (McolPreprocess()) before any subsequent call to McolMatch(). For more information about color-samples, see the Color-samples and color elements subsection of the Relative color calibration section earlier in this chapter. Defining color-samples for color matching is nearly identical to defining color-samples for relative color calibration. Area identifier image The area identifier image is a grayscale image, specified with McolMatch(), that defines the independent target areas where color matching will occur in the target image. You must use an area identifier image to match multiple target areas. Each unique, non-zero label in the area identifier image defines an independent target area. Typically, the area identifier image is the same size as the target image. In the following example, the 4 target areas in the area identifier image are defined according to the objects found with the MIL Model Finder module. The color of each corresponding target area, in the actual target image, is then matched (identified) with the best color-sample. To produce image results on an area basis (as illustrated above), use McolMatch() or McolDraw() with M_DRAW_AREA_MATCH_USING_LABEL or M_DRAW_AREA_MATCH_USING_COLOR. For more information, see the Image results subsection of this section. Acceptance The acceptance levels determine the minimum scores required for a successful match between a target area and a color-sample. You can set an acceptance level for the color-sample's match score, and for the target area's relevance score, using McolControl() with M_ACCEPTANCE and M_ACCEPTANCE_RELEVANCE. Color-sample acceptance (for the color-sample's match score) M_ACCEPTANCE is applied to the match score (McolGetResult() with M_SCORE), which indicates the similarity between the color of the color-sample and the color of the target area. The higher the acceptance, the closer the colors must be for them to match. For example, if you are using an M_STAT_MIN_DIST operation mode (McolSetMethod()), and you set M_ACCEPTANCE to 100, the colors will only match if they are absolutely the same. For an acceptance of 100 while using an M_MIN_DIST_VOTE operation mode, all pixels in the target area that are not outliers must have voted for the same color-sample to have a successful match. For more information on the actual match score calculated, see the Match score and relevance score subsection of this section. Relevance acceptance (for the target area's relevance score) M_ACCEPTANCE_RELEVANCE is applied to the target area's relevance score (McolGetResult() with M_SCORE_RELEVANCE), which indicates the significance (relevance) of the match score (M_SCORE). In statistics, this is similar to the confidence level. A high relevance score indicates that the best-matched color-sample was a vastly superior match compared to the other color-samples, while a low relevance score indicates that another color-sample came very close to being the best-matched color-sample. If you are using the M_STAT_MIN_DIST operation mode (McolSetMethod()), and you set M_ACCEPTANCE_RELEVANCE to 100, the match will only be successful if the best-matched color-sample was the only color-sample considered (this can occur if, for example, all the other color-samples fell outside of the distance tolerance (M_DISTANCE_TOLERANCE)). For a relevance acceptance of 100 while using an M_MIN_DIST_VOTE operation mode, all pixels in the target area must have voted for the same color-sample. For more information on the actual relevance score calculated, see the Match score and relevance score subsection of this section. Distance tolerance The distance tolerance refers to the maximum color distance, between the color-sample and the target area, allowed for a successful match. To specify the distance tolerance, use McolControl() with M_DISTANCE_TOLERANCE set to M_INFINITE, a specific value, or M_AUTO (the default). The greater the distance tolerance, the greater the distance (difference) between matching colors can be. For example, if your target area is green, you can set the distance tolerance to 0 to only match with the exact same green. However, by increasing the tolerance, you can match with colors that are progressively different than the original. When setting the tolerance, you must consider the specified distance type, the color space, and the color space encoding. For example, a distance tolerance of 1.0 when using an M_MAHALANOBIS distance type is not the same as when using an M_MANHATTAN distance type. Tolerance mode When setting M_DISTANCE_TOLERANCE to a specific value or M_AUTO, MIL determines the distance tolerance according to the tolerance mode, which you can set with M_DISTANCE_TOLERANCE_MODE. If you set the tolerance mode to M_ABSOLUTE (the default), MIL applies the M_DISTANCE_TOLERANCE value as is. For M_AUTO, the M_DISTANCE_TOLERANCE value is infinite. If you set the tolerance mode to M_RELATIVE, MIL internally calculates the distance between each pair of color-samples and finds the smallest distance value; half this value is then multiplied by the M_DISTANCE_TOLERANCE value. For example, if you have three different color-samples (sample 1, sample 2, and sample 3), the distances between each pair would be the distance between sample 1 and sample 2 (distance 12), sample 1 and sample 3 (distance 13), and sample 2 and sample 3 (distance 23). The tolerance value for sample 1 would then be the smaller value of distance 12 and distance 13, multiplied by M_DISTANCE_TOLERANCE. The tolerances for each of the other samples are calculated the same way. For M_AUTO, the M_DISTANCE_TOLERANCE value is 1. If you set the tolerance mode to M_SAMPLE_STDDEV, MIL specifies a tolerance strategy based on the color-sample's standard deviation. For color-samples defined from images (McolDefine() with M_IMAGE), the color-sample's standard deviation is computed as the distance between each pixel in the color-sample and the average color of the color-sample. MIL calculates this distance according to the current distance type set with McolSetMethod(). The resulting distance, which in this case is considered to be the color-sample's standard deviation, is multiplied by M_DISTANCE_TOLERANCE and is used as the acceptable distance tolerance. If the computed color-sample's standard deviation is less than 1.0, MIL uses 1.0 as the standard deviation. For color-samples composed of triplets (McolDefine() with M_TRIPLET), MIL uses 1.0 as the standard deviation. In this case, M_DISTANCE_TOLERANCE remains unchanged and is used as the acceptable distance tolerance. For M_AUTO, the M_DISTANCE_TOLERANCE value for M_SAMPLE_STDDEV is 3. Operation mode and distance type Before calling McolMatch(), you can use McolSetMethod() to set the match's operation mode to one of the following: M_STAT_MIN_DIST (default), which uses the average color of the color-sample and target area for the match. M_STAT_MIN_DIST is the fastest operation mode and is typically sufficient for simple applications. M_MIN_DIST_VOTE, which uses the average color of the color-sample and the color of each pixel in the target area for the match. Although typically slower than M_STAT_MIN_DIST, M_MIN_DIST_VOTE is a good option when dealing with outliers or images containing unneeded pixels/colors. As previously discussed, you must use M_MIN_DIST_VOTE to perform supervised color segmentation. M_HISTOGRAM_MATCHING, which uses the color histogram of the color-sample and the target area for the match. You should generally use M_HISTOGRAM_MATCHING when trying to match a mixture of colors. M_HISTOGRAM_VOTE, which uses the color histogram of the color-sample and the color of each pixel in the target area for the match. Each target pixel votes for the best color-sample and the color-sample with the most votes is considered the best match. Color matching operation modes only take color information into account; target areas are therefore only distinguishable if their color composition differs. Further details for each operation are described later in this section. You can also use McolSetMethod() to set the type of distance with which to perform the match. For RGB and CIELAB color spaces, a Euclidean color distance is calculated by default. For HSL color spaces, Manhattan is the default. For more information, see the Color distance types subsection of the Distance between colors section earlier in this chapter. M_STAT_MIN_DIST operation mode When using the M_STAT_MIN_DIST operation mode, color statistics are calculated (typically the mean/average) for each target area and for each color-sample defined in the context, and the color distance between each target area and each color-sample is determined. The resulting distances determine the score of the color-samples. The closer the colors, the higher the score. If a distance is not within a color-sample's distance tolerance, the color-sample's score is 0%. The color-sample with the highest score above the acceptance level is the target area's best-matched color-sample. In the following example, the target area's average color is calculated, and then matched with color-sample 1, which is the closest (best-match) color. For M_STAT_MIN_DIST, you should accurately define the target area, and it should generally consist of the required color, as indicated in the example above. M_MIN_DIST_VOTE operation mode When using the M_MIN_DIST_VOTE operation mode, color statistics are calculated (typically the mean/average) for each color-sample defined in the context, and the color distance between each pixel in each target area and each color-sample is determined. Each target pixel then votes for the color-sample with the closest color, and which is also within the distance tolerance. The number of votes that a color-sample accumulates determines its score. The greater the number of votes, the higher the score. The color-sample with the highest score above the acceptance level is the target area's best-matched color-sample. In the following example, each target pixel votes for the color-sample that has the closest color. In general, grapefruit pixels will vote for color-sample 1, while background pixels will vote for sample 2. Since there are more grapefruit pixels, color-sample 1 is the best-match. Since M_MIN_DIST_VOTE operates on a pixel-by-pixel basis, it provides more detailed results (for each pixel) than M_STAT_MIN_DIST and is generally more robust (but slower). M_MIN_DIST_VOTE is also less sensitive towards the accuracy of the target areas; as indicated in the example above, the target need not consist of only the grapefruit. M_HISTOGRAM_MATCHING and M_HISTOGRAM_VOTE operation modes MIL offers two histogram operation modes: M_HISTOGRAM_MATCHING and M_HISTOGRAM_VOTE. When using the M_HISTOGRAM_MATCHING operation mode, color histograms are calculated for each target area and for each sample defined in the context. A match score is then calculated between each target area histogram and the sample histograms. The sample with the highest match score above the acceptance is the target area's best-matched sample. When using M_HISTOGRAM_MATCHING, you must set M_DISTANCE_TOLERANCE_MODE to M_ABSOLUTE. In M_HISTOGRAM_MATCHING mode, pixel values are grouped together by subdividing their color space into bins of equal size. You must specify the number of bins for each color band using McolControl() with M_NB_BINS_BAND_.... The match score is based on the comparison of histogram frequencies for each bin, rather than each individual pixel value. Note that the histograms shown above are approximations intended for explanatory purposes. M_HISTOGRAM_MATCHING is generally the most robust operation mode when dealing with color-samples that contain a mixture of colors. For more detailed information on histogram matching, see the Histogram matching concepts subsection of the Advanced color matching settings and concepts section later in this chapter. The M_HISTOGRAM_VOTE operation mode uses a pixel voting process to find a target area's best-matched color-sample. When using this operation mode, MIL computes color histograms for each sample defined in the context. Then, each pixel in the target area votes for the color-sample that has a non-empty histogram bin containing the pixel's color. The number of votes that a color-sample accumulates determines its score. The color-sample with the highest score above the acceptance (McolControl() with M_ACCEPTANCE) is the target area's best-matched color-sample. In the case of a tie, either during the vote or after all votes have been cast, the color-sample with the highest label value is chosen as the best-match. When using an M_HISTOGRAM_VOTE operation mode, MIL does not perform typical distance calculations. When passing M_HISTOGRAM_VOTE to McolSetMethod(), you must also set the distance type to M_NONE. Also, MIL ignores distance settings such as McolControl() with M_DISTANCE_TOLERANCE_MODE and M_DISTANCE_TOLERANCE. Like M_HISTOGRAM_MATCHING, you should generally use M_HISTOGRAM_VOTE when color-samples are a mixture of colors. Although typically slower than M_HISTOGRAM_MATCHING, M_HISTOGRAM_VOTE is a good option when dealing with outliers or images containing unneeded pixels/colors. Basic results You can use McolGetResult() with the identifier of a color matching result buffer to retrieve context, target area, and color-sample results, such as the index of the target area's best-matched color-sample (M_BEST_MATCH_INDEX). To retrieve such results, you must have first used McolMatch() to write all results to a result buffer, by setting its ControlFlag parameter to M_DEFAULT. If you used McolMatch() to write the results to an image buffer (M_DRAW_...) instead, you will not have a result buffer to specify in McolGetResult(). For more information on calculating image results, see the Image results subsection of this section. Depending on the type of result to retrieve, you must set the appropriate label or index value for the target area (AreaLabel) and color-sample (ColorSampleIndexOrLabel) parameters, and use the proper data type to hold the results. The following table lists various types of results and the settings you should specify to retrieve them. Type of result AreaLabel parameter ColorSampleIndexOrLabel parameter Data type for result A color-sample result for a specific color-sample. For example, you want to retrieve a color-sample's match score (M_SCORE). Note that to get any color-sample result, you must specify the target area(s). A specific target area. A specific color-sample. Data type: double. A color-sample result for a specific color-sample in all target areas. For example, you want to retrieve the color distance of a color-sample in each target area (M_COLOR_DISTANCE). All target areas. A specific color-sample. Data type: array of type double. Array size: M_NUMBER_OF_AREAS. A color-sample result for all color-samples in a specific target area. For example, you want to retrieve the color distance of all color-samples in a target area (M_COLOR_DISTANCE). A specific target area. All color-samples. Data type: array of type double. Array size: M_NUMBER_OF_SAMPLES. A color-sample result for all color-samples in all target areas. For example, you want to retrieve whether each color-sample in each target area fulfills the color matching conditions (M_SAMPLE_MATCH_STATUS). All target areas. All color-samples. Data type: array of type double. Array size: M_NUMBER_OF_AREAS x M_NUMBER_OF_SAMPLES. A general target area result for one target area. For example, you want to retrieve the index of a target area's best-matched color-sample (M_BEST_MATCH_INDEX). A specific target area. M_GENERAL. Data type: double. A general target area result for all target areas. For example, you want to retrieve the index of each target area's best-matched color-sample (M_BEST_MATCH_INDEX). All target areas. M_GENERAL. Data type: array of type double. Array size: M_NUMBER_OF_AREAS. A general result for a color matching context. For example, you want to retrieve the number of target areas or color-samples (M_NUMBER_OF_AREAS or M_NUMBER_OF_SAMPLES). M_GENERAL. M_GENERAL. Data type: double. Best-matched color-sample You can either return the index or the label of each target area's best-matched color-sample, using M_BEST_MATCH_INDEX or M_BEST_MATCH_LABEL. If no color-sample has matched, -1 is returned for M_BEST_MATCH_INDEX; for M_BEST_MATCH_LABEL, the value set using McolControl() with M_OUTLIER_LABEL is returned. Match status You can either return the match status of a color-sample, using M_SAMPLE_MATCH_STATUS, or the match status of a target area, using M_STATUS. The status of a color-sample (M_SAMPLE_MATCH_STATUS) returns M_MATCH if that color-sample fulfills the match conditions, with respect to the acceptance (M_ACCEPTANCE) and the distance tolerance (M_DISTANCE_TOLERANCE); otherwise, M_NO_MATCH is returned. The status of a target area (M_STATUS) returns M_SUCCESS if at least one color-sample fulfills the match conditions, and if the target area's relevance score passes the relevance acceptance level (M_ACCEPTANCE_RELEVANCE); otherwise, M_FAILURE is returned. The color-sample with the highest score (M_SCORE) is referred to as the best-matched color-sample. To determine if a specific color-sample is the best-matched color-sample, compare the color-sample's index or label with M_BEST_MATCH_INDEX or M_BEST_MATCH_LABEL. Match score and relevance score The color-sample's match score is based on the operation mode specified, using McolSetMethod(): M_STAT_MIN_DIST. Distance refers to the current color-sample distance and MaxDistance refers to the maximum distance possible in the source color space. M_MIN_DIST_VOTE. NumberOfVotes refers to the current color-sample's number of votes, and the sum is over the number of votes for all color-samples. Similarly, the target area's relevance score is also based on the operation mode specified, using McolSetMethod(): M_STAT_MIN_DIST. BestDistance is the distance of the best-matched color-sample, and the sum is over all color-samples that have been matched by the target area. M_MIN_DIST_VOTE. NumberOfVotes refers to the current color-sample's number of votes, and the sum is over the number of votes for all color-samples. A low relevance score indicates that you should be cautious about the match results, even if the match score is high. For example, a high match score and a low relevance score could mean that the target area matched very well with a color-sample, but came very close to matching with a different color-sample; this implies that a slightly different target image, or even a difference in lighting, could change your results. You should therefore set appropriate acceptance levels for each of these scores. For more information, see the Acceptance subsection of this section. The following example shows two cases illustrating the difference between match and relevance scores in M_MIN_DIST_VOTE mode: Outlier coverage and color-sample coverage The outlier coverage (M_OUTLIER_COVERAGE) quantifies, as a percentage, the proportion of pixels in the target area that did not vote for any color-sample, while the sample coverage (M_SAMPLE_COVERAGE) quantifies the proportion of pixels that did vote for a specific color-sample. Since coverage results are based on pixel votes, they are only available when using an M_MIN_DIST_VOTE operation mode. Color distance The color distance (M_COLOR_DISTANCE) returns the distance (difference) between the color of the target area and the specified color-sample, when using an M_STAT_MIN_DIST operation mode. You can also return the maximum color distance (M_MAX_DISTANCE), which is the greatest color distance between the target area and all its matching color-samples. Image results To produce image results, you will typically use McolMatch() with the ControlFlag parameter set to M_DEFAULT, and pass the identifier of a result buffer in which to write the results of the color matching operation. You will then use this result buffer with McolDraw() to draw any image result (M_DRAW_...). You can also use this result buffer to retrieve any other type of result, using McolGetResult(). In some cases you can avoid calling McolDraw() and produce an image result directly using McolMatch() by setting the ControlFlag parameter to M_DRAW_..., and passing the identifier of an image buffer in which to write the results of the color matching operation. Since you do not have to call McolDraw(), producing an image this way is typically done to save time when you require just one specific image result. For example, when performing color segmentation, you might only be interested in the label of the color-sample for which each pixel voted; therefore, using McolMatch() with M_DRAW_PIXEL_MATCH_USING_LABEL to get that one resulting image is sufficient. Note that when using McolMatch() in this way, you are not producing a result buffer and therefore will not be able to retrieve results using McolGetResult(). If you use McolDraw() to draw the images, you must first enable the required control types to save the resulting images in the result buffer, using McolControl() with M_SAVE_AREA_IMAGE and M_GENERATE_.... For example, to draw M_DRAW_AREA_MATCH_USING_COLOR, you must first enable M_SAVE_AREA_IMAGE and M_GENERATE_SAMPLE_COLOR_LUT. Unless otherwise specified, all M_DRAW_... images are available with either McolDraw() or McolMatch(). Images drawn using individual pixel match results (M_DRAW_PIXEL_MATCH_...) can only be specified when using an M_MIN_DIST_VOTE distance type. Note that when image results include color elements or entire color-samples, they are drawn using either their triplet values (M_TRIPLET) or their estimated values (for example, the average) taken from a source image (M_IMAGE), depending on how you defined them (McolDefine()). Color elements and color-samples are not drawn using the color of the target area. Target areas For each target area, you can draw: The best-matched color-sample. The image can contain either: The color of the best-matched color-sample (M_DRAW_AREA_MATCH_USING_COLOR). The label of the best-matched color-sample (M_DRAW_AREA_MATCH_USING_LABEL). The color-sample for which each pixel voted. The image can contain either: The color of the color-sample for which each pixel voted (M_DRAW_PIXEL_MATCH_USING_COLOR). The label of the color-sample for which each pixel voted (M_DRAW_PIXEL_MATCH_USING_LABEL). The distance image. The distance image (M_DRAW_DISTANCE) contains the distance between the color of the target area (for an M_STAT_MIN_DIST operation mode) or target pixel (for an M_MIN_DIST_VOTE operation mode), and the color of its best-matched color-sample. This distance value is dependent of the type of distance calculated. For more information, see the Distance between colors section earlier in this chapter. If required, you can normalize distance results. For more information, see the Distance normalization settings subsection of the Advanced color matching settings and concepts section later in this chapter. The distance image (M_DRAW_DISTANCE) and the label images (M_..._USING_LABEL) draw numerical values and are therefore grayscale images. The other images (M_..._USING_COLOR) draw colors. For information on the size and depth of the required image buffer, use McolGetResult() or McolInquire(), as required. For example, to return the depth per band (in bits) required for the image buffer in which to draw M_DRAW_AREA_MATCH_USING_COLOR, use McolGetResult() with M_SAMPLE_COLOR_SIZE_BIT. The following image illustrates an example case where color matching is performed and the five target area results are drawn: You can also further process the distance image using other MIL modules. For example, you can use the Blob Analysis module to identify blobs using the grayscale distance image and compute their features, such as area, perimeter, and min/max diameter, or use the Edge Finder module to detect crests and contours (note that Edge Finder can also be used directly with color images). Color-samples and color elements For each color-sample, you can draw: A copy of one of the internal color element images defined or added with McolDefine() (M_DRAW_SAMPLE). A copy of one of the color element's masks defined or added with McolMask() (M_DRAW_SAMPLE_MASK). A copy of the mosaic image of all the color elements found in a specified sample defined with McolDefine() (M_DRAW_SAMPLE_MOSAIC). A copy of the mosaic mask of the specified color-sample defined with McolDefine() (M_DRAW_SAMPLE_MOSAIC_DONT_CARE). The 3-band color-sample label LUT, where the label value of each color-sample is associated with its average color (M_DRAW_SAMPLE_COLOR_LUT). This image can be useful for supervised color segmentation. For example, you can draw the grayscale label image (M_DRAW_PIXEL_MATCH_USING_LABEL), and then apply the LUT (M_DRAW_SAMPLE_COLOR_LUT) to draw each pixel's color. Note that this produces the same result as drawing the colored label image (M_DRAW_PIXEL_MATCH_USING_COLOR). These images are only available with McolDraw(); they cannot be produced directly with McolMatch(). Background and outliers Background pixels are pixels that are outside the target areas and are not used in the matching operation, while outlier pixels are pixels inside a target area, but do not match with any color-sample. When drawing image results, the destination buffer's bit depth must account for background and outlier pixels, if they are drawn. When drawing the distance image, (M_DRAW_DISTANCE), outliers are also drawn and can be difficult to identify if the outlier color is similar to the resulting distance. For example, if the outlier color is 0.0, you wouldn't be able to distinguish it if the resulting distance is also 0.0. In this case, you can use M_DRAW_..._USING_LABEL, which draws the outlier label. To set the outlier and background pixel value, use McolControl() with M_BACKGROUND_DRAW_COLOR and M_OUTLIER_DRAW_COLOR. To set the outlier label, use McolControl() with M_OUTLIER_LABEL. Note that unlike the outlier label, the background label is not specified explicitly; instead, it is taken from the background pixel color. Inverting colors You can add the combination value M_INVERTED_COLORS to certain M_DRAW_... settings in McolDraw() and McolMatch() to invert the color of the best-matched color-sample in the resulting image. For inversion, MIL uses the difference between each band value of the best-matched color-sample and the maximum possible value in the image. For example, with an 8-bit image, the color-sample band values are modified as follows: [255 - ColorSampleBand0, 255 - ColorSampleBand1, 255 - ColorSampleBand2]. In this case, if you use M_DRAW_AREA_MATCH_USING_COLOR and the best-matched color-sample value is [240, 10, 255], that color becomes [15, 245, 0] when you invert it (M_DRAW_AREA_MATCH_USING_COLOR + M_INVERTED_COLORS). In general, you can use M_INVERTED_COLORS to help increase the contrast between the pixels of the best-matched color-sample and the pixels of the underlying destination image. This can prove especially useful when both M_BACKGROUND_DRAW_COLOR and M_OUTLIER_DRAW_COLOR are set to M_TRANSPARENT. Color matching Steps to performing color matching Basics of color matching Color identification Supervised color segmentation Defining and adding color-samples and color elements Area identifier image Acceptance Color-sample acceptance (for the color-sample's match score) Relevance acceptance (for the target area's relevance score) Distance tolerance Tolerance mode Operation mode and distance type M_STAT_MIN_DIST operation mode M_MIN_DIST_VOTE operation mode M_HISTOGRAM_MATCHING and M_HISTOGRAM_VOTE operation modes Basic results Best-matched color-sample Match status Match score and relevance score Outlier coverage and color-sample coverage Color distance Image results Target areas Color-samples and color elements Background and outliers Inverting colors ",
      "wordCount": 5025,
      "subEntries": []
    },
    {
      "id": "UG_color_Advanced_color_matching_settings",
      "version": null,
      "title": "Advanced color matching settings and concepts",
      "subTitles": [
        "Color band specification settings",
        "Distance normalization settings",
        "Color space encoding",
        "Performing the encoding",
        "Histogram matching concepts"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\color\\Advanced_color_matching_settings.htm",
      "text": " Advanced color matching settings and concepts In addition to applying fundamental color matching specifications, you can also use advanced settings to write highly customized color matching applications. This includes color band selection, distance normalization, color space encoding, and the histogram match mode. Color band specification settings By default, all bands are used when performing the match operation. You can, however, specify one or two specific bands, using McolControl() with M_BAND_MODE. For example, if you are using HSL images, you can match with only the hue (H) component by setting M_BAND_MODE to M_COLOR_BAND_0. This can be useful if your image has non-uniform lighting, shadows, or highlights. A similar match can be performed if you are matching in CIELAB and match with only the chrominance components (bands A and B), by setting M_BAND_MODE to M_COLOR_BAND_1 + M_COLOR_BAND_2. Distance normalization settings MIL calculates color distances (with McolMatch() or McolDistance()) as numerical floating-point values and internally stores them as such. These exact distance values are then written to the destination buffer, provided that this buffer is of type floating-point. If the buffer is of type integer, distance values are truncated; that is, due to the constraint of the buffer type, the fractional (decimal) portion of the distance result is not written. However, before color distance results are written to the destination buffer, you can normalize them according to a multiplicative factor, using either the NormalizeValue parameter (for McolDistance()) or McolControl() with M_DISTANCE_IMAGE_NORMALIZE (for McolMatch()). You can specify a specific normalization value, or use M_MAX_NORMALIZE to normalize distances with the greatest calculated distance. This value corresponds to the result M_MAX_DISTANCE (McolGetResult()). To specify no normalization, use M_NO_NORMALIZE. The normalization performed is as follows (when using M_MAX_NORMALIZE): (Distance/GreatestCalculatedDistance) x MaximumPossibleValueOfDestBuf (for destination buffer's of type integer). (Distance/GreatestCalculatedDistance) (for destination buffer's of type floating-point). If the destination buffer is of type floating-point, you would typically not normalize the distance results, since all this will do is return a value between 0.0 and 1.0. The following example shows the distance between two colors (FirstSourceImage - SecondSourceImage) using no normalization (M_NO_NORMALIZE), and using maximum normalization (M_MAX_NORMALIZE). Distance results are always clipped at the destination buffer's maximal possible value, which can occur when specifying M_NO_NORMALIZE or a specific normalization factor. For example, when using an 8-bit unsigned destination buffer, distances higher than 255 will be written as 255. Note that if you specify a destination buffer of type floating-point, distance results would almost never be greater than the buffer's maximal possible value, since its limit is extremely high. Once normalized, the resulting value (grayscale) written in the destination buffer does not represent a precise distance (between colors) and should never be interpreted as such. You can however make the general conclusion that the higher (brighter) this grayscale value is, the greater the color distance was. Normalization can therefore be seen as a way to remap distance values according to the destination buffer's dynamic range to minimize the loss of data and to obtain a buffer that you can then use for other types of grayscale processing, such as thresholding and blob analysis. Normalization is applied when drawing the distance image (M_DRAW_DISTANCE), using either McolDraw() or McolMatch(). This can typically improve the display and visualization of this image. Note that since MIL internally stores distance values as 32-bit float, you can still draw the distance image in an unsigned image buffer of type integer. Color space encoding Color space encoding (of the source color space) determines how color is transformed from the range represented in an image buffer to its native (theoretical) range, which is device-independent. For example, RGB color space data is represented in an image buffer as values between 0 and 255 (8-bit); these values are then mapped to their native data range, which consists of all real numbers between 0 and 1. You must set the color space encoding according to the actual dynamic range of your color data, using McolControl() with M_ENCODING. By default, the Color Analysis module assumes that you are using an 8-bit color space encoding, regardless of the depth of your buffers. If this default is not appropriate, you should modify it with M_ENCODING accordingly. For example, if your color data was acquired with a 16-bit camera, you should set M_ENCODING to M_16BIT. A number of predefined color space encoding settings are provided (M_nBIT). These are typically sufficient for most applications. You can even use image buffers that exceed the actual dynamic range of your color data, provided that their content respects this range. For example, if you set M_ENCODING to M_8BIT, you can use 16-bit image buffers that contain values between 0 and 255 (8-bit); however, if they contain values outside this range, results will be inconsistent. If required, you can explicitly set, for each band, specific offset (M_OFFSET_BAND_n) and scale (M_SCALE_BAND_n) values that specify how the color data should be transformed from the range represented in an image buffer to its native (theoretical) range. In this case, you must set M_ENCODING to M_USER_DEFINED. Performing the encoding To actually encode the data, the following native range is used for the color spaces: RGB Color space band Native Min Native max R 0.0 1.0 G 0.0 1.0 B 0.0 1.0 LAB Color space band Native Min Native max L 0.0 100.0 A -128.0 127.0 B -128.0 127.0 HSL Color space band Native Min Native max H 0.0 1.0 (normalized angle) S 0.0 1.0 L 0.0 1.0 If you use M_nBIT, the Color Analysis module takes the color space's native range, and the range represented in the image buffer, and uses it to internally apply the following offset and scale, to perform the encoding: Offset Color Space M_ENCODING M_OFFSET_BAND_0 M_OFFSET_BAND_1 M_OFFSET_BAND_2 RGB M_nBIT 0 0 0 CIELAB M_8BIT 0 128 128 M_nBIT 0 128 x (2 n -1) / 255 128 x (2 n -1) / 255 HSL M_nBIT 0 0 0 Scale Color Space M_ENCODING M_SCALE_BAND_0 M_SCALE_BAND_1 M_SCALE_BAND_2 RGB M_nBIT 1.0 / (2 n -1) 1.0 / (2 n -1) 1.0 / (2 n -1) CIELAB M_nBIT 100.0 / (2 n -1) 255 / (2 n -1) 255 / (2 n -1) HSL M_nBIT 1.0 / (2 n ) 1.0 / (2 n -1) 1.0 / (2 n -1) In this table, n refers to the buffer's depth. Note that the H component in HSL is represented by a normalized angle, which is transformed according to the MIL angle convention. For example, in an 8-bit image buffer, 360 is mapped to 256. Since 0 and 360 are the same angle, both use the same numerical value, which is 0. If you use M_USER_DEFINED, the following encoding is applied using the specified offset (M_OFFSET_BAND_n) and scale (M_SCALE_BAND_n), for each band: The offset (M_OFFSET_BAND_n) is subtracted from the color value represented by the image buffer. The resulting color value, as modified by M_OFFSET_BAND_n, is then multiplied by M_SCALE_BAND_n. To determine the scale and offset values that you should use with M_USER_DEFINED, you can perform the following calculations: Where: [n1, n2] is the native range of the color space. [e1, e2] is the numerical range of the image buffer. Histogram matching concepts M_HISTOGRAM_MATCHING mode uses histogram bins as the basis of the matching operation. A frequency is associated with each bin, corresponding to the number of occurrences of the bin's color triplets within the image or color-sample. You must specify the number of bins for each color band using McolControl() with M_NB_BINS_BAND_n. The total number of histogram bins contained within the color space is the product of the number of bins located along each color band. For example, if M_NB_BINS_BAND_0 is set to 3, M_NB_BINS_BAND_1 is set to 4, and M_NB_BINS_BAND_2 is set to 5, the total number of histogram bins is 3 * 4 * 5 = 60 bins. The histogram matching is performed according to the steps described below: The color histograms of the target image and predefined color-samples are generated in the color space used to perform the matching. The frequency of each bin is calculated based on the individual frequencies of its constituent pixels. The bin frequencies are then normalized between 0.0 and 1.0, with 1.0 representing the sum of the frequencies of all bins. Matching occurs between the two bins with the highest frequencies, provided their color distance is within the distance specified using McolControl() with M_DISTANCE_TOLERANCE. The match score S is incremented according to the formula Sn = Sn-1 + min(f bin1, f bin2)*(1 - colorDistance(colorbin1, colorbin2)), where: f represents a normalized bin frequency (0.0 &lt;= f &lt;= 1.0). colorDistance corresponds to the normalized color distance between the two specified colors, within the color space used for matching (0.0 &lt;= colorDistance &lt;= 1.0). The lesser of the two bin frequencies is then subtracted from both bin frequencies, resulting in the lesser bin being completely emptied. Steps 3 and 4 are repeated until no more bins can be matched. The final match score is normalized as a percentage between 0.0 and 100.0, with a score of 100.0 indicating a perfect match. The relevance score for the matching operation is calculated according to the formula RelevanceScore = (100.0 - BestScore)-1 / sum((100.0 - Score)-1 ), where: BestScore refers to the score of the winning color-sample. The sum is taken over all the color-samples, and Score refers to the score of the sample being summed. You can also perform histogram matching using McolSetMethod() with M_HISTOGRAM_VOTE. In this case, MIL computes color histograms for each defined color-sample and, for each target pixel, identifies its histogram bin. Each target pixel votes for the best color-sample, provided its histogram bin is not empty. The number of votes that a color-sample accumulates determines its score. The color-sample with the highest score above the acceptance (McolControl() with M_ACCEPTANCE) is the target area's best-matched color-sample. Advanced color matching settings and concepts Color band specification settings Distance normalization settings Color space encoding Performing the encoding Histogram matching concepts ",
      "wordCount": 1649,
      "subEntries": []
    },
    {
      "id": "UG_color_Color_separation",
      "version": null,
      "title": "Color separation",
      "subTitles": [
        "Separation operation"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\color\\Color_separation.htm",
      "text": " Color separation You can use McolProject() with M_COLOR_SEPARATION to remove a color from an image. Eliminating a color can be useful since it allows you to isolate colors which should be considered part of the background. To perform color separation, you must identify the background, selected, and rejected colors; these are referred to as the separation colors. By properly specifying the colors, the projection operation is able to identify the unwanted color information (the stamp), and create a new version of the image that only contains the wanted color information (the background and the signature). The following two-dimensional graphs illustrate the color distribution of the signature/stamp image, and how that distribution is modified to remove the stamp and keep the signature. Various conditions, such as different cameras and illuminants, can cause color from identical images to appear dissimilar. If this occurs, you can call McolTransform() to perform a relative color calibration before separating colors. Relative color calibration ensures all colors are consistent according to a specified reference color. For more information, see the Relative color calibration section earlier in this chapter. Separation operation The color separation operation can return either an image with the color separation result, or the transformation matrix with which to perform the color separation. This depends on whether you store the result in an image buffer or a MIL array buffer. If you retrieve the matrix, you can then use it with MimConvert() to separate the colors. When doing this, make sure that the color distribution of all the images to convert is similar, otherwise unpredictable results can occur. You can pass the separation colors to McolProject() as either three explicit color values, or as colors taken from the source image. To specify explicit color values, you must use a 3x3 MIL array buffer. To specify colors from the source image, you must use a data identification image that identifies the corresponding source image pixels to use as the background (M_BACKGROUND_LABEL), selected (M_SELECTED_LABEL), and rejected (M_REJECTED_LABEL) colors. MIL ignores pixels in the source image that you do not identify as background, selected, or rejected. You must identify at least one pixel for each. If you identify multiple pixels as background, selected, or rejected, MIL averages the colors of those pixels. If the data identification image is smaller than the source image, MIL ignores the outlying source image pixels. Color separation Separation operation ",
      "wordCount": 398,
      "subEntries": []
    },
    {
      "id": "UG_color_Color_statistics",
      "version": null,
      "title": "Color statistics",
      "subTitles": [
        "Covariance",
        "Principal components",
        "Source label"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\color\\Color_statistics.htm",
      "text": " Color statistics You can call McolProject() to perform advanced statistical and analytical tasks, such as calculating the source image's covariance matrix or principal components, which are used in, for example, a principal component analysis (PCA). McolProject() can perform the calculation using the entire source image or specified areas of the source image. Various conditions, such as different cameras and illuminants, can cause color from identical images to appear dissimilar. If this occurs, you can call McolTransform() to perform a relative color calibration before calling McolProject(). Relative color calibration ensures all colors are consistent according to a specified reference color. For more information, see the Relative color calibration section earlier in this chapter. Covariance If you use M_COVARIANCE, and you specify a 3x3 MIL array as the destination buffer, McolProject() calculates the following covariance matrix, which indicates the variation of color in the source image: If you specify a 4x3 MIL array buffer, McolProject() also returns the mean color of the source in the fourth column. Principal components If you use M_PRINCIPAL_COMPONENTS, McolProject() performs a principal component analysis (PCA) to calculate the three principal components of the source image's color information. The three principal components correspond to the three eigenvectors of the image's covariance matrix. M_PRINCIPAL_COMPONENTS can also return each eigenvector's eigenvalue and the source image's average color. The information that M_PRINCIPAL_COMPONENTS returns depends on the size of the MIL array that you specify as the destination buffer: 3x3. M_PRINCIPAL_COMPONENTS returns the three eigenvectors columnwise in decreasing order of strength. The first eigenvector in the array is the strongest, and known as the first principal component. If no vector is stronger, the order is arbitrary. 4x3. In addition to the 3x3 array results, MIL returns each eigenvectors's eigenvalue in the fourth column. The eigenvalue corresponds to the eigenvector's strength. 5x3. In addition to the 4x3 array results, M_PRINCIPAL_COMPONENTS returns the mean color of the source image in the fifth column, band-by-band: SourceMeanValueBand0, SourceMeanValueBand1, and SourceMeanValueBand2. Source label Instead of McolProject() using the entire source image to perform its calculations, you can identify specific source pixels to use. This allows you to isolate the areas of the image that contain the required colors and achieve more specialized results. To identify the areas in the source image with which to perform the calculation, you must specify another image, referred to as a data identification image. In this image, you must set the corresponding pixels to M_SOURCE_LABEL. Color statistics Covariance Principal components Source label ",
      "wordCount": 412,
      "subEntries": []
    },
    {
      "id": "UG_color_Color_processing_and_analysis_example",
      "version": null,
      "title": "Color processing and analysis examples",
      "subTitles": [
        "Example of relative color calibration operations",
        "Example of color processing operations"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\color\\Color_processing_and_analysis_example.htm",
      "text": " Color processing and analysis examples The following examples demonstrate the various operations available with the MIL Color Analysis module: colorrelativecalibration.cpp mcol.cpp To run these examples, use the Matrox Example Launcher in the MIL Control Center. Example of relative color calibration operations The example ColorRelativeCalibration.cpp shows you how to use relative color calibration to perform: Food inspection, with M_HISTOGRAM_BASED (McolSetMethod()). Print inspection, with M_COLOR_TO_COLOR (McolSetMethod()). Electronic board inspection, with M_GLOBAL_MEAN_VARIANCE (McolSetMethod()). Example of color processing operations The example Mcol.cpp shows you how to use color processing operations to perform: Color segmentation of an image by classifying each pixel with 1 out of 6 color-samples. The ratio of each color in the image is then calculated. Color identification of circular regions in objects located with the MIL Model Finder module. Color separation of a signature and a stamp. Color processing and analysis examples Example of relative color calibration operations Example of color processing operations ",
      "wordCount": 154,
      "subEntries": []
    }
  ]
}]
[{
  "id": "UG_advanced-image",
  "version": "2024020714",
  "title": "Advanced image processing",
  "subTitles": null,
  "location": "MIL UG P03: 2D processing and analysis",
  "pageURL": "content\\UserGuide\\advanced-image\\ChapterInformation.htm",
  "text": " Chapter 4: Advanced image processing This chapter describes different advanced image processing techniques. Advanced image processing in general Custom spatial filters Finite Impulse Response (FIR) filters Infinite Impulse Response (IIR) filters An example Custom morphological operations Defining your own structuring element Erosion and dilation Using standard erosion and dilation An example Thinning and thickening Top and bottom hat Matching Searching for hits or misses Distance transform City Block transform Chessboard transform Chamfer 3-4 transform Watershed transformations Using watersheds to separate touching objects Using watersheds to separate objects from their background Minimum grayscale variation of a catchment basin Using marker images Non-labeled marker images Labeled marker images Style of the watershed lines Exact versus straight Skipping the last level Filling the source Connectivity mapping Using the connectivity code Locating different types of points Example Labeling Polar-to-rectangular and rectangular-to-polar transforms Warping First-order polynomial warpings Perspective polynomial warpings Polar-to-rectangular, rectangular-to-polar, and custom warpings Interpolation modes Points outside the source buffer Transforming coordinate lists Warping example Finding dominant image orientations Customizing your find orientation image processing context Using the find orientation image processing results An example Establishing a correlation ",
  "wordCount": 187,
  "subEntries": [
    {
      "id": "UG_advanced-image_Advanced_image_processing_in_general",
      "version": null,
      "title": "Advanced image processing in general",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\advanced-image\\Advanced_image_processing_in_general.htm",
      "text": " Advanced image processing in general Besides the image processing operations discussed in the previous chapter, MIL contains more advanced image processing operations. These advanced operations allow you to, for example, separate objects from their background, perform polar-to-rectangular operations, and correct image distortions. They also include performing neighborhood operations using custom structuring elements or kernels. Advanced image processing in general ",
      "wordCount": 61,
      "subEntries": []
    },
    {
      "id": "UG_advanced-image_Custom_spatial_filters",
      "version": null,
      "title": "Custom spatial filters",
      "subTitles": [
        "Finite Impulse Response (FIR) filters",
        "Infinite Impulse Response (IIR) filters",
        "An example"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\advanced-image\\Custom_spatial_filters.htm",
      "text": " Custom spatial filters Spatial filtering operations include operations such as smoothing, denoising, edge enhancement, and edge detection. Spatial filtering operations compute results based on an underlying neighborhood process: the weighted sum of a pixel value and its neighbors' values. There are two types of spatial filters: Finite Impulse Response (FIR) filters and Infinite Impulse Response (IIR) filters. FIR filters operate on a finite neighborhood, while IIR filters take into account all values in an image. In MIL, you can specify either a predefined FIR spatial filter or a custom FIR or IIR spatial filter. To apply custom FIR filters, you use MimConvolve(). To apply custom IIR filters, you use MimConvolve() or MimDifferential(), depending on the filter operation. Finite Impulse Response (FIR) filters For FIR filters, the weights are known as the kernel values. These kernel values determine the operation type of the spatial filter. For example, applying the following FIR filter results in a sharpening of the image: Whereas, applying the following FIR filter smooths an image (it also increases the intensity of the image by a factor of 16, so you will need to normalize the convolution result): When using FIR filters, the dimensions of the kernel determine the size of the neighborhood that is used in the operation. The result of the operation is stored in the destination buffer at the location corresponding to the kernel's center pixel. When the kernel has an even number of rows and/or columns, the center pixel is considered to be the top-left pixel of the central elements in the neighborhood. Calculate the X-coordinate of the top-left pixel of the central elements, as follows: If the width (SizeX) of the kernel is an odd number, the X-coordinate is (SizeX-1)/2. If the width (SizeX) of the kernel is an even number, the X-coordinate is (SizeX/2)-1. To calculate the Y-coordinate of the top-left pixel of the central elements, the same rules apply. Regardless of the location of the center pixel, there will be some border pixels that have an incomplete neighborhood. To deal with this issue, the image buffer is overscanned. There are several types of overscan. A transparent overscan uses the parent buffer to provide the overscan pixels needed for the border calculation. If the parent buffer is not available, a mirror overscan is performed. A mirror overscan specifies that the overscan pixels will be a mirror copy of the source buffer's border pixels. A replicate overscan specifies to repeat border pixel values along each row and column of the overscan region. A replacement overscan allows you to specify a specific value for the overscan pixel values during processing. It is recommended that you experiment to find the best overscan option for your application. For more information on overscan, see the Buffer overscan region section of Chapter 23: Data buffers. If the predefined FIR filters provided by MimConvolve() do not meet your requirements, you can create your own custom FIR filter. Before resorting to a custom filter, if speed is not an issue, try the bilateral adaptive filter M_BILATERAL, available using MimFilterAdaptive(), which uses a FIR filter. For more information, see the Adaptive filters subsection of the Denoise using spatial filtering and area open and close operations section of Chapter 3: Fundamental image processing. To define your own custom FIR filter: Allocate a kernel buffer, using MbufAlloc1d() or MbufAlloc2d() with M_KERNEL. The kernel size is specified when allocating the kernel buffer. Note that the kernel size can be constrained by the available resources. Load the required kernel values into this kernel buffer, using MbufPut() or MbufPut2d(). If required, modify the setting of the operation control types associated with your custom filter, using MbufControl(). The operation control types determine how the convolution operation will be performed. You can control: Whether or not the absolute value of the result is taken (M_ABSOLUTE_VALUE). The division (normalization) factor to apply to the result (M_NORMALIZATION_FACTOR). Whether or not to saturate the result (M_SATURATION). The position of the center pixel (M_OFFSET_CENTER_X and M_OFFSET_CENTER_Y). How the operation handles the borders (overscan) of the source buffer (M_OVERSCAN and M_OVERSCAN_REPLACE_VALUE). If overscan is disabled, the border pixels of the source image are not processed if additional processing time is needed. To apply your own custom FIR filter, call the MimConvolve() function, specifying the identifier of the required kernel buffer (FilterContextImOrKernelBufId). To increase the speed of the convolution operation when using your custom FIR filter, MimConvolve() will automatically separate large kernels, if separable, into two 1-dimensional kernels (aij = hivj ). Performing two separate convolutions, once with Hmx1 and once with V1xn , can be faster and is equivalent to performing one convolution operation using the original Amxn kernel. MIL will internally separate large kernels when it detects that the separation results in better performance. The following displays an Amxn kernel separated into two 1-dimensional kernels (Hm and Vn ). Infinite Impulse Response (IIR) filters When using IIR filters, the weights are automatically determined by the type of filter, the mode of the filter, the type of operation to perform, and the degree of smoothness (strength of denoising) applied by the filter. The type of filter determines the distribution of the neighborhoods' influence. MIL supports three types of IIR spatial filters: Deriche filter, Vliet filter, and Shen-Castan filter. The Deriche and Vliet filters are accurate approximations of the Gaussian function, with the Vliet filter being more accurate for a large sigma coefficient (MimControl() with M_FILTER_SMOOTHNESS). The Shen-Castan filter, on the other hand, is an accurate approximation of the Exponential function. For the Deriche and Vliet filters, the neighborhoods' influence decreases much slower as the distance from the central pixel increases, compared to the Shen-Castan filter. For more information on the Deriche and Shen-Castan filters in the context of edge detection, see the Customizing the edge extraction settings section of Chapter 10: Edge Finder. To define and use a custom IIR filter: Allocate a linear IIR filter context, using MimAlloc() with M_LINEAR_FILTER_IIR_CONTEXT. Use MimControl() and specify appropriate M_FILTER_TYPE, M_FILTER_OPERATION, M_FILTER_RESPONSE_TYPE, M_FILTER_SMOOTHNESS_TYPE, and M_FILTER_SMOOTHNESS operation control type settings. Use MimConvolve() to specify the source and destination image buffers, as well as to specify a linear IIR filter context. Alternatively, use MimDifferential() to specify the source and destination image buffers for specific IIR operations that require more than one convolution. The following code snippet performs a smoothing operation using a Deriche custom IIR filter: MIL_ID MilSystem = 0, /* System identifier. */ MilSrcImage = 0, /* Source image identifier */ MilDstImage = 0, /* Destination image identifier */ MilFilter = 0; /* Filter identifier. */ /* Smoothing the source image. */ /* Allocate the kernel. */ MimAlloc(MilSystem, M_LINEAR_FILTER_IIR_CONTEXT, M_DEFAULT, &amp;MilFilter); /* Set the kernel properties. */ MimControl(MilFilter, M_FILTER_TYPE, M_DERICHE); MimControl(MilFilter, M_FILTER_SMOOTHNESS, 50.0); MimControl(MilFilter, M_FILTER_OPERATION, M_SMOOTH); /**/ /* Smooth the image. */ MimConvolve(MilSrcImage, MilDstImage, MilFilter); /**/ /* Free the kernel. */ MimFree(MilFilter); This second snippet performs first derivative calculations: MIL_ID MilSystem = 0, /* System identifier. */ MilSrcImage = 0, /* Source image identifier */ MilDstDxImage = 0, /* X derivative destination image identifier */ MilDstDyImage = 0, /* Y derivative destination image identifier */ MilFilter = 0; /* Filter identifier. */ /* Calculate derivatives. */ /* Allocate the kernel. */ MimAlloc(MilSystem, M_LINEAR_FILTER_IIR_CONTEXT, M_DEFAULT, &amp;MilFilter); /* Set the kernel properties. */ MimControl(MilFilter, M_FILTER_TYPE, M_DERICHE); MimControl(MilFilter, M_FILTER_SMOOTHNESS, 50.0); MimControl(MilFilter, M_FILTER_RESPONSE_TYPE, M_SLOPE); /**/ /* Extract relevant derivatives. */ MimControl(MilFilter, M_FILTER_OPERATION, M_FIRST_DERIVATIVE_X); MimConvolve(MilSrcImage, MilDstDxImage, MilFilter); MimControl(MilFilter, M_FILTER_OPERATION, M_FIRST_DERIVATIVE_Y); MimConvolve(MilSrcImage, MilDstDyImage, MilFilter); /**/ /* Free the kernel. */ MimFree(MilFilter); An example The following is an example of a spatial filtering operation using a custom FIR filter with a 3 by 3 kernel. mimconvolve.cpp Custom spatial filters Finite Impulse Response (FIR) filters Infinite Impulse Response (IIR) filters An example ",
      "wordCount": 1278,
      "subEntries": []
    },
    {
      "id": "UG_advanced-image_Custom_morphological_operations",
      "version": null,
      "title": "Custom morphological operations",
      "subTitles": [
        "Defining your own structuring element",
        "Erosion and dilation",
        "Using standard erosion and dilation",
        "An example",
        "Thinning and thickening",
        "Top and bottom hat",
        "Matching",
        "Searching for hits or misses"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\advanced-image\\Custom_morphological_operations.htm",
      "text": " Custom morphological operations Morphological operations are neighborhood operations that compute new values according to geometric relationships and matches of known patterns in the input image. The MimMorphic() function supports different types of morphological operations: Erosion. Dilation. Thinning. Thickening. Top hat operations. Thickening. Bottom hat operations. Hit or miss transformation. You specify the required geometric relationships for each of these operations using a structuring element. Defining your own structuring element To define your own structuring element: Allocate a structuring element buffer (M_STRUCT_ELEMENT), using MbufAlloc2d(). The dimensions of the structuring element determine the size of the neighborhood that is used in the operation. The result of the operation is stored in the destination buffer at the location that corresponds to the structuring element's center pixel. When the structuring element has an even number of rows and/or columns, the center pixel is considered to be the top-left pixel of the central elements in the neighborhood (see the Custom spatial filters section earlier in this chapter). Load the structuring element values into this buffer, using MbufPut() or MbufPut2d(). Give the structuring element values according to the morphological operation that is to be performed. Any structuring element value can be used, including M_DONT_CARE (which means that the corresponding neighbor is not considered in the comparison). For all binary operations, as well as for the grayscale thin and thick operations, any structuring element value other than 0 and M_DONT_CARE are interpreted as a 1. For custom structuring elements, you can use MbufControl() to control how the operation handles the borders (overscan) of the source buffer (see the Custom spatial filters section earlier in this chapter) and the position of the neighborhood's center pixel. Erosion and dilation Two fundamental morphological operations are erosion and dilation. These functions allow you to view the possible growth stages of an object in the foreground (non-zero pixels) of an image. There are two versions of erosion and dilation: Erosion (M_ERODE). Binary erosion. If the structuring element does not match the corresponding neighborhood values exactly, the center pixel is set to zero; otherwise, it remains unchanged. In effect, binary erosion peels off layers of objects. Grayscale erosion. Subtracts each structuring element value from the corresponding pixel value in the neighborhood, and then replaces the center pixel of the neighborhood with the minimum value from the resulting neighborhood values. Dilation (M_DILATE). Binary dilation. If any of the structuring element values match the corresponding neighborhood values, the center pixel is set to the maximum value of the buffer (for example, 0xff for an 8-bit buffer); otherwise, it remains unchanged. In effect, binary dilation adds layers to the objects. Grayscale dilation. Adds each structuring element value to the corresponding pixel value in the neighborhood, and then replaces the center pixel of the neighborhood with the maximum value from the resulting neighborhood values. Note, in binary mode, erosion of the white pixels is the same as dilation of the black pixels. If the processing mode is set to M_BINARY, a binary erosion or dilation is performed and all non-zero pixels are considered as 1's; otherwise, the grayscale version of these operations is performed. Use MblobReconstruct() to perform a conditional dilation. Using standard erosion and dilation MIL also supports MimErode() and MimDilate(), functions specialized in performing the most standard form of erosion and dilation operation. These operations use the following structuring element when performing in binary mode: And use the following structuring element in grayscale mode: In other words, these functions execute a simple 3 by 3 minimum or maximum operation without adding or subtracting anything from the pixel. For example, to perform the most standard dilation operation on a source image buffer, use MimDilate() with the processing mode set to M_BINARY, or use MimMorphic() with a 3x3 structuring element of ones and the processing mode set to M_BINARY. Note, in general the standard version is faster. An example The following example shows how to define your own structuring element. It demonstrates, on an image with rounded objects, the difference between performing the standard opening operation, MimOpen(), and performing a custom opening with a circular type structuring element. Note, the latter preserves the original shape of the objects better than the square structuring element of the standard erosion. mimmorphic.cpp Thinning and thickening You can reduce or enlarge objects in the foreground (non-zero pixels) of an image, using operations based on a rigid match of the pixel's neighborhood and the structuring element. Using a thickening operation, you can enlarge the object and perform such operations as a convex hull. Using a thinning operation, you can reduce objects and perform such operations as finding their skeleton. You can perform a thinning or thickening operation with a specified structuring element, using MimMorphic(). These operations are typically performed several times, using a different structuring element so that the required pattern is sought in different directions. You can also perform standard thinning or thickening operations with MimThin() or MimThick(), respectively. There are two versions of thinning and thickening: Thinning (M_THIN). Binary thinning. This operation replaces the center pixel by the value zero if a pixel's neighborhood matches the structuring element exactly. However, if the neighborhood does not match, the pixel value remains unchanged. Grayscale thinning. If (MAX(0) &lt; center pixel &lt;= MIN(1)) then (center pixel = MAX(0)) else (center pixel is unchanged). Where MAX(0) is the maximum of all pixels in the neighborhood that correspond to zero in the structuring element, and MIN(1) is the minimum of all pixels in the neighborhood that correspond to one in the structuring element. Thickening (M_THICK). Binary thickening. This operation replaces the center pixel by the maximum possible value of the buffer (for example, 0xff for an 8-bit buffer) if the pixel's neighborhood matches the structuring element exactly. However, if the neighborhood does not match, the pixel value remains unchanged. Grayscale thickening. If (MAX(0) &lt;= center pixel &lt; MIN(1)) then (center pixel = MIN(1)) else (center pixel is unchanged). Where MAX(0) is the maximum of all pixels in the neighborhood that correspond to zero in the structuring element, or MIN(1) is the minimum of all pixels in the neighborhood that correspond to one in the structuring element. Note that all structuring elements values (other than 0 and M_DONT_CARE) are interpreted as a 1. If the processing mode is set to M_BINARY, a binary thinning or thickening is performed, otherwise the grayscale version of these operations is performed. Top and bottom hat To isolate thin elements within a given image, you can perform a top hat or bottom hat operation, using MimMorphic() with M_TOP_HAT or M_BOTTOM_HAT, respectively. A top hat operation produces an image containing those elements that are smaller than the provided structuring element and brighter than their surroundings. A top hat operation is the same as an open operation (M_OPEN) followed by a subtraction of the results from the source image. A bottom hat operation produces an image containing those elements that are smaller than the provided structuring element and darker than their surroundings. A bottom hat operation is the same as a close operation (M_CLOSE) followed by a subtraction of the source image from the results. For best effect, the structuring element should be at least half the width of the largest element to be extracted from the image. Successive open (or close) operations will make thinner objects disappear. Once removed from the image, a subtraction of the results of the operation from the source image will result in an image containing only the thin elements. Use the top hat and bottom hat operations to isolate defects in images, highlight a portion of an object that is narrower or wider than expected, or to make letters or numbers stand out from their background before using the MIL OCR, String Reader, or SureDotOCR modules. The size of the small bright objects (or dark objects respectively) that are isolated is determined by the size of the structuring element and the number of iterations performed. In the following image, seven iterations of the top hat operation were performed using a 3x3 structuring element. This results in the letter \"G\" becoming more prominent than its background. Matching Matching allows you to determine the degree of similarity between certain areas of the image and a pattern (specified by a structuring element). The operation takes a binary or grayscale source image and produces a corresponding grayscale image, whereby the value of each destination pixel is equal to the total number of matches between the neighborhood of the source image's corresponding pixel and the structuring element values. To perform this matching operation, use MimMorphic() with M_MATCH. Searching for hits or misses You can determine which pixels have neighborhoods that match a pattern exactly by performing a 'hit or miss' operation. When the neighborhood of a source image's pixel matches the pattern exactly, the value of the corresponding pixel in the destination image is set to the maximum value of the buffer (for example, 0xff for an 8-bit buffer), except in the case of a floating-point buffer. In the latter case, if an exact match is found, the destination pixel is set to 1. When the neighborhood does not match exactly, the pixel value is set to zero. To search for hits or misses, use MimMorphic() with M_HIT_OR_MISS. Custom morphological operations Defining your own structuring element Erosion and dilation Using standard erosion and dilation An example Thinning and thickening Top and bottom hat Matching Searching for hits or misses ",
      "wordCount": 1569,
      "subEntries": []
    },
    {
      "id": "UG_advanced-image_Distance_transform",
      "version": null,
      "title": "Distance transform",
      "subTitles": [
        "City Block transform",
        "Chessboard transform",
        "Chamfer 3-4 transform"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\advanced-image\\Distance_transform.htm",
      "text": " Distance transform You can produce a distance transform using MimDistance(). This function determines the minimum distance from each foreground (non-zero) pixel to a background (zero) pixel, and assigns this distance to the foreground pixel. It produces a type of contour mapping of an image's foreground (object) pixels. You can calculate the minimum distance using one of three transforms: City Block transform. Chessboard transform. Chamfer 3-4 transform. City Block transform The City Block transform (M_CITY_BLOCK) determines the minimum distance using only horizontal or vertical steps. Each step counts as 1. Chessboard transform The Chessboard transform (M_CHESSBOARD) determines the minimum distance using horizontal, vertical, or diagonal steps. Each step counts as 1. Chamfer 3-4 transform The Chamfer 3-4 transform (M_CHAMFER_3_4), like the Chessboard transform, determines the minimum distance using horizontal, vertical, or diagonal steps. However, horizontal and vertical steps are counted as 3 and diagonal steps as 4. This allows the transform to better approximate the true (Euclidean) distance between two pixels. However, it requires that the destination buffer be large enough to hold a number at least three times the maximum distance from a foreground to a background pixel. For example, an 8-bit buffer (255 max) can be used for a maximum distance of 85 pixels and a 16-bit buffer (65535 max) for a maximum distance of 21845 pixels. Distance transform City Block transform Chessboard transform Chamfer 3-4 transform ",
      "wordCount": 230,
      "subEntries": []
    },
    {
      "id": "UG_advanced-image_Watershed_transformations",
      "version": null,
      "title": "Watershed transformations",
      "subTitles": [
        "Using watersheds to separate touching objects",
        "Using watersheds to separate objects from their background",
        "Minimum grayscale variation of a catchment basin",
        "Using marker images",
        "Non-labeled marker images",
        "Labeled marker images",
        "Style of the watershed lines",
        "Exact versus straight",
        "Skipping the last level",
        "Filling the source"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\advanced-image\\Watershed_transformations.htm",
      "text": " Watershed transformations You can perform watershed transformations using MimWatershed(). A watershed transformation is generally used in conjunction with other processing operations to segment images, that is, to separate objects from their background and/or from each other. To understand what a watershed transformation is, it is useful to think of an image as a topographic surface with hills and valleys. In other words, the value of each pixel represents a certain height, with the lowest pixel value (the darkest pixel) representing the point of lowest elevation and the highest pixel value (the brightest pixel) representing the point of highest elevation. A minimum in the image is defined as a pixel or a set of connected pixels that is lower in value (or elevation) than all its neighboring pixels. A maximum is a pixel or a set of connected pixels which is higher in value (elevation) than all its neighboring pixels. Pixels are connected if they are vertically, horizontally, or diagonally adjacent. A catchment basin refers to a minimum or maximum's zone of influence. For example, for a minimum, a catchment basin refers to the set of pixels from which, if a drop of water were to fall, it would eventually reach that minimum. MimWatershed() labels an image's catchment basins and/or builds dividing lines between the catchment basins. These dividing lines are known as the watershed lines of the image. Note that catchment basins can be determined from the image's minima or its maxima. Using watersheds to separate touching objects You can use MimWatershed() in conjunction with MimDistance() and MimArith() to separate touching objects in a binary image. To summarize: Perform a distance transformation on the image. This will result in a grayscale image with a maximum in each object. Perform a watershed transformation on the resulting image. Note that: Catchment basins must be determined from the image's maxima rather than its minima since MimDistance() produces a maximum in each object. The transform must show only watershed lines. To save time, you can prevent watershed lines from extending into the background. You can also specify that the watershed lines be straight. Note that these options are discussed in more detail later. You must specify the minimum grayscale variation that is required to produce a new catchment basin (this is discussed in more detail later). In general, when separating touching objects in a binary image, a low value (2) is usually sufficient. Perform an AND operation between the original image and the result of step 2, using MimArith(). Using watersheds to separate objects from their background MimWatershed() can be used in conjunction with other processing operations to separate objects from their background. For example, if the objects have well-defined edges, an edge detection will produce a maximum along the edges of each object. These maxima will define each object as a catchment basin since they produce a minimum in each object. A watershed transformation will then label the catchment basins, effectively segmenting the image. To summarize: Perform an edge detection on the image. Determine, through some analysis of the resulting image, the minimum grayscale variation that is required to produce a new catchment basin (this is discussed in the next section). Perform a watershed transformation on the resulting image. You must specify that catchment basins be determined from the image's minima. In addition, the final image should only show labeled catchment basins. Minimum grayscale variation of a catchment basin A typical image contains a lot of unwanted extrema, often due to noise. If catchment basins were determined from each extremum, the transform would segment various noise areas, resulting in over-segmentation. To prevent over-segmentation while still separating objects from their background, you can specify the minimum grayscale variation of a catchment basin, using the MinVariation parameter of MimWatershed(). The minimum variation of a catchment basin can be more easily understood using the topographical surface analogy. When dealing with minima, the minimum grayscale variation can be thought of as the minimum depth of the valley required to produce a catchment basin. The reverse applies when dealing with maxima. The following shows a line profile across an object in an image (after an edge detection is performed on the image). In this case, the maximum difference between gray levels in the background (as well as within the object) is about 10, and the minimum difference between the background and the edges is about 50. In this case, therefore, the MinVariation parameter should be set to a value somewhere between 10 and 50, for example, 30. Note that, if it is set above 50, the object will not be separated from the background since its extrema will not produce a new catchment basin. The default value for the MinVariation parameter is 1, which means that each extremum produces a catchment basin. Using marker images If you are able to approximate the location of your objects in an image (either through some preprocessing or through some previous knowledge of the image), you might want catchment basins determined from a separate image (known as a marker image), instead of from the extrema in the source image. Marker images are useful in preventing over-segmentation since you control not only the location of the extrema but also the number of extrema. If you use a marker image, you can set the MinVariation parameter to M_OFF or 0, since you mark off the extrema in a separate image. There are two types of marker images: non-labeled and labeled. Non-labeled marker images In a non-labeled marker image, each group of touching pixels with the value zero in the marker image, known as a marker, starts a catchment basin in the corresponding area of the source image. Specifically, each marker in the marker image forces an extremum in the corresponding area of the source image. Pixels in the marker image are considered touching if they are vertically, horizontally, or diagonally adjacent, that is, if they are \"8-connected\". Labeled marker images In a labeled marker image, a marker is a set of pixels that do not have to necessarily touch and that have all the same label value (pixel intensity). Each marker starts a catchment basin in the corresponding area of the source image and each catchment basin of the destination image is assigned the label value of the marker that generated it. Note that, in a labeled marker image, a marker can touch other markers, allowing you to specify adjacent catchment basins. Valid marker label values are 1 to 2 n - 2 (where n is the marker image buffer depth). Pixels with the label value of 2 n - 1 are considered to be part of a \"don't care\" mask and are not processed, accelerating the watershed transformation. Finally, pixels with the label value of zero are interpreted as not being part of a marker. To use a labeled marker image, you must add M_LABELED_MARKER to the ControlFlag parameter. The following is an example that performs a watershed transformation using a labeled marker image. It will separate the pills from the background. To summarize: Perform an edge detection on the original image. Create a marker image that is able to locate your objects in the image. For this example, the labeled marker image is created from the edge detected image using a series of morphology, edge detection, and blob analysis operations. In this example, the almost-white boundary surrounding all the rings is one marker. This marker serves to define the background and is placed in the corresponding area of the background of the original image. All the rings in the marker image are considered to be one marker. This particular marker serves to identify the pills. These rings are smaller than the pills in the original image and are located in the corresponding areas of the pills. Perform the watershed transformation on the edge detected image. You must specify that the catchment basins be determined from the image's minima. Remark how the resulting catchment basins have the same label value as the marker that generated them. Note that catchment basins can be determined from markers in the marker image as well as from extrema in the source image. In this case, supply a marker image to MimWatershed() and also specify the minimum grayscale variation in the source image required to produce a new catchment basin. Style of the watershed lines Watershed lines can be 8-connected or 4-connected (set the ControlFlag parameter of MimWatershed() to M_4_CONNECTED or M_8_CONNECTED). In addition, they can be traced exactly or forced to be straight (set ControlFlag to M_REGULAR or M_STRAIGHT_WATERSHED). 8-connected watershed lines consist of pixels that are horizontally, vertically, or diagonally touching. 4-connected watershed lines consist of pixels that are just horizontally and/or vertically touching. 8-connected watershed lines can separate 4-connected blobs, that is, blobs whose pixels can touch horizontally or vertically. 4-connected watershed lines are required to separate 8-connected blobs, that is, blobs whose pixels can touch horizontally, vertically, or diagonally. MIL's Blob Analysis module allows you to define blobs as either 4- or 8-connected. Note that 4-connected watershed lines can also separate 4-connected blobs, however they cause over-separation. Exact versus straight For visual purposes, watershed lines can be traced exactly or forced to be straight. Skipping the last level When you perform MimWatershed(), you can skip the last intensity level of the transformation (by setting the ControlFlag parameter to M_SKIP_LAST_LEVEL). In other words, you can prevent an extremum's zone of influence from extending beyond Lmax - 1 (for a minimum) or Lmin - 1 (for a maximum), where Lmax is the maximum gray level in the image and Lmin is the minimum gray level. In effect, this prevents the background in the image from being processed, resulting in quicker processing times. This option should be used when separating touching objects since, in this case, watershed lines in the background are unnecessary. Filling the source When you perform MimWatershed(), you can fill the catchment basins of unwanted extrema in the source image by adding M_FILL_SOURCE to the ControlFlag parameter. M_FILL_SOURCE fills the catchment basins of the unwanted extrema until the local basins become plateaus. Any subsequent watershed transformation on the source image will no longer process the unwanted extrema because they are no longer extrema. The following images illustrate how M_FILL_SOURCE transforms the source image: Watershed transformations Using watersheds to separate touching objects Using watersheds to separate objects from their background Minimum grayscale variation of a catchment basin Using marker images Non-labeled marker images Labeled marker images Style of the watershed lines Exact versus straight Skipping the last level Filling the source ",
      "wordCount": 1758,
      "subEntries": []
    },
    {
      "id": "UG_advanced-image_Connectivity_mapping",
      "version": null,
      "title": "Connectivity mapping",
      "subTitles": [
        "Using the connectivity code",
        "Locating different types of points",
        "Example"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\advanced-image\\Connectivity_mapping.htm",
      "text": " Connectivity mapping In some cases, you might need to isolate specific points of a binary, skeletonized image (for example, end points or points of intersection). You could perform several morphological processing steps, where each step is performed using a different structuring element; however, the result is a time-consuming serial operation. The connectivity (or cellular) mapping function, MimConnectMap() can reduce this serial operation into a parallel one, increasing efficiency. Specifically, treating a source image as if it were binary, MimConnectMap() concatenates the binary values of the pixels in each pixel's 3x3 neighborhood and maps this 9-bit value, referred to as a connectivity code, through a custom LUT. Assuming you program the custom LUT with the values that would result if the required structuring elements were applied to the source image consecutively, you essentially reduce the serial operation to a parallel one. Since each connectivity code has 9 bits, the process requires a LUT buffer with at least 512 (that is, 2 9 ) entries. Using the connectivity code The connectivity code gives you the possibility to explicitly choose the destination value for any 3x3 source pixel configuration (pattern), with the connectivity code being the address of a position within the custom LUT. The connectivity code is obtained by concatenating the binary values of a pixel's 3x3 neighborhood into a single 9-bit number; all non-zero pixels are treated as 1. Neighborhood pixels are linked in the following order. The pixels are connected and mapped as follows: Result = LUTMAP[Connectivity code] The pixel configuration in the neighborhood of pixel n 8 (including pixel n 8 ) forms a binary number between 0 and 511 inclusively (with n 8 being the most significant bit and n 0 being the least significant bit). This 9-bit number is then used to address the user-supplied LUT that contains the value to be written to the destination buffer. For example, the 3x3 neighborhood source configuration produces the following binary number: 111100100 (or 484). The destination buffer for this position receives the value of the user-supplied LUT at position 484. Locating different types of points There are several different types of points that connectivity mapping can locate. Isolated points. Single points that are stand alone. The connectivity code for the 3x3 neighborhood of an isolated point is 100000000. End points. Points that represent the end of a line segment. The connectivity code for the 3x3 neighborhood of two possible end points are 100001000 and 100000100. Triple points. Points of intersection which occur at the common junction of 3 line segments. The connectivity code for the 3x3 neighborhood of two possible triple points are 101001010 and 101010100. Cross points. Points of intersection which occur at the common junction of 4 line segments. The connectivity code for the 3x3 neighborhood of two possible cross points are 110101010 and 101010101. Example In the following example, the image on the left (an example of a circuit) is converted into a skeletonized image, and then the triple points and end points are located and displayed. Connectivity mapping Using the connectivity code Locating different types of points Example ",
      "wordCount": 515,
      "subEntries": []
    },
    {
      "id": "UG_advanced-image_Labeling",
      "version": null,
      "title": "Labeling",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\advanced-image\\Labeling.htm",
      "text": " Labeling You can label objects or particles (known as blobs) in an image with MimLabel(). Labeling is useful for several operations: Identifying and distinguishing blobs. Finding the area of a blob. Once a blob is labeled, you find the area by generating a histogram and noting the number of pixels associated with that label value. Counting the number of blobs in the image. The label number assigned to the last blob is also the number of blobs in the image (assuming there are fewer blobs than possible labels). Using the result to eliminate some blobs based on their label value (for example, MimClip() or MimLutMap()). The MimLabel() function numerically identifies each blob in the specified image. Each non-zero pixel within a blob is given the same numerical value, and blobs within an image are given consecutive values. You can specify that the operation is performed using one of two types of connectivity modes: M_4_CONNECTED: If two pixels touch on the vertical or horizontal, they are considered part of the same blob. M_8_CONNECTED: If two pixels touch on the vertical, horizontal, or diagonal, they are considered part of the same blob. To distinguish between touching blobs, separate the blobs by performing an erosion operation before the labeling operation. Labeling ",
      "wordCount": 210,
      "subEntries": []
    },
    {
      "id": "UG_advanced-image_Polartorectangular_and_rectangulartopolar_transform",
      "version": null,
      "title": "Polar-to-rectangular and rectangular-to-polar transforms",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\advanced-image\\Polartorectangular_and_rectangulartopolar_transform.htm",
      "text": " Polar-to-rectangular and rectangular-to-polar transforms In a rectangular coordinate system, features or information which are curved can be awkward to read, interpret, or analyze. Transforming such features into a polar coordinate system, where the vertical and horizontal axes represent the radius and the angle respectively, can make it more intuitive or easier to read. With MIL, you can perform rectangular-to-polar or polar-to-rectangular transforms, using the MimPolarTransform() function. For example, the text on the following image of a disk is easier to read after a rectangular-to-polar transform. The borders of the zone of interest are defined by specifying the center, the start and end radius, and the start and end angle in a source buffer. The function scans the specified zone from the start angle to the end angle. The dotted lines define the borders of the zone of interest: The result will be mapped to the destination buffer as shown below: In our example, since the start angle is less than the end angle, the direction of the scan is counter clockwise. The increment in angle is determined by the length, in pixels, of the outside arc, calculated as follows: The valid range of angles is between -360.0 to 720.0°; there is no maximum span limit. These values are then mapped to a destination buffer. A polar-to-rectangular transform performs the reverse of the transform described above. It takes a source buffer and maps it to a destination buffer. The center, start angle, end angle, start radius, and end radius parameters are used to specify the position of the contents of the source buffer in the destination buffer. Polar-to-rectangular and rectangular-to-polar transforms ",
      "wordCount": 272,
      "subEntries": []
    },
    {
      "id": "UG_advanced-image_Warping",
      "version": null,
      "title": "Warping",
      "subTitles": [
        "First-order polynomial warpings",
        "Perspective polynomial warpings",
        "Polar-to-rectangular, rectangular-to-polar, and custom warpings",
        "Interpolation modes",
        "Points outside the source buffer",
        "Transforming coordinate lists",
        "Warping example"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\advanced-image\\Warping.htm",
      "text": " Warping In addition to functions which perform specific geometric transforms (MimFlip(), MimResize(), MimRotate(), MimTranslate(), and MimPolarTransform()), MIL includes a more general geometric function, MimWarp(). It can perform any of the specific transforms, as well as complex warpings. Specifically, it can perform: first-order polynomial warpings, perspective polynomial warpings, polar-to-rectangular (and vice versa) warpings, and custom warpings. Note that the functions which perform specific transforms are faster than MimWarp(). You should only use MimWarp() when the required transform cannot be otherwise performed. In addition, geometric distortions can also be resolved using the Camera Calibration module. For more information, see Chapter 28: Calibrating your camera setup. MimWarp() performs a warping by first associating each pixel position of the destination buffer, (xd, yd), with a specific point (not necessarily a pixel) in the source buffer, (xs, ys). The pixel value at (xd, yd) is then determined from an interpolation around its associated source point. Destination pixels can be associated with source points in two different ways: Using a 3x3 coefficients matrix that is used as follows: Using LUTs (an X-LUT and a Y-LUT) that are the same size as the destination image and are used as follows: X s = LUTX[X d, Y d] Y s = LUTY[X d, Y d] If you intend on performing the same warping operation on multiple images (which would require using the same LUTs), it might be faster to generate the LUTs for the operation once and repeatedly pass them to MimWarp(). However, if different warpings are required, or only one image is to be processed, it is faster to generate the coefficients and call MimWarp() than it is to generate the LUTs and then call MimWarp(). First-order polynomial warpings A first-order polynomial warping is equivalent to a combination of a linear translatation, rotation, resizing, and/or shearing of an image. First-order polynomial warpings are performed by associating points in the source buffer with pixels in the destination buffer according to the following equations: X s = a0 X d + a1 Y d + a2 Y s = b0 X d + b1 Y d + b2 To perform a first-order polynomial warping using a 3x3 coefficients matrix, you must pass MimWarp() the identifier of a single-band, 32-bit floating-point, 3x3 M_ARRAY buffer filled with the coefficients to MimWarp(). The elements of the last row of the coefficient matrix should be [0, 0, 1]. You can also specify a 3x2 M_ARRAY buffer filled with the coefficients, and the elements of the last row are assumed to be [0, 0, 1]. The coefficients of this matrix can be: Generated automatically using MgenWarpParameter(). Generated using the basic transformation functions MimResize(), MimRotate(), and MimTranslate(). User-established. When using MgenWarpParameter(), specify how to perform the warping (for example, specify by how much to rotate and resize an image); the function then generates the coefficients required to produce such a warping. To combine coefficients, use separate calls to MgenWarpParameter(). For example, to generate coefficients for a rotation and translation, call MgenWarpParameter() twice, using the output buffer of the first call as the input buffer of the second call. To create the LUTs using MgenWarpParameter(), you can either pass the same information as required to generate the coefficients or you can pass the 3x3 coefficients matrix itself. When using the basic transformation functions to generate coefficients, specify the source image buffer as M_NULL, and provide transformation details. You must also provide a destination buffer that has an M_ARRAY attribute and that has dimensions of 3x3 to store the generated coefficients. For more information, see the individual reference page for each function. Perspective polynomial warpings A perspective polynomial warping is used to map an arbitrary quadrilateral onto a rectangle or to map a rectangle onto an arbitrary quadrilateral. Perspective polynomial warpings are performed by associating points in the source buffer with pixels in the destination buffer according to the following equations: X s = (a0 X d + a1 Y d + a2)/(c0 X d + c1 Y d + c2) Y s = (b0 X d + b1 Y d + b2)/(c0 X d + c1 Y d + c2) To perform a perspective polynomial warping using a 3x3 coefficients matrix, you must pass MimWarp() the identifier of a single-band, 32-bit floating-point 3x3 M_ARRAY buffer filled with the coefficients. The coefficients of this matrix can be generated automatically using MgenWarpParameter(), or can be user established. When using MgenWarpParameter(), you must specify the coordinates of the four corners of the quadrilateral or the two opposite corners of the rectangle. The coordinates are illustrated in the image below. Polar-to-rectangular, rectangular-to-polar, and custom warpings In addition to first-order polynomial and perspective polynomial warpings, it is possible to perform more complex warping operations using MimWarp(), if used with two 2D LUTs. In this case, the LUTs map destination pixels (Xd, Yd ) to specified points (Xs , Ys ) in the source image buffer as follows: Xs is determined from (Xd, Yd ) through one LUT (X-LUT) and Ys is determined from (Xd, Yd ) through another LUT (Y-LUT). Both LUTs should have the same X and Y-size as the destination image. You can generate these LUTs using MimPolarTransform(), to warp from the polar to the rectangular coordinate system (or vice versa). Alternatively, you can fill the LUTs with a custom transformation generated from another source. Once the X-LUT and Y-LUT are loaded with your values, you can pass them to MimWarp() to perform the warpings. A polar-to-rectangular warping takes a polar image as is shown in the left-most image, and transforms it to a rectangular image, as shown in the right-most image. See the Polar-to-rectangular and rectangular-to-polar transforms section earlier in this chapter for more information. To perform polar-to-rectangular or rectangular-to-polar transformations using MimWarp(), first call MimPolarTransform() with the required information to perform the transformation. However, instead of specifying a source and destination image, specify a 2D LUT buffer (M_LUT) to store the X-coordinate mapping, and a 2D LUT buffer to store the Y-coordinate mapping. Then, call MimWarp() with the LUTs and the source and destination image buffers. The LUT buffers that you specify must be the same size as the destination image. To perform a custom warping using MimWarp(), you must fill two 2D LUTs (M_LUT) representing the X and Y-coordinates, using MbufPut2d(). Every position in the X-LUT (X, Y) specifies the X-coordinates (Xs ) of the point in the source image every position in the Y-LUT (X, Y) specifies the Y-coordinates (Ys ) in the source image. This type of warping arbitrarily maps the pixel position of the destination buffer to a specific point in the source buffer, based on what was passed to the LUTs in MbufPut2d(). You can specify sub-pixel coordinates for the source point. Interpolation modes When you perform a warping, each pixel position in the destination buffer (Xd, Yd), gets associated with a specific point in the source buffer (Xs, Ys), and a computed intensity value for (Xs, Ys) is then copied to (Xd, Yd). The destination coordinates have integer values but the source coordinates, in general, do not. Therefore, the pixel value at (Xd, Yd) is determined from several source pixels that are near (Xs, Ys), according to a specified interpolation mode. The various interpolation modes available when using MimWarp() are: A standard nearest neighbor interpolation (M_NEAREST_NEIGHBOR). A standard bilinear interpolation (M_BILINEAR). A standard bicubic interpolation (M_BICUBIC). In general, nearest-neighbor interpolation is the fastest to perform, and bicubic interpolation is the slowest. However, nearest-neighbor interpolation produces the least accurate results, and bicubic interpolation produces the most accurate. Bilinear interpolation is often the best compromise between speed and accuracy. For more information on interpolation modes, see the Interpolation modes subsection of the Basic geometric transforms section of Chapter 3: Fundamental image processing. Points outside the source buffer Sometimes, the point associated with a destination pixel will fall outside the source buffer. In such cases, the new value for the destination pixel can be determined in one of the following ways: You can use pixels from the source buffer's ancestor buffer. If the source buffer is not a child buffer or if the point falls outside the ancestor buffer, the destination pixel will be left as is. You can just leave the destination pixel as is. You can set the destination pixel to 0. In general, you should use pixels from the source buffer's ancestor buffer when the source buffer is a child buffer. This will ensure that the pixels you use are related to the source buffer. If the source buffer is not a child buffer, use one of the other options. Note that you can set the destination pixels that correspond to values outside the source buffer to a fixed value other than 0 by first clearing the destination buffer to that value. Transforming coordinate lists If you only want to establish the location in the source image to which destination positions are mapped, you can use MimWarpList() to transform a list of coordinates (points) using a specified warping matrix. By default, MimWarpList() transforms the coordinates using the specified warping as is. This is referred to as a reverse warping transformation (M_REVERSE) because this is how the MimWarp() function associates each pixel position of the destination buffer with a specific point in the source buffer. You can also have MimWarpList() use the inverse of the specified matrix (M_FORWARD); this is referred to as a forward warping transformation. This type of transformation is useful if you need to determine if a specific pixel in the source image is actually mapped to a destination buffer pixel. You can only pass MimWarpList() a coefficient matrix; this allows for either first-order polynomial warpings or perspective polynomial warpings. Regardless of whether performing an M_REVERSE or an M_FORWARD transformation, pass the points to transform in an array to the SrcCoordXArrayPtr and SrcCoordYArrayPtr parameters. This is because the operation is the same; only the coefficient matrix is affected. Warping example A warping example, MimWarp.cpp, can be found in your MIL examples directory. mimwarp.cpp Warping First-order polynomial warpings Perspective polynomial warpings Polar-to-rectangular, rectangular-to-polar, and custom warpings Interpolation modes Points outside the source buffer Transforming coordinate lists Warping example ",
      "wordCount": 1690,
      "subEntries": []
    },
    {
      "id": "UG_advanced-image_Finding_Optimal_Image_Orientations",
      "version": null,
      "title": "Finding dominant image orientations",
      "subTitles": [
        "Customizing your find orientation image processing context",
        "Using the find orientation image processing results",
        "An example"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\advanced-image\\Finding_Optimal_Image_Orientations.htm",
      "text": " Finding dominant image orientations An image's orientation can be affected by a wide variety of circumstances such as misalignment of the camera, robot movements, or objects landing in various positions on a conveyor belt. Using the consistent spatial patterns in an image, MIL can find the dominant orientations (best viewing angles). Finding the dominant orientations of your image, and then rotating the image accordingly, can greatly improve the robustness of your applications, since some modules have limited angle support, such as the MIL Code and OCR modules. You can find the dominant orientations of your images using MimFindOrientation(). You can find multiple orientations to check that the most dominant orientation of the image, as calculated by MIL, is the best orientation for your particular application. The above images demonstrate one possible use of the find orientation operation. On the left is an image of some rotated text with an arrow in the direction of the most dominant orientation drawn onto the image. The image on the right shows the rotated image with the text at the right orientation. The orientations are stored in a find orientation image processing result buffer, which you must allocate using MimAllocResult() with M_FIND_ORIENTATION_LIST. Customizing your find orientation image processing context The find orientation operation requires that your image has dimensions (X-size and Y-size) that are a power of two. If the image buffer's dimensions do not meet this requirement, MimFindOrientation() determines the orientation of a clipped or resized version of the source image, but does not alter the original source image. If you set MimControl() with M_MODE to M_CLIP_CENTER, the find orientation operation will find the best orientations of the largest centered portion of the image with dimensions that are a power of two. Otherwise, the function will perform the operation on either a subsampled version of the image (set using MimControl() with M_RESIZE_DOWN) or a zoomed version of the image (set using MimControl() with M_RESIZE_UP). Using MimControl() with M_INTERPOLATION_MODE, you can specify how to interpolate during the resizing. If speed and precision are an issue, you can try using the find orientation operation on a smaller section of the image. Larger images have more information, and will take longer to process. For more information on resizing, see the Basic geometric transforms section of Chapter 3: Fundamental image processing. The MimFindOrientation() function indirectly relies on straight edges to find the orientations; so your image should ideally have objects that have well-defined, linear edges. For example, images of bar codes and text typically have better edges than images of rounded objects (for example, gaskets and washers). If your image does not have well-defined edges due to focus or resolution problems, performing a sharpening operation on the image can resolve the problem in some cases. The find orientation operation looks for consistent spatial patterns, and can filter out unwanted frequencies for a more robust operation. The operation excludes frequencies as a percentage of the maximum possible frequency in the image, which is dictated by the size of your image. Larger images will have higher maximum possible frequencies. The default setting is to disregard frequencies with a value below 5% of the maximum possible frequency in the image; however, if the default settings picks up artifacts (for example, shadows), and as such, are not optimally configured for your application, you can set the low frequency cutoff value using MimControl() with M_FREQUENCY_CUTOFF_RATIO_LOW. Additionally, the high frequencies in the image usually delineate edges, which are useful for the operation, but very high frequencies might be noise, and therefore detrimental to the precision of the operation. Having a well-tuned high frequency cutoff can reduce noise in your image and can enhance the accuracy of the find orientation operation. The default setting is to disregard frequencies with a value above 95% of the maximum possible frequency in the image. To set this high frequency cutoff as a percentage of the maximum frequency in your image, use MimControl() with M_FREQUENCY_CUTOFF_RATIO_HIGH. The majority of applications will return meaningful results with the default values, and they should only be changed for advanced applications. If your image does not contain prominent, sharp edges (for example, an image of a cell with faint membranes), MimFindOrientation() might return better results with both frequency cutoffs lowered. Like frequencies that are too high or too low, the borders of the image can potentially pose a problem and obfuscate the results. You can use M_BORDER_ATTENUATION to specify whether MIL can ignore the borders of the image when performing the find orientation operation. Using the find orientation image processing results MimFindOrientation() will find the N most dominant image orientations of your image, where N is the number of entries with which you have allocated your find orientation image processing result buffer. You can either draw the found orientations for a visual representation, or retrieve the image orientation results for further use. The former can help you determine which orientation is the best for your specific application. To get a visual representation of the orientations, you can draw arrows pointing in the directions of the orientations using MimDraw() with M_DRAW_IMAGE_ORIENTATION. You can draw one or multiple orientations at a time; each arrow intersects the center of the image. The arrows are drawn at the angle of the orientation that they represent, and their size is proportional to their score; the longest arrow has the highest score. The image below shows the arrows drawn by MimDraw() with the scores (retrieved using MimGetResult1d()) labeled for convenience. The dominant orientations are angle values that you can use to rotate your image to achieve the required orientation. Use MimGetResult1d() with M_ANGLE to retrieve the angle values of each orientation. For information on how to rotate your image using the angles found using the find orientation operation, see the Basic geometric transforms section of Chapter 3: Fundamental image processing. The results are automatically sorted and stored in descending order, with the best image orientation always in the first position, at index 0. You can use MimGetResult1d() with M_SCORE to establish the score of each orientation. The results are scored as a percentage, and are relative to each other. For example, the first orientation always has a score of 100, and MIL considers it to be the best orientation based on the given information. Other orientations will have lower scores. An example The following is an example of a find orientation operation used on a variety of different images. findimageorientation.cpp Finding dominant image orientations Customizing your find orientation image processing context Using the find orientation image processing results An example ",
      "wordCount": 1093,
      "subEntries": []
    },
    {
      "id": "UG_advanced-image_Finding_a_match",
      "version": null,
      "title": "Establishing a correlation",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\advanced-image\\Finding_a_match.htm",
      "text": " Establishing a correlation One way to find a model image in your source image is to use MimMatch(). This function establishes the correlation at every pixel in the target area of the source image between the pixel's neighborhood and a model image. The MimMatch() operation results in an image of correlation data. The closer the resulting pixel value is to the specified maximum possible score, the closer the match between the pixel's neighborhood in the target area and the model image. Other MIL match operations (such as, a pattern match using MpatFind() or MimMorphic()) don't produce the same types of results and don't necessarily use the same type of underlying correlation algorithm. To store process settings, MimMatch() uses a match image processing context. You must set up the match image processing context with a model image, with which to perform the match, using MimControl() with M_MODEL_IMAGE. All other settings within the match image processing context are optional. By default a grayscale correlation is performed, using the entire model image with the maximum (match) score determined by the destination buffer type. There are three different types of computation (modes) that can be performed when trying to match a model image to a source image neighborhood. These modes are set using MimControl() with M_MODE. The modes are: Absolute sum of the differences. Calculates the correlation using the absolute sum of the difference between the source image and the model image, within the neighborhood. Grayscale correlation. Calculates the correlation using a grayscale correlation. Normalized correlation. Calculates the correlation using a normalized grayscale correlation. When using the normalized grayscale correlation mode, you can specify how to compute the final score, using MimControl() with M_SCORE_TYPE. The options are: To perform a normalized grayscale correlation. To perform a square of the normalized grayscale correlation. To perform one of the above and then clip the resulting value so that values less than 0 are set to 0. Before calling MimMatch(), set up a match image processing context by performing the following: Allocate a match image processing context, using MimAlloc() with M_MATCH_CONTEXT. Specify the MIL image buffer containing the model image, using MimControl() with M_MODEL_IMAGE. Optionally, to reduce the size of the model image matched against the neighborhood of each pixel in the source image, specify a mask, using MimControl() with M_MASK_IMAGE. The part of the model that should be used in the match should contain zero values. All non-zero values are ignored. Optionally, set the type of computation to perform, using MimControl() with M_MODE (for example, correlation mode). Optionally, if using normalized grayscale correlation mode, specify whether to square or clip the final correlation score, using MimControl() with M_SCORE_TYPE. Optionally, specify whether to examine every pixel or every other pixel in the model image when matching, using MimControl() with M_MODEL_STEP. The former is slower but offers a more complete correlation. Optionally, if using the normalized grayscale correlation mode, specify the maximum final correlation score, using MimControl() with M_MAX_SCORE. Preprocess the context by calling MimMatch() with M_PREPROCESS. Both the source and/or destination image buffers can be set to M_NULL. If, however, the source or destination image buffer is provided, it should be a typical source or destination image buffer, respectively, and it will be used by the preprocess operation to better optimize future calls. If the preprocess operation is not done explicitly, it will be done when MimMatch() is first called. Establishing a correlation ",
      "wordCount": 568,
      "subEntries": []
    }
  ]
}]
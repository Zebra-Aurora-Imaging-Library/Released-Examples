[{
  "id": "UG_Grabbing_from_3D_sensors",
  "version": "2024020714",
  "title": "Grabbing from 3D sensors",
  "subTitles": null,
  "location": "MIL UG P06: 3D related information",
  "pageURL": "content\\UserGuide\\Grabbing_from_3D_sensors\\ChapterInformation.htm",
  "text": " Chapter 42: Grabbing from 3D sensors This chapter describes how to grab from 3D sensors. Grabbing from 3D sensors overview Steps to grab from compliant 3D sensors Basic concepts for grabbing from 3D sensors Working with compliant cameras Displaying grabbed 3D data Processing grabbed 3D data Working with non-compliant cameras Grabbing data from non-compliant 3D sensors Formatting grabbed data for use with MbufConvert3d Layout of data in a component Range Disparity Confidence Intensity Reflectance Normals Examples Aligning or flipping the coordinate system Changing the Anchor coordinate system of a GenIcam-compliant 3D sensor Inverting or flipping the Z-axis Aligning the working coordinate system with a known location or orientation Applying the transformations to your point cloud Correcting for 3D profile sensor placement Steps to correct for 3D profile sensor placement Types of distortions Pitch Scale Yaw Example of correcting the placement of your 3D profile sensor setup Alignment object constraints and requirements Alignment pyramid constraints and requirements Recommended alignment pyramid dimensions Alignment disk constraints and requirements Recommended alignment disk dimensions Hand-eye calibration Eye-in-hand calibration Eye-to-hand calibration Transformation matrices Steps to perform hand-eye calibration ",
  "wordCount": 184,
  "subEntries": [
    {
      "id": "UG_Grabbing_from_3D_sensors_Grabbing_from_3D_sensors_overview",
      "version": null,
      "title": "Grabbing from 3D sensors overview",
      "subTitles": null,
      "location": "MIL UG P06: 3D related information",
      "pageURL": "content\\UserGuide\\Grabbing_from_3D_sensors\\Grabbing_from_3D_sensors_overview.htm",
      "text": " Grabbing from 3D sensors overview You can grab 3D data from a 3D sensor using functions from the Mdig...() module. In most respects, grabbing 3D data from a 3D sensor is similar to grabbing images from a standard camera (as described in Chapter 27: Grabbing with your digitizer). However, to work with 3D data in MIL, you need a container. Typically, if your 3D sensor transmits 3D data in a format defined by an industry standard (such as GigE Vision or GenICam), you can grab directly into a container (previously allocated using MbufAllocContainer()). For more information, see the Working with compliant cameras section later in this chapter. For 3D sensors that do not transmit data suitable for grabbing into a container, you must grab that data into a buffer (or grab the data using a third-party SDK and create a buffer on its memory), and manually prepare it for use with MIL by putting the data in the components of a container. For more information, see the Working with non-compliant cameras section later in this chapter. In most cases, you will need to convert grabbed 3D data using MbufConvert3d() before using it with other MIL functions. For more information, see the Preparing a container for display or processing section of Chapter 41: 3D Containers. Grabbing from 3D sensors overview ",
      "wordCount": 221,
      "subEntries": []
    },
    {
      "id": "UG_Grabbing_from_3D_sensors_Steps_to_grab_from_multipart_or_GenDC_compliant_3D_sensors",
      "version": null,
      "title": "Steps to grab from compliant 3D sensors",
      "subTitles": null,
      "location": "MIL UG P06: 3D related information",
      "pageURL": "content\\UserGuide\\Grabbing_from_3D_sensors\\Steps_to_grab_from_multipart_or_GenDC_compliant_3D_sensors.htm",
      "text": " Steps to grab from compliant 3D sensors The following steps provide a typical methodology to grab into a container from 3D sensors that transmit 3D data in a format defined by an industry standard (such as GigE Vision or GenICam). You can determine whether your 3D sensor transmits data in a format suitable for grabbing directly into a container using MdigInquire() with M_TARGET_BUFFER_OBJECT. Configure your 3D sensor to transmit the range or disparity component and associated confidence information (either transmitted in a confidence component or in the range/disparity component using an invalid data value). MIL provides the following methods to perform this configuration: Use Matrox Capture Works to configure your 3D sensor at runtime. Use the Feature Browser to configure your 3D sensor at runtime. You can launch the Feature Browser using MdigControl() with M_GC_FEATURE_BROWSER (after allocating your digitizer in step 2). Use MdigControlFeature() to configure your 3D sensor from within your application (after allocating your digitizer in step 2). Optionally, you can preconfigure a GenICam User Set on your device and use this function to select that User Set (with the feature name \"UserSetSelector\") and load it (with the feature name \"UserSetLoad\"). Use Matrox Intellicam to create a DCF for your device. Allocate a digitizer for your 3D sensor using MdigAlloc() (specifying the appropriate DCF file if you created one). Allocate a container with the M_GRAB attribute using MbufAllocContainer(). To display the grabbed data directly using this container, you must also specify the M_DISP attribute. Grab into the container using MdigGrab(), MdigProcess(), or MdigGrabContinuous(). All components of the container are freed and components appropriate to store the grabbed data are allocated automatically. These components will be reused for subsequent grabs, unless the 3D sensor transmits components that cannot be stored in the automatically allocated components (for example, because the components have a different size for each grab). If your 3D sensor transmitted multiple sets of components (for example, 2 range components and 2 associated confidence components), allocate a child container to access a single set of components (for example, only the components associated with group 1) using MbufChildContainer(). If required, convert the container to a format that is M_3D_PROCESSABLE or M_3D_DISPLAYABLE using MbufConvert3d(). Free all your allocated objects using MdigFree(), unless M_UNIQUE_ID was specified during allocation. Steps to grab from compliant 3D sensors ",
      "wordCount": 386,
      "subEntries": []
    },
    {
      "id": "UG_Grabbing_from_3D_sensors_Basic_concepts_for_grabbing_from_3D_sensors",
      "version": null,
      "title": "Basic concepts for grabbing from 3D sensors",
      "subTitles": null,
      "location": "MIL UG P06: 3D related information",
      "pageURL": "content\\UserGuide\\Grabbing_from_3D_sensors\\Basic_concepts_for_grabbing_from_3D_sensors.htm",
      "text": " Basic concepts for grabbing from 3D sensors The basic concepts and vocabulary conventions when dealing with 3D sensors are: 3D sensor. A device that transmits 3D information. Compliant 3D sensor. A 3D sensor that transmits 3D data in a format compliant with an industry standard (such as GigE Vision or GenICam), suitable for grabbing into a container. Non-compliant 3D sensor. A 3D sensor that transmits 3D data in a non-standard format. Some of these devices use an image streaming protocol supported by MIL (such as Camera Link) to transmit 3D data; the format is typically described in the device's manual. Others transmit 3D data that must be grabbed using a third-party SDK supplied by the device manufacturer. Range component. A component with the component type M_COMPONENT_RANGE. A range component stores 3D coordinates or a depth map. Disparity component. A component with the component type M_COMPONENT_DISPARITY. A disparity component store a disparity map, typically grabbed from a stereoscopic camera. Depth map. An image where the gray value of a pixel represents its depth in the world. Disparity map. A monochrome 2D image that stores the difference (measured in pixels) between the apparent positions of objects, as seen by each image sensor of a stereoscopic camera. This information can be used to determine the distance of each object from the camera, and therefore constitutes 3D information. Confidence information. Information about the accuracy of 3D data. MIL treats any point of a point cloud or pixel of a depth map or disparity map that is associated with 0 percent confidence as invalid data. Laser profiler. A 3D sensor that generates 3D data by measuring the displacement of a laser line projected onto an object of interest. Typically, the object is on a conveyor belt and the profiler takes multiple slices before compiling them into a single point cloud. Stereoscopic camera. A 3D sensor that generates 3D data by capturing images from 2 image sensors simultaneously and comparing the positions of objects in the two images. This is the same principle that human vision uses to obtain 3D information. Structured light camera. A 3D sensor that generates 3D data by analyzing images taken while projecting several calibrated light patterns. Time-of-flight sensor. A 3D sensor that generates 3D data by emitting light with a specific wavelength and measuring (for each pixel) the time taken for that light to bounce off a surface and hit the image sensor. Basic concepts for grabbing from 3D sensors ",
      "wordCount": 411,
      "subEntries": []
    },
    {
      "id": "UG_Grabbing_from_3D_sensors_Working_with_compliant_cameras",
      "version": null,
      "title": "Working with compliant cameras",
      "subTitles": [
        "Displaying grabbed 3D data",
        "Processing grabbed 3D data"
      ],
      "location": "MIL UG P06: 3D related information",
      "pageURL": "content\\UserGuide\\Grabbing_from_3D_sensors\\Working_with_compliant_cameras.htm",
      "text": " Working with compliant cameras If your 3D sensor transmit 3D data in a format defined by an industry standard (such as GigE Vision or GenICam), grab this data into a container with the M_GRAB attribute (previously allocated using MbufAllocContainer()). Typically, you can convert the data directly to a format that is 3D-processable and/or 3D-displayable using MbufConvert3d(). You can determine whether your 3D sensor transmits data in a format suitable for grabbing into a container using MdigInquire() with M_TARGET_BUFFER_OBJECT. You can perform a single grab into a container using MdigGrab(), or you can continuously grab and process the grabbed data, without dropping frames, using MdigProcess(). You can also use MdigGrabContinuous() to continuously acquire frames of data for use with a 2D or 3D display. Displaying grabbed 3D data If you grabbed into a container allocated with the M_DISP attribute, you can display 3D data immediately after it is grabbed by selecting the container to a 3D display (using M3ddispSelect()). If the container is not 3D-displayable, but could be converted to a 3D-displayable container using MbufConvert3d(), MIL will internally convert the 3D data. Typically, you will need to convert the grabbed data to a 3D-processable container using MbufConvert3d(); in this case, it is more efficient to convert the data to a container that is also3D-displayable, and select that container to the 3D display instead of the grab container. Note that you can select an empty container to a 3D display. When you grab 3D data into the empty container, it will be shown in the 3D display immediately. Processing grabbed 3D data Typically, you must convert the grabbed data for processing using MbufConvert3d() before you can use it with MIL 3D-processing functions. For more information, see the Preparing a container for display or processing section of Chapter 41: 3D Containers. Grabbed 3D data can be used with MIL 3D-processing functions immediately if the following conditions are all met: The 3D sensor is configured to transmit data in a format that is 3D-processable. The grab container was allocated with both the M_GRAB and M_PROC attributes. The 3D settings (such as shear, offset, and scale) of the range component are set to their default values (except for M_3D_DISTANCE_UNIT). You can find these settings in the table For specifying settings useful with components that store 3D data. Typically, transmitted components have non-default values for 3D settings; MIL applies these settings to the underlying data when the container is passed as a source to MbufConvert3d(). For example if M_3D_SCALE_X is set to 1.5, MbufConvert3d() scales the coordinates by 1.5 in the destination and sets M_3D_SCALE_X to the default value of 1 in the destination container. You can inquire whether a container is 3D-processable using MbufInquireContainer() with M_3D_PROCESSABLE. You can still apply 3D settings (such as shear, offset, or scale) to a 3D-processable container by setting them for the range component (using MbufControlContainer()) and passing the container as a source to MbufConvert3d(). Working with compliant cameras Displaying grabbed 3D data Processing grabbed 3D data ",
      "wordCount": 499,
      "subEntries": []
    },
    {
      "id": "UG_Grabbing_from_3D_sensors_Working_with_noncompliant_cameras",
      "version": null,
      "title": "Working with non-compliant cameras",
      "subTitles": [
        "Grabbing data from non-compliant 3D sensors",
        "Formatting grabbed data for use with MbufConvert3d",
        "Layout of data in a component",
        "Range",
        "Disparity",
        "Confidence",
        "Intensity",
        "Reflectance",
        "Normals",
        "Examples"
      ],
      "location": "MIL UG P06: 3D related information",
      "pageURL": "content\\UserGuide\\Grabbing_from_3D_sensors\\Working_with_noncompliant_cameras.htm",
      "text": " Working with non-compliant cameras If your 3D sensor transmits 3D data in a format defined by an industry standard (such as GigE Vision or GenICam), typically the data will already be correctly formatted for you to use it with MIL. Otherwise, your 3D sensor is non-compliant; the data it transmits is not natively compatible with MIL 3D functions, and you must refer to your 3D sensor manual to determine how 3D information is stored in the transmitted data. You will need to manually prepare this data for use with MIL (possibly using a third-party SDK provided by the device manufacturer). You will also need to set the correct component type and 3D settings for the grabbed data. Grabbing data from non-compliant 3D sensors If your non-compliant 3D sensor transmits 3D data using a protocol that MIL can grab (for example, as a standard image stream over GigE Vision), and each frame of data contains only a single type of information (such as point cloud coordinates or a disparity map), you should grab the data directly into a component of a container. You can do this by passing the MIL identifier of the component (previously allocated using MbufAllocComponent()) to MdigGrab() or MdigProcess(). Note that the component must be in a container with the M_GRAB attribute. If your non-compliant 3D sensor transmits 3D data using a protocol that MIL can grab, and each frame of data contains multiple types of information (such as coordinates, confidence, and intensity), you should grab the data into a standard image buffer. Refer to your camera manual to determine where, and in what format, each type of 3D data is stored in the image. Then, use MbufCreateComponent() to create components for a container with the correct attributes and component type and map them on to these areas in memory. If your 3D sensor requires you to grab data using a third-party SDK, you need put the grabbed data in a component. Typically, you should do this by copying the values grabbed using the third-party SDK into one or more arrays. You can then pass these arrays, and the MIL identifier of an image buffer component that has the appropriate component type (previously allocated using MbufAllocComponent()) to MbufPut() or MbufPutColor(). If your 3D sensor transmits more than one type of data, you can use this procedure with multiple components to put all of that data in the same container. If the third-party SDK provides the ability to export 3D data in either the PLY or STL file format, you can load the 3D data from that file into a container using MbufLoad(), MbufRestore(), or MbufImport(). This is useful when performance is not critical (for example, early in application development). Note that point cloud data loaded from a PLY or STL file is typically unorganized. For information on point cloud organization, see the Organized and unorganized point clouds subsection of the Working with points in a point cloud section of Chapter 35: 3D image processing. When you are working with data grabbed using a third-party SDK, if you require maximum performance and know the exact layout of the data in memory, you can map a component onto that memory using MbufCreateComponent() (if the layout of the data is compatible with MIL). To do this, you must determine the Host address, format, and pitch of the grabbed data. If the third-party SDK continues to manipulate that space in memory after grabbing (for example, by clearing it), you should instead create a buffer on the memory using an MbufCreate...() function, and then copy that buffer to a component using MbufCopyComponent(). Formatting grabbed data for use with MbufConvert3d Typically, if you need to manually prepare the data transmitted by your 3D sensor to a format supported by MIL, you should first determine which M_3D_REPRESENTATION most closely resembles the structure of the transmitted data. You must then set the M_3D_REPRESENTATION setting of the range or disparity component to this value, and perform any additional operations required for your data to fully match this setting. To learn the exact layout of data required by MIL for each 3D representation, see the Layout of data in a component subsection of this section. You must also include confidence information, either stored in a confidence component or in the range or disparity component itself (using MbufControlContainer() to set the M_3D_INVALID_DATA_VALUE and M_3D_INVALID_DATA_FLAG settings). If your 3D sensor is supplied with a third-party SDK, you might find it easier to use that SDK to partially, or completely, format your 3D data before putting it into a component. In other cases, you should put the data in a component immediately and manipulate it using MIL functions (such as MimArith()). The best strategy will depend upon the format in which the data is transmitted, and the functionality provided by the third-party SDK. Once the grabbed 3D data is stored in a container and correctly formatted for use by MIL, you should convert the container to a format that is 3d-displayable and/or 3d-processable using MbufConvert3d(). Layout of data in a component MIL expects the data in a component to be laid out in a specific way depending on its component type. Many component types are also related to one another. For example, in a container, the same position in the range component and confidence component should describe information about the same point; therefore, the data in these components must be aligned. For this reason, all 3D components in a 3D container must have the same M_SIZE_X and M_SIZE_Y (aside from the mesh component, which you should typically not access directly). In a point cloud container, no one component stores the points; for example, the range component stores the coordinates of each point, the confidence component stores the confidence of each point, and the intensity component stores the color of each point. MIL can only process 3D information stored in a container that is M_3D_PROCESSABLE, even if the data in your components is arranged correctly. For more information, see the Preparing a container for display or processing section of Chapter 41: 3D Containers. The following sections describes how data must be arranged in components you might need to manipulate directly. Range An M_COMPONENT_RANGE component is an image buffer that stores the coordinates of points, or a depth map. There are many different ways that this data can be arranged; the exact layout is identified by the component's coordinate system type and 3D representation (set using MbufControlContainer() with M_3D_COORDINATE_SYSTEM_TYPE and M_3D_REPRESENTATION respectively). MIL only supports range components with M_3D_COORDINATE_SYSTEM_TYPE set to M_CARTESIAN. For this coordinate system type, each pixel-position in the image buffer stores, at most, XYZ-coordinates for a single point. The range component's 3D representation identifies how many bands it must have, and how the XYZ-coordinates are stored in those bands. All 3D representations, with the exception of M_CALIBRATED_XYZ_UNORGANIZED, are for organized 3D data. This means it is assumed that points close to each other in 3D space are also close to each other in the pixel coordinate system. Typically, disparity components and organized range components should have M_SIZE_Y greater than 1. If your 3D sensor transmits range or disparity data for multiples lines in a single row (for example, multiple profiles from a laser profiler), you can make it organized by splitting that data at a known interval to put it in multiple rows. In most cases, using MbufConvert3d() with a container that has a range component with an organized 3D representation, but only single row of data, will not produce a correct result. Similarly, specifying an organized 3D representation for 3D data that is unorganized can lead to invalid results when the converted container is passed as a source to MIL processing functions. The following table describes the required layout for each 3D representation type: 3D representation Bands Description M_CALIBRATED_XYZ 3 X-coordinates are stored in band 0, Y-coordinates are stored in band 1, and Z-coordinates are stored in band 2. M_CALIBRATED_XYZ_UNORGANIZED 3 X-coordinates are stored in band 0, Y-coordinates are stored in band 1, and Z-coordinates are stored in band 2. The data is unorganized, meaning that it is stored in a single row, with no implied association between pixel-position and 3D position. Since the coordinates are unorganized, M_SIZE_Y must be 1. M_CALIBRATED_XZ_EXTERNAL_Y 3 X-coordinates are stored in band 0 and Z-coordinates are stored in band 2 (band 1 is not used). The Y-coordinate for each pixel row is stored in a separate 1-dimensional MIL array buffer that stores one value per row in the component. This M_ARRAY buffer should not be part of the container, and there is no component type for this buffer. For more information, see MbufConvert3d(). M_CALIBRATED_XZ_UNIFORM_Y 3 X-coordinates are stored in band 0 and Z-coordinates are stored in band 2 (band 1 is not used). Y-coordinates are stored implicitly by the point's row position in the pixel coordinate system (multiplied by M_3D_SCALE_Y). M_CALIBRATED_Z 1 Z-coordinates are stored explicitly in the buffer. There are no XY-coordinates in the 3D data stored with this setting. M_CALIBRATED_Z_EXTERNAL_Y 1 Z-coordinates are stored explicitly in the buffer. The Y-coordinate for each pixel row is stored in a separate 1-dimensional MIL array buffer that stores one value per row in the component. This M_ARRAY buffer should not be part of the container, and there is no component type for this buffer. For more information, see MbufConvert3d(). There are no X-coordinates in the 3D data stored with this setting. M_CALIBRATED_Z_UNIFORM_X_EXTERNAL_Y 1 Z-coordinates are stored explicitly in the buffer. X-coordinates are stored implicitly by the point's column position in the pixel coordinate system (multiplied by M_3D_SCALE_X). The Y-coordinate for each pixel row is stored in a separate 1-dimensional MIL array buffer that stores one value per row in the component. This M_ARRAY buffer should not be part of the container, and there is no component type for this buffer. For more information, see MbufConvert3d(). M_CALIBRATED_Z_UNIFORM_XY 1 Z-coordinates are stored explicitly in the buffer. XY-coordinates are stored implicitly by the point's column and row position in the pixel coordinate system respectively (multiplied by M_3D_SCALE_X and M_3D_SCALE_Y). This is also sometimes referred to as a depth map. M_UNCALIBRATED_Z 1 Z-coordinates are stored explicitly in the buffer. These coordinates are not natively calibrated. There are no XY-coordinates in the 3D data stored with this setting. Disparity An M_COMPONENT_DISPARITY component is a 1-band image buffer that stores a disparity map. A disparity map stores the difference in the apparent position of objects in the 2 images produced by a stereoscopic camera. Pixels with higher values indicate that the object shown is further apart in the two images, meaning that the object is closer to the camera. There are many different ways that this data can be arranged; the exact layout is identified by the component's 3D representation (set using MbufControlContainer() with M_3D_REPRESENTATION). A disparity map does not explicitly store coordinates, but you can use MbufConvert3d() to convert a container with a disparity map to a point cloud container. Depending on the 3D representation of the disparity component, the Y-coordinates in the resulting range component will be calculated differently. To correctly convert a container with a disparity component to a point cloud container, you must correctly set the disparity-specific 3D settings (using MbufControlContainer() with control type settings that start with M_3D_DISPARITY_...) for the disparity component. The correct values should be provided by your camera manufacturer. The following table describes the required layout for each 3D representation type: 3D representation Description M_DISPARITY Y-coordinates are calculated by applying a perspective projection to the image. Typically, this representation type should be used with disparity components that have been generated using areascan cameras. M_DISPARITY_EXTERNAL_Y The Y-coordinate for each pixel row is stored in a separate 1-dimensional MIL array buffer that stores one value per row in the component. This M_ARRAY buffer should not be part of the container, and there is no component type for this buffer. For more information, see MbufConvert3d(). Typically, this representation type should only be used with disparity components that have been generated using linescan cameras. M_DISPARITY_UNIFORM_Y Y-coordinates for each pixel are stored implicitly by row index in the pixel coordinate system (multiplied by M_3D_SCALE_Y). Typically, this representation type should only be used with disparity components that have been generated using linescan cameras. Confidence An M_COMPONENT_CONFIDENCE component is a 1-band image buffer that stores confidence information. When a confidence component is part of a 3D container, it stores the probability that each point (or each pixel of a disparity component) is accurate. In MIL, 3D information associated with a confidence value of 0 is considered invalid and is not used by 3D image processing or analysis functions. The confidence component must have the same M_SIZE_X and M_SIZE_Y as the range or disparity component. Confidence information can also be indicated within the range or disparity component itself. If an invalid data value is enabled and set (using MbufControlContainer() with M_3D_INVALID_DATA_FLAG and M_3D_INVALID_DATA_VALUE respectively) for the range or disparity component, any point with the Z-coordinate value (or disparity map pixel value) equivalent to the invalid data value will be treated as invalid data when the container is passed to MbufConvert3d(). Intensity An M_COMPONENT_INTENSITY component is an image buffer that stores a standard 2D image. When an intensity component is part of a 3D container, it stores color information for each point (or each pixel of a disparity component). The intensity component must have the same M_SIZE_X and M_SIZE_Y as the range or disparity component. Unlike the other component types, an intensity component is often used to store a photographic image that is not related to the range or disparity component (for example, the image from which the laser line was extracted if the 3D data was acquired using a laser profiler). In this case, the intensity component should not be used as part of a 3D container, because it does not store values for the points in the container. Reflectance An M_COMPONENT_REFLECTANCE component is an image buffer that stores a reflectance map. The exact usage of this component varies depending on the 3D sensor used to generate the data. When a reflectance component is part of a 3D container, it stores intensity information for each point (for example, the intensity of the laser line at each point if the 3D data was acquired using a laser profiler). Unlike an intensity component, a reflectance map is typically only used to store values for points, rather than a photographic image. The reflectance component must have the same M_SIZE_X and M_SIZE_Y as the range or disparity component. Normals An M_COMPONENT_NORMALS_MIL component is a 3-band floating-point image buffer that stores a unit vector indicating a direction for each point. Typically, normals indicate what direction is perpendicular to the surface of the object at each point (or, in rare cases, each pixel of a disparity component). The XYZ-components of the normal vector for each point are stored in band 0,1, and 2 of the buffer, respectively. Each normal vector must be normalized to 1, or be set to 0 in all bands (indicating that there is no normal vector stored for that point). You can use M3dimFix() with M_NORMALS_NORMALIZED to normalize the normal vectors to 1 if this condition is not met. The normals component must have the same M_SIZE_X and M_SIZE_Y as the range or disparity component. Examples The following code snippet shows how to create components on an image buffer containing 3D data, grabbed from a non-compliant 3D sensor: /* Allocate a buffer for grabbing from 3D sensor that transmits a point cloud as four 512 by 512 blocks */ /* representing X-coordinates, Y-coordinates, Z-coordinates, and reflectance respectively. */ MIL_ID GrabBuffer = MbufAlloc2d(MilSystem, 512, 2048, 16 + M_UNSIGNED, M_IMAGE + M_GRAB, M_NULL); /* Calculate the host address of each type of data in the grab buffer. */ void* AddressOfRangeX = (void*)MbufInquire(GrabBuffer, M_HOST_ADDRESS, M_NULL); void* AddressOfRangeY = (void*)(MbufInquire(GrabBuffer, M_HOST_ADDRESS, M_NULL) + MbufInquire(GrabBuffer, M_PITCH_BYTE, M_NULL) * 512); void* AddressOfRangeZ = (void*)(MbufInquire(GrabBuffer, M_HOST_ADDRESS, M_NULL) + MbufInquire(GrabBuffer, M_PITCH_BYTE, M_NULL) * 1024); void* AddressOfRangeXYZ[3] = { AddressOfRangeX, AddressOfRangeY, AddressOfRangeZ }; void* AddressOfReflectance[] = { (void*)(MbufInquire(GrabBuffer, M_HOST_ADDRESS, M_NULL) + MbufInquire(GrabBuffer, M_PITCH_BYTE, M_NULL) * 1536) }; /* Allocate a container that will have components mapped onto the same memory as the grab buffer, */ /* and a container that will store data converted for processing and display. */ MIL_ID Grab3dContainer = MbufAllocContainer(MilSystem, M_PROC, M_DEFAULT, M_NULL); MIL_ID Converted3dContainer = MbufAllocContainer(MilSystem, M_DISP + M_PROC, M_DEFAULT, M_NULL); /* Create a 3-band range component and a 1-band reflectance component */ /* mapped onto the same memory as the grab buffer. */ MIL_ID RangeComponent = MbufCreateComponent(Grab3dContainer, 3, 512, 512, 16 + M_UNSIGNED, M_IMAGE, M_HOST_ADDRESS + M_PITCH, M_DEFAULT, AddressOfRangeXYZ, M_COMPONENT_RANGE, M_NULL); MIL_ID ReflectanceComponent = MbufCreateComponent(Grab3dContainer, 1, 512, 512, 16 + M_UNSIGNED, M_IMAGE, M_HOST_ADDRESS + M_PITCH, M_DEFAULT, AddressOfReflectance, M_COMPONENT_REFLECTANCE, M_NULL); /* Set the scale settings of the created range component, using settings either provided by the */ /* camera manufacturer or required by the application. The coordinates in the grabbed data will */ /* be multiplied by the scale setting when converted using MbufConvert3D(). */ MbufControlContainer(Grab3dContainer, M_COMPONENT_RANGE, M_3D_SCALE_X, 0.13f); MbufControlContainer(Grab3dContainer, M_COMPONENT_RANGE, M_3D_SCALE_Y, 0.13f); MbufControlContainer(Grab3dContainer, M_COMPONENT_RANGE, M_3D_SCALE_Z, 0.29f); /* Enable the use of an invalid data value to indicate invalid points in the range component, */ /* and set that value to the invalid data value specified by your camera manufacturer. */ MbufControlContainer(Grab3dContainer, M_COMPONENT_RANGE, M_3D_INVALID_DATA_FLAG, M_TRUE); MbufControlContainer(Grab3dContainer, M_COMPONENT_RANGE, M_3D_INVALID_DATA_VALUE, 1984); while(Grabbing){ /* Grab new data into the grab buffer. */ MdigGrab(MILDig, GrabBuffer); /* Convert the data in the container on which you created the components to a format supported */ /* for processing and display. The converted data is stored in automatically allocated */ /* components in the destination container. */ MbufConvert3d(Grab3dContainer, Converted3dContainer, M_NULL, M_DEFAULT, M_COMPENSATE); } The following code snippet shows how to put 3D data (grabbed using a third-party SDK) in the components of a container: /* Obtain data from a third-party SDK that grabs 3D data in four 512 by 512 blocks */ /* representing X-coordinates, Y-coordinates, Z-coordinates, and reflectance respectively. */ /* Allocate a container that will store the data obtained from the third-party SDK */ /* and a container that will store data converted for processing and display. */ MIL_ID Unconverted3dContainer = MbufAllocContainer(MilSystem, M_PROC, M_DEFAULT, M_NULL); MIL_ID Converted3dContainer = MbufAllocContainer(MilSystem, M_DISP + M_PROC, M_DEFAULT, M_NULL); /* Allocate a 3-band range component and a 1-band reflectance component */ /* to store the data so that it can be converted using MbufConvert3d() */ MIL_ID RangeComponent = MbufAllocComponent(Unconverted3dContainer, 3, 512, 512, 16 + M_UNSIGNED , M_IMAGE + M_PROC, M_COMPONENT_RANGE, M_NULL); MIL_ID ReflectanceComponent = MbufAllocComponent(Unconverted3dContainer, 1, 512, 512, 16 + M_UNSIGNED , M_IMAGE + M_PROC, M_COMPONENT_INTENSITY, M_NULL); /* Allocate arrays to store the data grabbed using the third-party SDK */ unsigned short XCoordinates[512 * 512]; unsigned short YCoordinates[512 * 512]; unsigned short ZCoordinates[512 * 512]; unsigned short ReflectanceValues[512 * 512]; /* Set the scale settings of the allocated range component, using settings either provided by the */ /* camera manufacturer or required by the application. The coordinates in the data obtained from the */ /* third-party SDK will be multiplied by the scale setting when converted using MbufConvert3d(). */ MbufControlContainer(Unconverted3dContainer, M_COMPONENT_RANGE, M_3D_SCALE_X, 0.13f); MbufControlContainer(Unconverted3dContainer, M_COMPONENT_RANGE, M_3D_SCALE_Y, 0.13f); MbufControlContainer(Unconverted3dContainer, M_COMPONENT_RANGE, M_3D_SCALE_Z, 0.29f); /* Enable the use of an invalid data value to indicate invalid points in the range component. */ /* The default invalid data value is 0. You can change the invalid data value using M_3D_INVALID_DATA_VALUE. */ /* In some cases, it might be simpler to allocate a confidence component and use it to indicate invalid points */ /* in the grabbed data instead. */ MbufControlContainer(Unconverted3dContainer, M_COMPONENT_RANGE, M_3D_INVALID_DATA_FLAG, M_TRUE); /* Grab and convert the data */ while (Grabbing){ /* Using the third-party SDK, grab the data and copy the appropriate values into each array. */ /* You might need to use the third-party SDK to perform additional operations on the data (such as calibrating it) */ /* before copying it into the arrays. Ensure that invalid points have a Z-coordinate of 0, so that these values will */ /* be marked as invalid data in the converted container. */ /* Use MbufPutColor() and MbufPut() to copy the data from the arrays into the previously allocated components */ MbufPutColor(RangeComponent, M_SINGLE_BAND, 0, XCoordinates); MbufPutColor(RangeComponent, M_SINGLE_BAND, 1, YCoordinates); MbufPutColor(RangeComponent, M_SINGLE_BAND, 2, ZCoordinates); MbufPut(ReflectanceComponent, ReflectanceValues); /* Convert the data in the container in which you put the data for processing and display. */ /* The converted data is stored in automatically allocated components in the destination container. */ MbufConvert3d(Unconverted3dContainer, Converted3dContainer, M_NULL, M_DEFAULT, M_COMPENSATE); } Working with non-compliant cameras Grabbing data from non-compliant 3D sensors Formatting grabbed data for use with MbufConvert3d Layout of data in a component Range Disparity Confidence Intensity Reflectance Normals Examples ",
      "wordCount": 3435,
      "subEntries": []
    },
    {
      "id": "UG_Grabbing_from_3D_sensors_Aligning_or_flipping_the_coordinate_system",
      "version": null,
      "title": "Aligning or flipping the coordinate system",
      "subTitles": [
        "Changing the Anchor coordinate system of a GenIcam-compliant 3D sensor",
        "Inverting or flipping the Z-axis",
        "Aligning the working coordinate system with a known location or orientation",
        "Applying the transformations to your point cloud"
      ],
      "location": "MIL UG P06: 3D related information",
      "pageURL": "content\\UserGuide\\Grabbing_from_3D_sensors\\Aligning_or_flipping_the_coordinate_system.htm",
      "text": " Aligning or flipping the coordinate system Even if your 3D sensor is properly aligned, the default coordinate system of the 3D sensor (and of the resulting point cloud or depth map) might not be the most ideal for your application. This section presents strategies to align, move, or flip a point cloud's coordinate system and its axes. If you are trying to adjust for distortion or to correct for the placement of your 3D profile sensor, refer to the Correcting for 3D profile sensor placement section later in this chapter. Changing the Anchor coordinate system of a GenIcam-compliant 3D sensor The Anchor coordinate system is a Cartesian coordinate system relative to which, by default, 3D sensors return results. Some GenICam-compliant 3D sensors allow you to change the location of the Anchor coordinate system's origin and the orientation of its axes. For example, Matrox AltiZ allows you to place the Anchor coordinate system in one of three locations, or modes, as shown in the following image: ReferencePoint mode, Distance mode, and Height mode. Note that all three of the Anchor coordinate system modes supported by Matrox AltiZ respect the right-hand rule. Inverting or flipping the Z-axis If you have already acquired 3D data in the form of a point cloud from a 3D sensor, you might want to change the coordinates of the points in the point cloud. For example, if the 3D sensor's Anchor coordinate system had its Z-axis pointing from the sensor towards the scanned objects, as is the case in ReferencePoint mode and Distance mode for Matrox AltiZ, then the parts of the scanned objects that are closest to the sensor will correspond to points in the point cloud with the lowest Z-coordinates. Assuming that the 3D profile sensor is above the object to be scanned, then the tallest parts of the object will correspond to the points with the lowest Z-coordinates, which can be unintuitive or problematic for further analysis. You can invert, or flip, the Z-axis of your point cloud's working coordinate system, so that the tallest parts of the object correspond to points with the greatest Z-coordinates. To invert, or flip, the Z-axis of your point cloud's working coordinate system, you can rotate the points in the point cloud about the X-axis or Y-axis. To rotate the point cloud, use M3dimRotate() with either M_ROTATION_X or M_ROTATION_Y set to 180 degrees. Rotating the points by 180 degrees about one axis negates their coordinates along the other two axes. The working coordinate system will continue to respect the right-hand rule. Equivalently, you can calculate the transformation necessary to rotate the points in the point cloud such that the direction of the Z-axis is flipped, using M3dmapControl() with M_ALIGN_Z_DIRECTION set to M_REVERSE, and M3dmapAlignScan(). The following images depict a point cloud undergoing a rotation about the X-axis. In the leftmost image, the point cloud is originally oriented such that the top of the object corresponds to points with lower Z-coordinates than the bottom of the object. The second image shows a rotation of 180 degrees about the X-axis; note that this rotation inverts, or flips, the Y- and Z-coordinates of the points in the point cloud. In the rightmost image, the display is reoriented so that the point cloud appears right side up. The top of the point cloud now corresponds to points with higher Z-coordinates than the bottom of the point cloud; the Z-axis has effectively been flipped. You might be tempted to negate the Z-coordinates of each point in the point cloud by simply scaling the points using M3dimScale(), with ScaleZ set to -1. However, doing so mirrors the object, instead of resulting in the same object, which can lead to incorrect analysis. For more information, see the Scaling subsection of the Moving or scaling a point cloud or 3D geometry section of Chapter 35: 3D image processing. Note that even if you correct the direction of the Z-axis in the point cloud's working coordinate system, the location of the point cloud's origin might not be suitable for your application. For example, you might want points corresponding to the bottom of your scanned object to have no height (Z=0) and lie in the XY-plane. You can use M3dimTranslate() with TranslationZ to translate all points in your point cloud up or down along the Z-axis, allowing you to control where the XY-plane (Z=0) lies in your point cloud. You can also refer to the following subsection for more strategies on manipulating the position of your point cloud's working coordinate system. The following image shows the rotated point cloud from above translated up 5 units along the Z-axis. The points corresponding to the bottom of the gear now lie in the XY-plane. Aligning the working coordinate system with a known location or orientation You can align the working coordinate system of your point cloud with a known location or orientation, using M3dmapAlignScan(). This function uses the point cloud of a flat surface, an alignment pyramid (with expected proportions and a chamfer), or an alignment disk (with expected proportions and two directional holes) in the target setup to establish the transformations. For information on how to construct an appropriate alignment object, refer to the Alignment object constraints and requirements section later in this chapter. Using the point cloud of a flat surface is useful when you do not have the ability to create an alignment object (pyramid or disk). With the point cloud of a flat surface perpendicular to your 3D sensor, you can align the origin of the Z-axis (Z=0) of your point cloud's working coordinate system to a known location and/or you can change the positive direction of its axes. Prior to calling M3dmapAlignScan(), use M3dmapControl() with M_OBJECT_SHAPE set to M_FLAT_SURFACE. The following image shows an example of how you can use a flat surface object to align your point cloud's working coordinate system. Using M3dmapControl() with M_ALIGN_Z_POSITION and M3dmapAlignScan(), you can calculate the transformation necessary to align the coordinate system with the flat surface object such that the flat surface defines the XY-plane (Z=0). You can change the positive direction of one or more axes of the working coordinate system using M_ALIGN_XY_DIRECTION and M_ALIGN_Z_DIRECTION. This allows you to specify the positive direction of any axis that is best suited for your setup, depending on the location and orientation of your 3D sensor with respect to the conveyor or plane of motion. Changing the direction of one axis automatically updates the other axes to maintain the right-hand rule. The flat surface object can be any non-elastic flat surface, such as the flat conveyor belt, a flat square-like piece of sandblasted aluminum, 96% alumina ceramic, or an equivalently flat piece of medium-density fiberboard (MDF). In any case, your flat surface object must fill the 3D sensor's whole field of view in your target setup. Calculations that rely on the presence of an alignment pyramid or alignment disk will not be calculated if M_OBJECT_SHAPE is set to M_FLAT_SURFACE. In this case, M_3D_SCALE_Y, M_3D_SHEAR_X, and M_3D_SHEAR_Z will not be calculated or obtainable in M3dmapGetResult(). The matrix that corrects the shear and scale for the laser scan lines (M3dmapCopyResult() with M_SHEAR_MATRIX) will be the identity matrix, and the matrix that corrects the shear, scale, and rotations of the laser scan lines (M3dmapCopyResult() with M_TRANSFORMATION_MATRIX) will be the same as M_RIGID_MATRIX. If you do have an appropriate alignment pyramid or disk, you can control how M3dmapAlignScan() calculates the transformations necessary to align your point cloud with the point cloud of the alignment object at a known location or orientation. For example, you can specify to calculate the transformation to align the origin of the working coordinate system with the center of the alignment object in the X-direction by calling M3dmapControl() with M_ALIGN_X_POSITION set to M_OBJECT_CENTER. After calling M3dmapAlignScan(), you can then get the transformation needed to align the working coordinate system and apply the calculated transformation matrix to your point cloud. You can specify to calculate the transformation to align the origin of the working coordinate system along the Z-axis with either the top or bottom of the alignment object using M3dmapControl() with M_ALIGN_Z_DIRECTION set to M_OBJECT_TOP or M_OBJECT_BOTTOM, respectively. M3dmapAlignScan() assumes that the alignment object or flat surface object is parallel to the floor of your setup, and, in the case of a flat surface object, that the 3D sensor is perpendicular to the target setup. You cannot, for example, align your coordinate system with the point cloud of an alignment object or flat surface object that is elevated on one end and therefore is not parallel to the floor. Attempting to do so will cause your application to fail. Even if you have correctly positioned your target setup, your 3D sensor might not be perfectly perpendicular to the objects you need to scan, which results in small distortions in the resulting point clouds. To correct these distortions using an alignment pyramid or disk, refer to the Correcting for 3D profile sensor placement section later in this chapter. You can also align the working coordinate system without an alignment object or flat surface object, by fitting the background of your point cloud to a plane. For example, you can use M3dmetFit() with M_PLANE to fit the bottom of the point cloud, corresponding to the surface of the conveyor belt, to a plane. You can then copy the results of the fitting operation into a transformation matrix, using M3dmetCopyResult() with M_FIXTURING_MATRIX. This transformation matrix represents the transformation that could move the bottom of the point cloud to the XY-plane (Z=0), with its normal as the Z-axis. The centroid of all inlier points used for the fitting becomes the origin. For more information on fixturing your coordinate system in 3D, refer to the How to fixture in 3D section of Chapter 45: Fixturing in 3D. Applying the transformations to your point cloud Once the transformations to align the coordinate system are calculated, you can apply them to your 3D sensor data. To do so, use M3dimMatrixTransform() and apply the calculated transformation matrix (M3dmapCopyResult() with M_TRANSFORMATION_MATRIX) to a point cloud acquired by your 3D sensor. Aligning or flipping the coordinate system Changing the Anchor coordinate system of a GenIcam-compliant 3D sensor Inverting or flipping the Z-axis Aligning the working coordinate system with a known location or orientation Applying the transformations to your point cloud ",
      "wordCount": 1722,
      "subEntries": []
    },
    {
      "id": "UG_Grabbing_from_3D_sensors_Steps_to_calibrate_3D_profile_sensor",
      "version": null,
      "title": "Correcting for 3D profile sensor placement",
      "subTitles": [
        "Steps to correct for 3D profile sensor placement",
        "Types of distortions",
        "Pitch",
        "Scale",
        "Yaw",
        "Example of correcting the placement of your 3D profile sensor setup"
      ],
      "location": "MIL UG P06: 3D related information",
      "pageURL": "content\\UserGuide\\Grabbing_from_3D_sensors\\Steps_to_calibrate_3D_profile_sensor.htm",
      "text": " Correcting for 3D profile sensor placement Correcting for the placement of your 3D profile sensor is an important step in your setup. Ideally, your 3D profile sensor should be setup so that the direction of motion is perpendicular to the laser plane and scale in the Y-direction reflects the distance between profiles. If this is not possible, you will need to align your 3D profile sensor to account for the pitch, scale, and yaw distortions that will result from such a setup. To correct for your 3D profile sensor's placement, you can use M3dmapAlignScan() to establish the adjustments that must be made. You can then use these results to modify 3D profile sensor settings or to transform the point cloud acquired from the 3D profile sensor. M3dmapAlignScan() uses the point cloud of an alignment disk or alignment pyramid (with expected proportions) in the target setup to establish the transformations required to align the setup. Steps to correct for 3D profile sensor placement The following steps provide a basic methodology for correcting the placement of your 3D profile sensor setup: Grab a point cloud of the alignment object (alignment disk or alignment pyramid) in the target setup, using MdigGrab(). For information on the requirements for the alignment object, see the Alignment object constraints and requirements section later in this chapter. If necessary, call MbufConvert3d() to convert the grabbed point cloud if it is not 3D processable. Allocate both a 3D alignment context and 3D alignment result buffer using M3dmapAlloc() with M_ALIGN_CONTEXT and M3dmapAllocResult() with M_ALIGN_RESULT, respectively. Specify the type of alignment object: For an alignment disk, set the object type using M3dmapControl() with M_OBJECT_SHAPE set to M_DISK. For an alignment pyramid, set the object type using M3dmapControl() with M_OBJECT_SHAPE set to M_PYRAMID. Set the dimensions of the alignment object: For an alignment disk, set the diameter and height using M3dmapControl() with M_DIAMETER and M_HEIGHT, respectively. For an alignment pyramid, set the pyramid base length, pyramid top face length, and height using M3dmapControl() with M_PYRAMID_BOTTOM_BASE_LENGTH, M_PYRAMID_TOP_BASE_LENGTH, and M_HEIGHT, respectively. If you know the distance between scan lines (scan step) in the Y-direction and you don't want it to be calculated, you can manually set it using M3dmapControl(); first set M_STEP_LENGTH_MODE to M_USER_DEFINED, and then set M_STEP_LENGTH to the required value. If using a linear/rotary encoder in your 3D profile sensor setup, the scan step is typically the step of the encoder. Call M3dmapAlignScan() to calculate the transformations necessary to correct the alignment of your 3D profile sensor. Check the status of the alignment operation's calculation with M3dmapGetResult() using M_STATUS. The status tells you if the calculation was successful or explains why if it did not complete successfully. If the retrieved M_STATUS is not M_COMPLETE, make sure your setup is following the constraints outlined in the Alignment object constraints and requirements section later in this chapter before attempting other corrective measures. You can also verify that the holes on the alignment disk were properly found and taken into consideration during the calculation using M3dmapGetResult() with M_HOLES_FOUND. Once the transformations to correct the 3D profile sensor data is calculated, you can correct your setup using the following different ways: You can use M3dimMatrixTransform() to apply the calculated transformation matrix (M3dmapCopyResult() with M_TRANSFORMATION_MATRIX or M_XY_TRANSFORMATION_MATRIX) to point clouds acquired by the 3D profile sensor. You can use MbufConvert3d() when converting the 3D sensor data to a format that is 3D-processable. To do so, prior to calling MbufConvert3d(), set MbufControl() with M_3D_SHEAR_X, M_3D_SCALE_Y, M_3D_SHEAR_Z to the values retrieved using M3dmapGetResult() with M_3D_SHEAR_X, M_3D_SCALE_Y, M_3D_SHEAR_Z, respectively. You can pass the calculated results (such as pitch, yaw, and shear values from M3dmapAlignScan()) to a compatible 3D profile sensor (such as Matrox AltiZ) using MdigControlFeature(). In this case, the 3D profile sensor will directly output corrected scan lines (Y-axis). Free your allocated objects, using M3dmapFree(), unless M_UNIQUE_ID was specified during allocation. Types of distortions Depending on your 3D profile sensor setup, you might need to correct for one or more of the following distortion types. Pitch (shear between the Z-axis and Y-axis). Scale (scale distortions in Y cause shrinking or stretching). Yaw (shear between the X-axis and the Y-axis). Pitch Pitch around the X-axis will cause a shear between the Z-axis and the Y-axis in the scanned scene, which will produce an inaccurate thickness seen in the alignment disk (depth/Z-values). The image below provides a visualization of pitch and shear in the Z-direction. Typically, Z-values along the Y-axis are reported as if the 3D profile sensor is perfectly parallel to the plane of motion, not taking into account the pitch and that certain parts appear further. This can be seen with the lack of visible background gradient in the incorrect graphic (shown above), while a gradient is present in the corrected graphic, showing that the pitch has been corrected and taken into account. You can retrieve by how much to offset the Z-coordinates of the point cloud from the Z-coordinates of its previous row (to correct the shear in Z) using M3dmapGetResult() with M_3D_SHEAR_Z. Using MbufControlContainer(), you can set M_3D_SHEAR_Z of the range component to this value, prior to calling MbufConvert3d(), to correct the shear in Z. Instead of retrieving the correction values, you can also retrieve the pitch of the 3D profile sensor (in degrees) with M_SENSOR_PITCH. Scale Scale distortions in Y cause shrinking or stretching of the object scanned by the 3D profile sensor. The image below provides a visualization of what scale distortion in the Y-direction looks like. These scaling distortions occur when the speed of the conveyor or plane of motion is either faster or slower than the expected speed, causing the distance between scan lines (scan step) in the Y-direction to be closer or further apart. The Y-scaling factor necessary to correct the Y-coordinates of the point cloud can be retrieved using M3dmapGetResult() with M_3D_SCALE_Y. Using MbufControlContainer(), you can set M_3D_SCALE_Y of the range component to this value, prior to calling MbufConvert3d(), to correct the scale in Y. You can also retrieve the norm of the calculated step vector, which includes the scale in the Y-axis and, if present, the shear in the X and Z-axes; to do so, use M3dmapGetResult() with M_STEP_LENGTH. Yaw Yaw around the Z-axis will cause a shear between the X-axis and the Y-axis in the scanned scene, which will produce a distorted result. The image below provides a visualization of yaw, and what shear in the X-direction looks like. Typically, X-values are reported as if the laser plane was perfectly perpendicular to the direction of motion. You can retrieve by how much to offset the X-coordinates of the point cloud from the X-coordinates of its previous row (to correct the shear in X) using M3dmapGetResult() with M_3D_SHEAR_X. Using MbufControlContainer(), you can set M_3D_SHEAR_X of the range component to this value, prior to calling MbufConvert3d(), to correct the shear in X. Instead of retrieving the correction values, you can also retrieve the yaw of the 3D profile sensor (in degrees) with M_SENSOR_YAW. Example of correcting the placement of your 3D profile sensor setup The example alignlaserscans.cpp demonstrates how to correct the placement of your 3D profile sensor setup using M3dmapAlignScan(). It covers all the concepts explained in this section. alignlaserscans.cpp To run this example, use the Matrox Example Launcher in the MIL Control Center. Correcting for 3D profile sensor placement Steps to correct for 3D profile sensor placement Types of distortions Pitch Scale Yaw Example of correcting the placement of your 3D profile sensor setup ",
      "wordCount": 1247,
      "subEntries": []
    },
    {
      "id": "UG_Grabbing_from_3D_sensors_Alignment_object_requirements",
      "version": null,
      "title": "Alignment object constraints and requirements",
      "subTitles": [
        "Alignment pyramid constraints and requirements",
        "Recommended alignment pyramid dimensions",
        "Alignment disk constraints and requirements",
        "Recommended alignment disk dimensions"
      ],
      "location": "MIL UG P06: 3D related information",
      "pageURL": "content\\UserGuide\\Grabbing_from_3D_sensors\\Alignment_object_requirements.htm",
      "text": " Alignment object constraints and requirements There are three different types of alignment objects that can be used for correction. The flat surface object (M_FLAT_SURFACE) is useful when you do not have the ability to create an alignment pyramid (M_PYRAMID) or alignment disk (M_DISK). The flat surface object is limited to aligning the origin of the Z-axis of the working coordinate system and changing the positive direction of one or more axes of the working coordinate system, while maintaining the right hand rule. Calculations that rely on the presence of an alignment pyramid or disk will not be calculated, such as the values necessary to correct the scaling factor in the Y-direction and the shear in both the X- and Z-directions. Scanning the alignment pyramid or alignment disk and calling M3dmapAlignScan() will calculate these values and make the transformations necessary to calibrate your 3D profile sensor obtainable. For more information on 3D profile sensor placement and the types of distortions, see the Correcting for 3D profile sensor placement section earlier in this chapter. Alignment pyramid constraints and requirements The alignment pyramid is a machined calibration object that adheres to certain constraints and when scanned can be compared to the point cloud your setup. This real-world object should be created based on the specifications and constraints listed below, or simply use the recommended dimensions (in millimeters) for the object depending on your setup. Note that the alignment pyramid is a truncated square pyramid on a wider rectangular base that has a chamfer in the bottom right corner. The truncated pyramid is off-center along the length on the wider rectangular base. The alignment pyramid's wider rectangular base refers to the flat rectangular surface below the truncated pyramid, with (W) in the X-direction and (L) in the Y-direction, while the truncated pyramid's base refers to the truncated pyramid's square base (B). When using M3dmapAlignScan() with the alignment pyramid, the following constraints must be met: The top face and base of its truncated pyramid must be squares. The angle between the top and side faces of its truncated pyramid must be between 35 to 55 degrees (ideally 45 degrees). Its truncated pyramid's top face length (A) must be approximately 50% of the truncated pyramid's base length (B). The alignment pyramid's wider rectangular base must be approximately 25% wider (W) in the X-direction and 50% longer (L) in the Y-direction than its truncated pyramid's base (B). The alignment pyramid must cover at least 50% of the scanned width (X-direction). The alignment pyramid must cover at least 30% of the scanned length (Y-direction). Its truncated pyramid's faces (top and sides) and the top of the alignment pyramid's wider rectangular base must each be composed of at least 2000 points. The alignment pyramid's wider rectangular base must have a chamfer in the bottom right corner (F) with a length of at least 30% of the truncated pyramid's top face (A). The chamfer must be at an approximate angle of 45 degrees. Recommended alignment pyramid dimensions The following are the recommended dimensions for the alignment pyramid that should be used with M3dmapAlignScan() to calculate the transformations and information necessary to correct the alignment of a 3D profile sensor. You must specify the dimensions of the alignment pyramid using M3dmapControl() with M_PYRAMID_BOTTOM_BASE_LENGTH, M_PYRAMID_TOP_BASE_LENGTH, and M_HEIGHT before calling M3dmapAlignScan(). You should use matte gray sandblasted aluminum or 96% alumina ceramic for the material of the alignment pyramid. The top face of the truncated pyramid and the bottom of the alignment pyramid's wider rectangular base must have a flatness tolerance that falls between two parallel planes spaced 0.05 mm apart. Typically, medium-sized alignment pyramids are best for most alignment setups. Recommended values (millimeters) 1 L (Alignment pyramid's wider rectangular base length) W (Alignment pyramid's wider rectangular base width) B (Truncated pyramid's base length) A (Truncated pyramid's top face length) E (Side channel width) F (Chamfer length) H (Truncated pyramid's height) T (Alignment pyramid's wider rectangular base thickness) 2 Small 66.0 55.0 44.0 22.0 5.5 5.0 11.0 5.0 Medium 108.0 90.0 72.0 36.0 9.0 10.0 18.0 5.0 Large 210.0 175.0 140.0 70.0 17.5 15.0 35.0 5.0 1 Note that the alignment pyramid's wider rectangular base refers to the flat rectangular surface below the truncated pyramid, while the truncated pyramid's base refers to the truncated pyramid's square base. 2 The thickness is not used for the calibration, and is therefore free to be modified as needed for manufacturing purposes. Alignment disk constraints and requirements The alignment disk is a machined calibration object that adheres to certain constraints and when scanned can be compared to the point cloud your setup. This real-world object should be created based on the specifications and constraints listed below, or simply use the recommended dimensions (in millimeters) for the object depending on your setup. When using M3dmapAlignScan() with the alignment disk, the following constraints must be met: The alignment disk must cover at least 50% of the scanned width (X-direction). The alignment disk must cover at least 30% of the scanned length (Y-direction). The alignment disk's edge must be fully visible in the scan. The alignment disk's holes must be at least 30 scan lines (Y-direction) and 30 points (X-direction). The radii of the holes must be within 5 to 10% of the disk's radius, and the depth of the holes (P) must be at least 20% of the total disk's height (H). The floor (background) must be present in the scan. If you are using a stage, ensure its surface is parallel to the motion. Recommended alignment disk dimensions The following are the recommended dimensions for the alignment disk that should be used with M3dmapAlignScan() to calculate the transformations and information necessary to correct the alignment of a 3D profile sensor. You must specify the diameter and height of the alignment disk using M3dmapControl() with M_DIAMETER and M_HEIGHT before calling M3dmapAlignScan(). You should use matte gray sandblasted aluminum or 96% alumina ceramic for the material of the alignment disk, and the surface for both sides of the alignment disk must have a flatness tolerance that falls between two parallel planes spaced 0.005 mm apart. Typically, medium-sized alignment disks are best for most alignment setups. Recommended values (millimeters) D (Alignment disk diameter) d (Alignment disk hole diameter) P (Alignment disk hole depth) A (First hole distance from center) B (Second hole distance from center) H (Alignment disk height) Small 45.0 6.0 10.0 15.0 10.0 30.0 Medium 70.0 6.0 10.0 25.0 15.0 50.0 Large 150.0 10.0 10.0 50.0 20.0 50.0 Alignment object constraints and requirements Alignment pyramid constraints and requirements Recommended alignment pyramid dimensions Alignment disk constraints and requirements Recommended alignment disk dimensions ",
      "wordCount": 1104,
      "subEntries": []
    },
    {
      "id": "UG_Grabbing_from_3D_sensors_Using_a_different_working_coordinate_system",
      "version": null,
      "title": "Hand-eye calibration",
      "subTitles": [
        "Eye-in-hand calibration",
        "Eye-to-hand calibration",
        "Transformation matrices",
        "Steps to perform hand-eye calibration"
      ],
      "location": "MIL UG P06: 3D related information",
      "pageURL": "content\\UserGuide\\Grabbing_from_3D_sensors\\Using_a_different_working_coordinate_system.htm",
      "text": " Hand-eye calibration McalCalculateHandEye() calculates the transformations between coordinate systems in a robotics setup. The system of matrices typically takes the form of AX=ZB where A and B are known transformation matrices, and X and Z are unknown transformation matrices (X and Z do not refer to the X and Z-axes). The resulting matrices are useful to transform points/positional results so that they can be passed, for example, to a robot controller to move its arm to a specific position. To establish the unknown matrices, use a reference object (for example, a grid). You will need to provide the information from at least 3 different poses. For each pose, you must change the position and orientation of the tool holding the camera or grid with respect to the robot base. More specifically, you must rotate the tool along at least two non-parallel axes. The two most common types of robotics setups that can be solved using McalCalculateHandEye() are: Eye-in-hand calibration (the camera attached to robotic arm). Eye-to-hand calibration (the camera stationary and robotic arm moves). Note that McalCalculateHandEye() does not calibrate a camera; it only solves for the unknown transformation matrices. You must use M3dimMatrixTransform() with the resulting transformation matrices to apply the transformation. Therefore, this function is useful when your setup is using a precalibrated 2D camera or 3D sensor. In the case where you still need to calibrate your camera, you should use an M_3D_ROBOTICS camera calibration mode to calibrate your setup instead of McalCalculateHandEye(). For more information on this type of calibration, see the Robotics mode subsection of the 3D camera calibration modes section of Chapter 28: Calibrating your camera setup. Eye-in-hand calibration Eye-in-hand calibration is used for camera setups where the camera is mounted on the last joint of a robot arm, while a calibration grid or reference object is stationary and in the camera's field of view. Using McalCalculateHandEye() to solve the system of matrices, the two known transformation matrices for the eye-in-hand type of robotics set up are: Matrix A which transform points from the tool coordinate system to the robot base coordinate system (typically provided by the robot software). Matrix B which transform points from the camera coordinate system to the reference object coordinate system. The two unknown transformation matrices are: Matrix X which transforms points from the camera coordinate system to the tool coordinate system. Matrix Z which transforms points from the reference object coordinate system to the robot base coordinate system. Eye-to-hand calibration Eye-to-hand calibration is used for a camera setup where the camera is stationary and the robot arm, with a calibration grid or reference object mounted on the last joint, is moving. Using McalCalculateHandEye() to solve the system of matrices, the two known transformation matrices for the eye-in-hand type of robotics set up are: Matrix A which transforms points from the robot base coordinate system to the tool coordinate system (typically provided by the robot software). Matrix B which transforms points from the camera coordinate system to the reference object coordinate system. The two unknown transformation matrices are: Matrix X which transforms points from the camera coordinate system to the robot base coordinate system. Matrix Z which transforms points from the reference object coordinate system to the tool coordinate system. Transformation matrices The following table summarizes what is said above. Note that the transformation matrices are not limited to the descriptions shown in the table below. There are many different types of systems of matrices, both in robotics and non-robotics applications, that take the form of AX=ZB where two transformation matrices are known and two are unknown. The transformation matrices A, B, X, and Z represent different coordinate systems based on the setup. You must provide both the A and B transformation matrices, where A is typically obtained using the robotic software. Transformation matrix 1 A Known (Must be provided) X Unknown (Calculated and returned) Z Unknown (Calculated and returned) B Known (Must be provided) Eye-in-hand calibration Camera attached to robotic arm Transform points from the tool coordinate system to the robot base coordinate system Transforms points from the camera coordinate system to the tool coordinate system Transforms points from the reference object coordinate system to the robot base coordinate system Transform points from the camera coordinate system to the reference object coordinate system Eye-to-hand calibration Camera stationary and robotic arm moves Transforms points from the robot base coordinate system to the tool coordinate system Transforms points from the camera coordinate system to the robot base coordinate system Transforms points from the reference object coordinate system to the tool coordinate system Transforms points from the camera coordinate system to the reference object coordinate system 1 In this table, the camera can be a 2D camera calibrated using a 3D camera calibration mode or a precalibrated 3D sensor. Note that while the reference object is a calibration grid in the visualizations above, other types of reference objects such as spheres or calibration markers can be used in place of the calibration grid. Steps to perform hand-eye calibration The following steps provide a basic methodology for calculating the transformations required to perform operations such as robotic hand-eye calibration: Allocate a calculate hand-eye context, using McalAlloc() with M_CALCULATE_HAND_EYE_CONTEXT. Allocate a minimum of 8 transformation matrix objects using M3dgeoAlloc() with M_TRANSFORMATION_MATRIX and pass information about a minimum of 3 poses (each pose consists of an A matrix and a corresponding B matrix) to both HandMatrix3dgeoIdArrayPtr and EyeMatrix3dgeoIdArrayPtr, respectively. While you must pass a minimum of 3 poses, more poses improves the accuracy of the calculation. For each pose, you must change the position and orientation of the robot arm with respect to the robot base. More specifically, you must rotate the tool along at least two non-parallel axes. Two of the transformation matrices (M_TRANSFORMATION_MATRIX) will be used to retrieve the resulting X and Z transformation matrices (using McalCopyResult()). The number of transformation matrix objects in the array passed to HandMatrix3dgeoIdArrayPtr must be the same as EyeMatrix3dgeoIdArrayPtr. Allocate the result buffer using McalAllocResult() with M_CALCULATE_HAND_EYE_RESULT. Call McalCalculateHandEye() and retrieve your X and Z transformation matrices using McalCopyResult() with M_MATRIX_X and M_MATRIX_Z, respectively. At runtime, using the same robotics setup: Grab a point cloud. Optionally, transform the point cloud so that points are represented in the robot coordinate system, using M3dimMatrixTransform(). For eye-in-hand calibration, use a composition of the A and X transformation matrices (M3dgeoMatrixSetTransform() with M_COMPOSE_TWO_MATRICES), and for eye-to-hand calibration, only use the X transformation matrix. Perform the required analysis. If you didn't transform the point cloud in step b, transform positional results into the robot coordinate system using the above-mentioned transformations. Have the robot move the robot arm to the calculated position. Free all your allocated objects, using McalFree() and M3dgeoFree(), unless M_UNIQUE_ID was specified during allocation. Hand-eye calibration Eye-in-hand calibration Eye-to-hand calibration Transformation matrices Steps to perform hand-eye calibration ",
      "wordCount": 1138,
      "subEntries": []
    }
  ]
}]
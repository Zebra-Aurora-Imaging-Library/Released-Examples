[{
  "id": "UG_grabbing",
  "version": "2024020714",
  "title": "Grabbing with your digitizer",
  "subTitles": null,
  "location": "MIL UG P04: 2D related information",
  "pageURL": "content\\UserGuide\\grabbing\\ChapterInformation.htm",
  "text": " Chapter 27: Grabbing with your digitizer This chapter discusses how to grab images using a digitizer allocated on your imaging board and the features available when grabbing. Cameras and video sources Basic concepts for the MIL digitizers module Data format Device number Using a frame grabber Simultaneous acquisition Using a grabberless interface camera Using a Matrox Iris smart camera Line-scan cameras Grabbing to the display Grabbing a sequence of frames in real-time Grabbing and processing Grab mode Multiple buffering Grabbing large images Reference levels, lookup tables, and scaling Black and white reference levels Color image reference levels Mapping grabbed data through a LUT Scaling Grabbing with triggers Simple triggered grab Triggered grab with a camera in asynchronous reset mode Triggering on level high or level low Auto-focusing Optimum focus search strategies Bisection strategy Refocus strategy Scan-All strategy Smart-scan strategy Evaluate the focus indicator Simulated digitizer Using MIL with GenICam Basic concepts when dealing with GenICam Steps to view your GenICam-compliant camera's features Using GenICam with Camera Link cameras ",
  "wordCount": 170,
  "subEntries": [
    {
      "id": "UG_grabbing_Cameras_and_input_devices",
      "version": null,
      "title": "Cameras and video sources",
      "subTitles": null,
      "location": "MIL UG P04: 2D related information",
      "pageURL": "content\\UserGuide\\grabbing\\Cameras_and_input_devices.htm",
      "text": " Cameras and video sources MIL supports input from any type of video source connected to the hardware associated with an acquisition-type MIL system (such as a MIL Rapixo CXP system or MIL GigE Vision system). Each acquisition-type system has one or more available acquisition paths. MIL refers to the acquisition paths used to grab from one video source as a digitizer. Note, since most video sources are cameras, they will hereafter be referred to as such. For a digitizer to be recognized by MIL, it must be allocated on the target system using MdigAlloc() (or MappAllocDefault()). The allocation sets up the digitizer to match your camera's data format. Once you have finished using a digitizer, you should free it using MdigFree(). To make a more portable application, you can allocate and use the default system and digitizer set up when installing MIL or using the MILConfig utility. To allocate the default system and digitizer, use MappAllocDefault(). Alternatively, you can use MsysAlloc() with M_SYSTEM_DEFAULT to allocate the default system. Use the returned system identifier with MdigAlloc() and set the device number and data format both to M_DEFAULT to allocate the default digitizer. Use MdigGrab(), MdigProcess(), or MdigGrabContinuous() to grab data from a camera through a digitizer. When grabbing from a 2D camera, pass an image buffer with an M_GRAB attribute as the location in which to store grabbed images. The image buffer should have as many bands as the grabbed data has color components. If you need to grab and process concurrently, you can use MdigProcess(). Both the image buffer and digitizer must be allocated on the same system. If you have a 3D sensor, the basic procedure for grabbing is similar to grabbing from a 2D camera. However, you should typically pass a container with an M_GRAB attribute. For more information, see the Grabbing from 3D sensors overview section of Chapter 42: Grabbing from 3D sensors. When your application requests a grab, either directly using MdigGrab() or automatically using MdigProcess(), the grab is queued; MIL grabs the next transmitted frame into the specified buffer. It is possible to configure some frame grabbers and cameras to wait for a hardware trigger or timer signal to grab/transmit a frame of data. When developing an application that uses triggers, it is recommended to begin with both your frame grabber and camera configured to grab/transmit frames of data continuously. Once you have implemented core functionality in your application, you can configure your frame grabber or camera to grab/transmit data using triggers. This approach can simplify debugging your application. Cameras and video sources ",
      "wordCount": 430,
      "subEntries": []
    },
    {
      "id": "UG_grabbing_Basic_concepts",
      "version": null,
      "title": "Basic concepts for the MIL digitizers module",
      "subTitles": null,
      "location": "MIL UG P04: 2D related information",
      "pageURL": "content\\UserGuide\\grabbing\\Basic_concepts.htm",
      "text": " Basic concepts for the MIL digitizers module The basic concepts and vocabulary conventions for the MIL digitizers are: Acquisition path. A path that has the components to digitize or capture a video input signal. Camera. A generic term to refer to any physical device used for creating image/video data that can be transmitted it to a computer (for example, by connecting it to a frame grabber. Configurable camera. A generic term to refer to cameras that are configurable using software (for example, a GigE Vision camera or a Camera-Link camera). Frame grabber. A type of imaging board which can acquire video data from a camera. Digitizer. The acquisition path(s) with which to grab from one camera of the specified type. When several MIL digitizers are allocated, their device number, along with their digitizer configuration format (DCF), identify if they represent the same path(s) (but perhaps for a different input format) or independent path(s) for simultaneous acquisition. Data input channel (channel). Identifies which camera to use when several of its type can be connected to the same acquisition path(s) (for example, grab from channel 0 or channel 1 of digitizer 0). Independent acquisition path. An acquisition path that can, if required, acquire data from an input source independently from another such path on the same frame grabber. Each independent acquisition path has its own digitizing components to manage all video timing and synchronization for the path. When dealing with a grabberless interface camera, the connection between the camera and the interface board is considered an independent acquisition path. Interface board. A generic term to refer to the third-party board or chip on the motherboard used to communicate with one or more cameras, for example a Network interface card (NIC) is used to communicate with a GigE Vision-compliant camera. Imaging board. A generic term to refer to all boards that manipulate video data, including: frame grabbers, vision processors, compression boards, and other specialized boards. Grabberless interface camera. Any camera which uses a physical interface that does not require a frame grabber (for example, a USB or network camera). Vision processor. A type of imaging board which can process and analyze image/video data. A vision processor includes a CPU and additional memory. GenICam. An industry standard that specifies a common software interface for machine vision cameras, administered by the European Machine Vision Association (EMVA). CoaXPress (CXP). A high performance digital interface for cameras, using packetized transmission over one or more coaxial cables connected to a compatible frame grabber. The CXP protocol can be used to stream any type of information, including 3D data and information about the camera's current configuration. CXP cameras can be configured and controlled over the same coaxial cable used to stream image data. Some CXP cameras also support power-over-CoaXPress (PoCXP). All CXP cameras comply with the GenICam interface standard. Camera Link. A high performance digital interface for cameras, using low voltage differential signaling (LVDS) over one or more Camera Link cables connected to a compatible frame grabber. The Camera Link streaming protocol is only designed to transmit image data and does not have provisions for streaming other types of data. Camera Link cameras can be configured and controlled over the same Camera Link cable used to stream image data. Some Camera Link cameras also support power-over-Camera-Link (PoCL). Some Camera Link cameras support the GenICam interface standard through the use of additional libraries that you must specify (such as a GenCP library installed with MIL, or a third-party, vendor-supplied CLProtocol library). USB3 Vision. A software interface for cameras, utilizing standard USB 3.0 (or higher) connections. All USB3 Vision cameras comply with the GenICam interface standard. USB3 Vision cameras are a type of grabberless interface camera. GigE Vision. A software interface for network cameras, utilizing standard Gigabit Ethernet (or higher) connections. All GigE Vision cameras comply with the GenICam interface standard. GigE Vision cameras are a form of grabberless interface camera. GenICam Standard Feature Naming Convention (SFNC). A set of standard camera feature (setting) names, defined as part of the GenICam standard. Camera manufacturers are encouraged (and sometimes mandated) to use these standard names for devices that implement GenICam. GenICam Generic Data Container (GenDC). A data block encoding format for transmitting one or more types of data in a single grab, defined as an optional part of the GenICam standard. A GenDC container has one or more components, each of which can have its own data type and layout. Data transmitted in the GenDC format is suitable for grabbing into a MIL container. Basic concepts for the MIL digitizers module ",
      "wordCount": 761,
      "subEntries": []
    },
    {
      "id": "UG_grabbing_The_data_format",
      "version": null,
      "title": "Data format",
      "subTitles": null,
      "location": "MIL UG P04: 2D related information",
      "pageURL": "content\\UserGuide\\grabbing\\The_data_format.htm",
      "text": " Data format MdigAlloc() needs the digitizer configuration format (DCF) that corresponds to your camera to perform the digitizer allocation. The DCF defines such settings as the input frequency and resolution, and will determine limits when grabbing an image. Once a digitizer has been allocated, many of the settings are established based on the selected DCF. You can change these settings to values other than the default using MdigControl() and other Mdig...() functions. You can use MdigInquire() to inquire about a digitizer's settings. MIL provides a number of predefined DCFs for the basic cameras supported by your frame grabber. MIL provides a few alternate DCF files that you can load if the predefined DCFs do not suit your needs. These alternate DCF files were installed when you installed MIL. If you find a DCF file that is appropriate for your camera, but you need to adjust some of the more common settings, you can do so directly, without adjusting the file, using the Mdig...() functions. For more specialized adjustments, you can adjust the DCF file itself, using Matrox Intellicam. If you cannot find an appropriate DCF file because, perhaps, you have a non-standard camera or other video source, you can create your own DCF file, using Matrox Intellicam. For more information on Matrox Intellicam, refer to the Matrox Intellicam User Guide manual. If you cannot develop the required DCF using Matrox Intellicam, you should provide the camera specifications to your Matrox Technical Support Engineer. A suitable customized DCF file can then be developed, if your frame grabber supports the camera. With GigE Vision-compliant devices (such as Matrox AltiZ), Camera-Link standard (such as Matrox Solios eV-CL) and CoaXPress (CXP) communication standard-compliant frame grabbers (such as Matrox Radient eV-CXP), the Matrox Feature Browser accesses the XML file used for your camera's configuration, and allows you to view and change its information. The Matrox Feature Browser can be launched through Matrox Intellicam or through MdigControl() with M_GC_FEATURE_BROWSER. Data format ",
      "wordCount": 327,
      "subEntries": []
    },
    {
      "id": "UG_grabbing_The_digitizer_number",
      "version": null,
      "title": "Device number",
      "subTitles": [
        "Using a frame grabber",
        "Simultaneous acquisition",
        "Using a grabberless interface camera",
        "Using a Matrox Iris smart camera"
      ],
      "location": "MIL UG P04: 2D related information",
      "pageURL": "content\\UserGuide\\grabbing\\The_digitizer_number.htm",
      "text": " Device number In addition to the data format, MdigAlloc() requires that you specify the number or string to identify the acquisition path(s) to allocate for a digitizer. Typically, only one acquisition path is required to grab from a camera. Note that a connection to a grabberless interface camera is considered an acquisition path. Typically, each acquisition path can only be associated with one digitizer at a time. In these cases, if the specified acquisition path is not available (for example, because no camera is connected, or because the acquisition path is already allocated), the allocation fails and an error is generated. Note that some older frame grabbers did not have this restriction and would perform grabs from these digitizers sequentially. Using a frame grabber When using a camera connected to a frame grabber, use M_DEVn (where n is the device number) to specify which acquisition path(s) on the frame grabber to allocate for the digitizer. Typically, this corresponds to the index of the connector on the frame grabber. For a CXP camera that uses multiple connections, you set the device number of the digitizer to the index of the connector used for the master connection. The other connections (acquisition paths) used for the camera are associated with the digitizer automatically. Simultaneous acquisition With most Matrox frame grabbers, you can connect multiple cameras and grab from all connected cameras simultaneously. To grab from several sources simultaneously, you must allocate an independent digitizer for each camera using MdigAlloc() and then make multiple calls to MdigGrab(), MdigGrabContinuous(), or MdigProcess() with the identifiers of the digitizers. Refer to the MIL Hardware-specific Notes for more information regarding the number of cameras from which you can acquire data simultaneously using your Matrox frame grabber. The following example shows you how to allocate two digitizers on a system and use them to perform two continuous grabs simultaneously. mdiggrabmultiple.cpp For some older frame grabbers, multiple connections could be associated with the same acquisition path. In these cases, each camera must be connected to a different acquisition path to allow simultaneous grabbing. Using a grabberless interface camera A grabberless interface camera is an all encompassing term for any camera whose physical interface does not require a frame grabber. For example, MIL supports GigE Vision compliant (network) cameras and USB3 Vision compliant cameras. MIL can support other types of grabberless interface cameras if they are distributed with a standard-compliant GenTL Producer (library). When using a grabberless interface camera, you can use the camera name or device number (the camera's rank, determined when MIL enumerates the cameras that are visible from the operating system of your computer) to identify the camera to allocate for the digitizer. For GigE Vision cameras, you can also use the IP address to identify the camera to allocate for the digitizer. All GigE Vision cameras can transmit data to the Host independently of each other. However, completion of a grab from a GigE Vision camera might be delayed due to the bandwidth limitations of your network, packet size limitations, and/or network adapter interrupt moderation settings. While GigE Vision cameras can be used over any Ethernet network, you can guarantee that a camera has the maximum bandwidth by connecting it directly to a network adapter. Since the camera's classification depends on the standards of communication, additional information on grabberless interface cameras can be found in the MIL Hardware-specific Notes section of the MIL Help. Using a Matrox Iris smart camera For a Matrox Iris smart camera, the device number is always M_DEV0. Device number Using a frame grabber Simultaneous acquisition Using a grabberless interface camera Using a Matrox Iris smart camera ",
      "wordCount": 606,
      "subEntries": []
    },
    {
      "id": "UG_grabbing_Linescan_cameras",
      "version": null,
      "title": "Line-scan cameras",
      "subTitles": null,
      "location": "MIL UG P04: 2D related information",
      "pageURL": "content\\UserGuide\\grabbing\\Linescan_cameras.htm",
      "text": " Line-scan cameras If your target digitizer supports it, you can grab from a line-scan camera as you would, for example, an RS-170 type camera. However, you should be aware of how data from these cameras is stored. When acquiring data from a line-scan camera, each line (row) of each destination buffer band is filled from top to bottom. The operation will only end once the entire buffer has been filled. Line-scan cameras ",
      "wordCount": 74,
      "subEntries": []
    },
    {
      "id": "UG_grabbing_Grabbing_to_the_display",
      "version": null,
      "title": "Grabbing to the display",
      "subTitles": null,
      "location": "MIL UG P04: 2D related information",
      "pageURL": "content\\UserGuide\\grabbing\\Grabbing_to_the_display.htm",
      "text": " Grabbing to the display To grab to the display, you should grab into a displayable grab image buffer (MbufAlloc...() with M_DISP + M_GRAB) that is selected for display (MdispSelect()). The display of the selected buffer is typically maintained with separate internal buffers in display memory or Host non-paged memory. The selected buffer and the display are maintained and synchronized internally. The synchronization frequency is dependent upon whether performing a monoshot or continuous grab. A monoshot grab updates the selected buffer first and then its display. A continuous grab updates the specified buffer's display immediately after each frame is grabbed; only the last frame is stored in the selected buffer for processing. If you need to save/process each grabbed frame, you should perform the grab using MdigGrab() (that is, perform a monoshot grab); alternatively, you can use MdigProcess() for its multiple buffering capabilities. Grabbing to the display ",
      "wordCount": 148,
      "subEntries": []
    },
    {
      "id": "UG_grabbing_Grabbing_a_sequence_of_frames_in_realtime",
      "version": null,
      "title": "Grabbing a sequence of frames in real-time",
      "subTitles": null,
      "location": "MIL UG P04: 2D related information",
      "pageURL": "content\\UserGuide\\grabbing\\Grabbing_a_sequence_of_frames_in_realtime.htm",
      "text": " Grabbing a sequence of frames in real-time To grab a sequence of frames in real-time, simply use successive, asynchronous calls to MdigGrab(): /* Put digitizer in asynchronous mode */ MdigControl(MilDigitizer, M_GRAB_MODE, M_ASYNCHRONOUS); /* Grab the sequence. */ for (n=0; n&lt;NbFrames; n++){ /* Grab one buffer at a time. */ MdigGrab(MilDigitizer, MilImage[n]); } Alternatively, you can make a single call to MdigProcess(). For more information, see the Multiple buffering subsection of the Grabbing and processing section later in this chapter. In either case, you must allocate buffers to store the frames of the sequence. After you have grabbed a sequence, you can use the MbufExportSequence() function to export the sequence of image buffers (compressed or uncompressed 8-bit) to an AVI file. When exporting, you must specify the number of buffers and the frame rate (number of images/second) of the sequence. Note, the MIL identifiers of the image buffers to export must be kept in an array. Use the MbufImportSequence() to import a sequence of images from an AVI file into separate image buffers. You can import images in all formats supported by MIL. You can also choose to import the sequence into automatically allocated buffers or previously allocated buffers. Grabbing a sequence of frames in real-time ",
      "wordCount": 207,
      "subEntries": []
    },
    {
      "id": "UG_grabbing_Grabbing_and_processing",
      "version": null,
      "title": "Grabbing and processing",
      "subTitles": [
        "Grab mode",
        "Multiple buffering"
      ],
      "location": "MIL UG P04: 2D related information",
      "pageURL": "content\\UserGuide\\grabbing\\Grabbing_and_processing.htm",
      "text": " Grabbing and processing To optimize application performance when grabbing, you can: Set the grab mode. Perform multiple buffering. Grab mode When grabbing data with MdigGrab(), you can control the synchronization by setting the M_GRAB_MODE control type of MdigControl() to M_SYNCHRONOUS, M_ASYNCHRONOUS, or M_ASYNCHRONOUS_QUEUED (if supported). If the grab mode is set to M_SYNCHRONOUS, your application will be synchronized with the end of a grab operation. In other words, your application will wait until the grab has finished before executing the next function. If the grab mode is set to M_ASYNCHRONOUS, your application will not be synchronized with the end of a grab operation. This option allows other functions to execute while you are still grabbing. This is a useful option when performing multiple buffering, a technique whereby you can grab data into one buffer while processing previously grabbed buffers (discussed below). Note, another call to MdigGrab() before the current grab has finished will cause your application to wait until the current grab has finished. If your frame grabber supports queuing, you can set the grab mode to M_ASYNCHRONOUS_QUEUED; if another grab is issued before the first one is finished, the grab will be queued on-board, allowing you to perform other processes while waiting for the next MdigGrab() to be executed. Note, you can still force your application to wait until the end of a grab before executing an operation, by calling MdigGrabWait(). Note that MdigGrabContinuous() is by definition asynchronous since you must use MdigHalt() to stop the grab. Multiple buffering Multiple buffering involves grabbing into one image buffer while processing previously grabbed images. This technique allows you to grab and process images concurrently. To perform multiple buffering in MIL, you can use MdigProcess(). It allows you to use a list of previously-allocated buffers to hold a series of sequentially grabbed images and process them as they are being grabbed. These grabs can either continue until stopped (using MdigProcess() with M_START to start and then M_STOP to stop) or continue until all buffers in the list are filled (using MdigProcess() with M_SEQUENCE). In the former case, MdigProcess() grabs round-robin through the list of buffers (buffers are filled with the grabbed data typically in the order they are stored in the list, wrapping around to the first buffer in the list once the last buffer in the list is filled); however, care must be taken to ensure that the average time it takes to process a frame is not greater than the frame rate of a camera, so that frames will not be missed. Once a frame of data is grabbed into a buffer/container, MIL will not grab into that buffer/container again until the hook function has finished processing it. Once the hook function finishes executing, the buffer/container becomes available for MIL to grab into it. If all buffers/containers are filled with data and are waiting to be processed by the hook function, MIL waits for the next available buffer/container (typically the next buffer/container in the list) before grabbing another frame of data. Optionally, instead of grabbing data into a list of previously allocated buffers, MdigProcess() can automatically allocate an appropriate image buffer to store each grabbed frame of data. This is particularly useful when grabbing from a camera that is configured to transmit images with different dimensions for each grab. To specify that the buffers should be automatically allocated, set DestContainerOrImageBufArrayPtr to M_NULL. MdigProcess() hooks a user-defined function to the modification of any buffer in the specified list. So every time a new frame is grabbed in a buffer in the list, the user-defined function is called. If the hook-handler function modifies the buffer that called it, the hook-handler function for that buffer will not be called again until that buffer is modified by a new grab. This is to avoid infinite recursive calls being generated. Note that, if the buffer is modified by some function other than the one hooked to it, the hooked function will be called. In addition, if the hook-handler function modifies another buffer which has another function hooked, that function will be called. Note, processing is generally faster if the buffer is not on the display. The following example shows you how to perform multiple buffering. mdigprocess.cpp Grabbing and processing Grab mode Multiple buffering ",
      "wordCount": 710,
      "subEntries": []
    },
    {
      "id": "UG_grabbing_Grabbing_large_images",
      "version": null,
      "title": "Grabbing large images",
      "subTitles": null,
      "location": "MIL UG P04: 2D related information",
      "pageURL": "content\\UserGuide\\grabbing\\Grabbing_large_images.htm",
      "text": " Grabbing large images In some cases, you might need to grab an image whose size is in the order of gigabytes. Ideally, you would use a digitizer whose DCF specifies an appropriate frame size. In practice, this is not always possible because the on-board frame size is restricted by the memory space on-board. If you try to set a larger frame size using Matrox Intellicam, you will obtain an error when you call MdigAlloc() due to insufficient memory space for the temporary on-board buffers required for the large frame. A solution to the above problem is to do the following: Allocate a large buffer on the Host using MbufAlloc...() with M_GRAB. Allocate child buffers in the large parent buffer using MbufChild...(). The size of each child buffer will have to be equal to or smaller than the supported frame size specified by the DCF settings. Put each child buffer into an array for easy access later. Grab sectional frames of the large image into each child buffer in the array. As mentioned in the Multiple buffering subsection of the Grabbing and processing section earlier in this chapter, to ensure that no frames are missed during a grab, you can use MdigProcess(). MdigProcess() grabs a sequence of images into an array of image buffers and can cause a user-defined function to be called after an image has been grabbed into any of these buffers. Grabbing large images ",
      "wordCount": 238,
      "subEntries": []
    },
    {
      "id": "UG_grabbing_Reference_levels_lookup_tables_and_scaling",
      "version": null,
      "title": "Reference levels, lookup tables, and scaling",
      "subTitles": [
        "Black and white reference levels ",
        "Color image reference levels ",
        "Mapping grabbed data through a LUT ",
        "Scaling"
      ],
      "location": "MIL UG P04: 2D related information",
      "pageURL": "content\\UserGuide\\grabbing\\Reference_levels_lookup_tables_and_scaling.htm",
      "text": " Reference levels, lookup tables, and scaling MIL provides functions to improve the appearance of a grabbed image on input (if your hardware allows it). You can adjust the brightness and contrast of the images, as well as the hue and saturation for color grabs, by fine-tuning the controls of the analog-to-digital converters in your system. You can also correct and precondition the input data prior to storing it, through scaling, or by mapping it through an input LUT. Black and white reference levels When digitizing images, the black and white reference levels determine the zero and full-scale levels, respectively, of the input voltage range. The analog-to-digital converters convert any voltage above the white reference level to the maximum pixel value, and any voltage below the black reference level to a zero pixel value. Matrox digitizers support fine-tuning of these reference levels. By reducing or increasing either or both the black and white reference levels, you affect the brightness of the image. By reducing one reference level and increasing the other, you affect the contrast of the image. MIL linearly represents the distance between the minimum and maximum voltages, in which the black reference level can be adjusted (hardware-specific), as units between M_MIN_LEVEL and M_MAX_LEVEL. The same is done for the white reference level adjustment range. These units are the values by which you can adjust the specified reference level, using MdigControl(). To calculate the value to pass to MdigControl(), use the following equation with the appropriate voltages specified for your particular board. The smallest voltage increment supported by your board can differ such that consecutive reference-level settings might produce the same result. Note, the new reference level might not take effect until the next grab, at which point, a certain amount of delay might be incurred as the hardware adjusts to the reference-level changes. Color image reference levels When grabbing composite color images, MdigControl() provides specific control parameters to adjust the levels of contrast, brightness, hue, and saturation. These levels can be set to values from 0 to 255; use values appropriate for your particular board. Mapping grabbed data through a LUT If supported by your digitizer, grabbed data is mapped through a physical lookup table (LUT). By default, the physical LUT is transparent; that is, it is loaded with data so that each input pixel is mapped to its original value. You can load the physical LUT with your own custom LUT data if, for example, you want to correct or precondition grabbed data. To do so, copy a LUT buffer to the digitizer's physical input LUT, using MdigControl() with M_LUT_ID. MIL uses the data format (DCF) of the digitizer to determine whether a physical LUT is supported. If it is not, an error is generated. The characteristics of the LUT buffer and the digitizer's DCF establishes how the physical LUT is actually configured. Configuration of the digitizer's LUT Determining factor Number of entries in the digitizer's physical LUT Data being grabbed (DCF) Depth (8- or 16-bit) LUT buffer Number of bands DCF &amp; LUT buffer The digitizer's physical LUT is typically configured to have the same number of components (bands) as either the LUT buffer or the data to be grabbed (determined by the digitizer's DCF), depending on which has more bands. The digitizer's physical LUT is also configured to have the same number of entries as the maximum possible value per band of the data to grab. The depth of a digitizer's physical LUT is configured to be either 8- or 16-bits per band, depending on if the LUT buffer depth is 8-bit or 16-bit, respectively. Cameras, however, are not so limited. When dealing with a 10- or 12-bit camera, use a 16-bit destination grab buffer and load the digitizer's physical LUT with the difference zero-padded. To copy the data from a LUT buffer to the digitizer's physical LUT, the number of entries in the LUT buffer must match those of the digitizer's physical LUT. In addition, if the digitizer's physical LUT cannot support the depth of the LUT buffer, an error will occur. LUT buffer data is loaded into the digitizer's physical LUT, as follows: LUT buffer Digitizer's physical LUT Result 1 band 1 band The LUT buffer is copied directly into the digitizer's physical LUT. 1 band 3 band The LUT buffer is copied into each component of the digitizer's physical LUT. 3 band 1 band The first band of the LUT buffer is copied into the digitizer's physical LUT. 3 band 3 band Each of the LUT buffer's bands are copied into the corresponding component of the digitizer's physical LUT. If the destination grab buffer depth is larger than that of the digitizer's physical LUT, the bits from the physical LUT are copied over and the remaining upper bits in the destination grab buffer are set to 0. If the digitizer's physical LUT depth is greater than that of the destination host grab buffer, the most-significant bits of the data (the non-zero values) are used when the data is grabbed. To revert back to using a transparent LUT, you must copy the default LUT (MdigControl() with M_LUT_ID set to M_DEFAULT) to the digitizer's physical input LUT. Scaling The MdigControl() function allows you to scale grabbed data horizontally and vertically. If you scale grabbed data, the stored image size is different from the original image by the specified factors in the X- and/or Y-direction. The scaled image is written in contiguous locations in the image buffer, starting from the top-left corner. For example, if you set both the X- and Y-scaling factors to 0.5, only one column and one row out of two are written to the image buffer (starting with the first row and column). The X- and Y-scaling factors are independent. Note, depending on the Matrox frame grabber and camera used, some scaling factors might not be available. To disable scaling, set the scaling factors to 1. Reference levels, lookup tables, and scaling Black and white reference levels Color image reference levels Mapping grabbed data through a LUT Scaling ",
      "wordCount": 1008,
      "subEntries": []
    },
    {
      "id": "UG_grabbing_Grabbing_with_triggers_and_exposures",
      "version": null,
      "title": "Grabbing with triggers",
      "subTitles": [
        "Simple triggered grab",
        "Triggered grab with a camera in asynchronous reset mode",
        "Triggering on level high or level low"
      ],
      "location": "MIL UG P04: 2D related information",
      "pageURL": "content\\UserGuide\\grabbing\\Grabbing_with_triggers_and_exposures.htm",
      "text": " Grabbing with triggers Upon a grab command (MdigGrab(), MdigGrabContinuous(), or MdigProcess()) in normal untriggered mode, the digitizer always grabs the next valid frame. However, if the camera is continuously streaming images, the frame grabbed might not be the required frame. If this is the case and you want to grab a frame upon a specific event (for example, an object passing in front of a sensor), you need to issue the grab command in triggered mode (triggered grab); that is, you must have the digitizer wait to receive a trigger signal before performing the grab command. You can specify to perform a triggered grab and control trigger settings either in MIL using MdigControl() with the M_GRAB_TRIGGER... control types, or using an appropriate digitizer configuration format (DCF); for examples of DCF files, see the Camera Interface Application Notes for your camera, available on Matrox's website. Callout # MdigControl() control type 1 M_GRAB_TRIGGER_SOURCE 2 M_GRAB_TRIGGER_ACTIVATION 3 M_GRAB_TRIGGER_STATE Note that some cameras support triggered acquisition; that is, they support acquiring and transferring frames (or lines) only upon receiving an external trigger. In this case, the allocated digitizer must be in continuous mode (untriggered mode) to grab all the frames (or lines) sent by the camera. When working with grabberless interface cameras (for example, GigE Vision cameras), the M_GRAB_TRIGGER... control types always control the trigger settings of the camera; the digitizer (on the Host side) is always left in continuous mode. If supported by your digitizer, you can also trigger a grab with a software trigger using MdigControl() with M_GRAB_TRIGGER_SOURCE set to M_SOFTWARE. In this case, following a grab call, nothing is grabbed until you call MdigControl() with M_GRAB_TRIGGER_SOFTWARE. Note that the grab call must be asynchronous (that is, you must issue the grab with MdigGrab() in asynchronous mode or with MdigGrabContinuous()), or the grab call must be made on a separate thread. Note that, the remainder of this section does not deal with grabberless interface cameras. Simple triggered grab In a simple triggered grab, when the digitizer receives a trigger signal, the digitizer waits for the video data to reach the start of a new frame before it begins to grab an image. For example, in the illustration below, the digitizer receives a trigger signal when the camera has already started to stream a new frame. The digitizer will only grab an image once the current frame has completed streaming and the camera starts streaming the next frame. Triggered grab with a camera in asynchronous reset mode With a simple triggered grab, it is possible that the image grabbed is not the image you intended to grab because the time lapse between the arrival of the trigger pulse and the start of the next valid frame might not have been within your required tolerances. To more accurately schedule a triggered grab, a timer output signal can also be sent to reset the camera, if the camera is in asynchronous reset mode. In a triggered grab with camera reset, as illustrated below, the digitizer receives a trigger signal that is sent to activate an on-board timer. Upon the rising edge of the timer output signal, the camera will stop transmitting the current frame, reset, and re-expose its CCD; it will then immediately start sending the next frame. In this illustrated example, the grab controller is triggered on the falling edge of the timer output signal. Note that the camera exposure duration is typically set using a third-party camera vendor tool or if supported, a GenICam interface (for example, MdigControlFeature() or Matrox Intellicam's feature browser); if this is the case, the exposure time is fixed by the camera's internal exposure control settings. When the camera can use an external exposure control signal, the active portion of the timer output can be used to specify the exposure duration. To set up a triggered grab that uses a timer to reset the camera and control the exposure time, as illustrated above, perform the following steps: Set up and enable a timer (for example, M_TIMER1). If your camera can use an external exposure control signal, the timer output signal's active portion (M_TIMER_DURATION) can be used as the exposure duration. For information on how to set-up a timer and route its output to a camera, refer to the Timers and coordinating events section of Chapter 56: I/O signals and communicating with external devices. Specify the source of the trigger signal for the grab, using MdigControl() with M_GRAB_TRIGGER_SOURCE. For example, if using timer 1 from the previous step, set M_GRAB_TRIGGER_SOURCE to M_TIMER1. Specify to trigger the grab upon the rising edge and/or the falling edge, or the high or low state, of the timer output signal, using MdigControl() with M_GRAB_TRIGGER_ACTIVATION. Set the digitizer to wait for a trigger after a grab command is issued before grabbing, using MdigControl() with M_GRAB_TRIGGER_STATE set to M_ENABLE. Issue a grab command (MdigGrab(), MdigGrabContinuous(), or MdigProcess()). Since the grab trigger state was enabled, the digitizer will wait for a trigger before grabbing each frame. Triggering on level high or level low As mentioned above, you can specify to trigger the grab upon the rising edge and/or the falling edge, or the high or low state of a signal, using MdigControl() with M_GRAB_TRIGGER_ACTIVATION. Triggering on the high or low state of a signal is useful when grabbing from a line scan camera with frames of variable length, such that you cannot specify the number of lines to grab for each image at the beginning of each grab. In this case, you would use your trigger signal to also indicate when to stop the grab; while the trigger signal is high (or low), it would continuously trigger to grab the next line of data, and while the trigger signal is in the opposite state, it would not issue any trigger so no lines would be grabbed. For example, if you want to grab an image that includes only the object to analyze, you can use the signal from a sensor as a trigger signal to grab. Then, set the trigger activation mode to M_LEVEL_HIGH so that the digitizer is continuously triggered to grab during the signal's level high. When the sensor detects the presence of an object on a conveyor belt, have it transmit a level high signal; if there is no object passing through the sensor, have it transmit a level low signal. The following image depicts a frame grabber connected to a sensor to grab images of objects on a conveyor belt when they pass through the sensor. In this setup, the line scan camera exposes a new line if the object moves forward. A rotary encoder is used to detect movement. A sensor is used to detect when to grab and when not to grab. For more information on rotary decoders, refer to the Using quadrature input from a rotary encoder section of Chapter 56: I/O signals and communicating with external devices. The following image shows the timing diagram of the set-up shown above, with M_GRAB_TRIGGER_ACTIVATION set to M_LEVEL_HIGH. Note that lines are only grabbed when the frame valid signal (the grab trigger signal) is high. Grabbing with triggers Simple triggered grab Triggered grab with a camera in asynchronous reset mode Triggering on level high or level low ",
      "wordCount": 1206,
      "subEntries": []
    },
    {
      "id": "UG_grabbing_Autofocusing",
      "version": null,
      "title": "Auto-focusing",
      "subTitles": [
        "Optimum focus search strategies",
        "Bisection strategy",
        "Refocus strategy",
        "Scan-All strategy",
        "Smart-scan strategy",
        "Evaluate the focus indicator"
      ],
      "location": "MIL UG P04: 2D related information",
      "pageURL": "content\\UserGuide\\grabbing\\Autofocusing.htm",
      "text": " Auto-focusing You can use MdigFocus() to automatically adjust the lens motor of your camera to a position that produces optimum focus in your images. This function is primarily useful when the quality of the focus in your images is unacceptable and physically moving the grabbed object is not possible. MdigFocus() determines the optimum focus position by calling a user-defined function to move the lens motor to a specified initial position, grabbing an image, calculates the focus indicator of the grabbed image, and based on the focus indicator and the specified optimum focus search strategy, calculating a new position for the lens motor. The process repeats, calling the user-defined function to move the lens motor to the newly calculated position and comparing the new focus indicator with the old. The process continues repeating until the optimum focus position is found. The focus quality of an image (known as its focus indicator) is measured by analyzing its edges. An image with good focus quality (a high focus indicator) has well-defined edges, that is, has a sharp difference in gray-levels between its object edges and its background. By default, MdigFocus() subsamples and filters each grabbed image before analyzing it. This makes it easier to analyze the image. If necessary, you can specify that the subsampling and/or filtering operations be skipped. Skipping these operations will result in a more accurate analysis of the image's focus quality. It is primarily useful to skip these operations when your images contain fine details since subsampling or filtering can remove these details. Note that subsampling the grabbed images increases the speed of MdigFocus(); filtering the grabbed images slows down MdigFocus(). If necessary, you can specify that only a sub-region of the image be analyzed, by passing a child buffer to the function. This is primarily useful if there are objects at different distances within the camera's field of view. In such a case, each object will have a different optimum focus position, so you need to use a child buffer to specify the object on which to focus. Optimum focus search strategies When you perform MdigFocus(), you have to specify the minimum, maximum, and starting position of the lens motor. Given these parameters, different strategies can be used to find the optimum focus position. These strategies determine how the position is updated (in which direction and by how much) between grabs. They can affect the speed and accuracy of the operation. The default search strategy used by MIL is the smart-scan strategy. Bisection strategy The bisection strategy (M_BISECTION) breaks down the given positional range, step-by-step, until it finds the optimum focus position. In general, the bisection strategy processes the fewest amount of images. However, it is most sensitive to noise and requires that the lens motor travel the greatest distance. Refocus strategy The refocus strategy (M_REFOCUS) scans upward or downward until it finds the optimum focus position or until it reaches the minimum or maximum position. While scanning in one direction, if the focus indicator decreases continuously (indicating an out-of-focus condition), the focus position is returned to its starting point and scanning is started in the opposite direction. By default, if a peak in focus indicator values is found, the next two positions are scanned to make sure the peak is truly the optimum. If necessary, you can change the number of positions used to verify a peak. The refocus strategy is the best strategy to use when the current focus position is close to optimum. Scan-All strategy The scan-all strategy (M_SCAN_ALL) scans each positions between the minimum and maximum and returns the position which produced the highest focus indicator. The scan-all strategy is the slowest but most accurate. Smart-scan strategy The smart-scan strategy (M_SMART_SCAN) performs three refocus searches, each with a smaller positional increment. You specify the initial positional increment; the subsequent increments are factors of the initial one. As with the refocus strategy, the default number of positions used to verify a peak is 2 but can be changed. The smart-scan strategy is a compromise between the speed of a bisection and the accuracy of a scan-all. Evaluate the focus indicator Rather than determine the optimum focus position, MdigFocus() can be used to simply return the focus indicator value for a given image or for the image grabbed at the current lens position. To do so, use M_EVALUATE. This focus indicator value varies according to the number of edges present and their sharpness. An image without any edges has a focus indicator value of 0. To determine the best focus position, compare the focus indicator values found with multiple calls to M_EVALUATE on images of the same scene, taken with different focus distances. Larger focus indicator values indicate that the image is more in focus. If the image is brought out of focus, the focus indicator decreases accordingly. It is important not to have saturation in the images being analyzed, because saturation can create strong artificial edges that move when the focus changes. Auto-focusing Optimum focus search strategies Bisection strategy Refocus strategy Scan-All strategy Smart-scan strategy Evaluate the focus indicator ",
      "wordCount": 846,
      "subEntries": []
    },
    {
      "id": "UG_grabbing_Simulated_Digitizer",
      "version": null,
      "title": "Simulated digitizer",
      "subTitles": null,
      "location": "MIL UG P04: 2D related information",
      "pageURL": "content\\UserGuide\\grabbing\\Simulated_Digitizer.htm",
      "text": " Simulated digitizer In some cases, you might need to develop or debug an application on a computer that does not have access to the hardware which will be used or is used to grab images. In these cases, you can use a simulated digitizer to grab the required images and to develop/test the rest of your application. A simulated digitizer can grab a single image from a single file (stored in any file format supported by MbufImport() except for *.raw), multiple images from a single video file (*.avi), a series of images from all supported files in a directory, or multiple images from a pattern generator. When grabbing from a video or directory of images, once the last image is grabbed, grabbing will restart from the beginning of the video or from the first file in the directory. To use a simulated digitizer, you must allocate a Host system, using MsysAlloc() with M_SYSTEM_HOST. Then, allocate a simulated digitizer on this system, using MdigAlloc() with M_EMULATED. Note that these allocations can be done with a single call, using MappAllocDefault() if a Host system and a simulated digitizer are set as the defaults at installation or using the MILConfig utility. This is actually the recommended way because it allows you to debug your code without significant modification. Although you can import an image (or a sequence of images), using MbufImport() or MbufImportSequence(), using a simulated digitizer to grab images grants access to common digitizer controls, inquires, hooks, and LUTs. Note that, when using a simulated digitizer, the X- and Y-size of each image in the path should be the same; otherwise, the X- and Y-size is taken from the first image and all subsequent images are either cropped to these dimensions, or expanded without zooming or stretching the original image, accordingly. In the latter case, portions of the previous image(s) might still be visible at the borders if the new image is smaller than the previous image. When allocating a simulated digitizer, you must specify to use either a pattern generator (a simulated DCF or SDCF) or the path to one or a series of static images or a video file. The video file or pattern generator is best used when testing the timing of your triggers and grabbing, whereas a series of static images is best used when testing specific problem cases. To learn the various settings available for your simulated digitizer, refer to the Host system column of the MIL command reference in the Mdig...() module. Note that, if you try to perform an operation using a simulated digitizer but the operation is not supported, an error or unpredictable behavior can occur. In the case where a simulated digitizer is being used to debug an application intended for another MIL system (for example, a Matrox Radient eV-CL or Matrox Clarity UHD), you should encapsulate any reference to hardware-specific control types or inquire types that are not supported by a simulated digitizer (such as camera lock sensitivity, timers, and I/Os) in an if statement. This allows your application to run without error even when using a simulated digitizer. bool UseClarityUHDSystem = MsysInquire(MilSystem, M_SYSTEM_TYPE, M_NULL) == M_SYSTEM_CLARITY_UHD_TYPE; if (UseClarityUHDSystem) { MdigControl(MilSystem, M_INPUT_FILTER, M_LOW_PASS_0); } Once you have finished using a simulated digitizer, you should free it, using MdigFree(). Simulated digitizer ",
      "wordCount": 550,
      "subEntries": []
    },
    {
      "id": "UG_grabbing_Using_MIL_with_GenICam",
      "version": null,
      "title": "Using MIL with GenICam",
      "subTitles": [
        "Basic concepts when dealing with GenICam",
        "Steps to view your GenICam-compliant camera's features",
        "Using GenICam with Camera Link cameras"
      ],
      "location": "MIL UG P04: 2D related information",
      "pageURL": "content\\UserGuide\\grabbing\\Using_MIL_with_GenICam.htm",
      "text": " Using MIL with GenICam MIL supports the use of GenICam when dealing with GigE Vision, CoaXPress (CXP), USB3 Vision, and Camera Link cameras (the latter might require third-party software). GenICam is an industry standard designed to provide a common software interface for machine vision cameras, and is administered by the European Machine Vision Association (EMVA). For more information on GenICam and the GenICam standard feature naming convention (SFNC), refer to the GenICam website at: http://www.genicam.org. For more information regarding which boards can connect to the above-mentioned cameras, refer to the MIL Hardware-specific Notes. Note that when dealing with Camera Link cameras, you must first install, identify, and enable the CLProtocol for your camera; to do so, refer to the Using GenICam with Camera Link cameras subsection of this section. Basic concepts when dealing with GenICam The following terms are common when dealing with GenICam. Camera's device description file (XML). An XML file on your camera that contains data used to identify the camera's features (such as, the feature name, type, and value). A copy of this XML file is also stored on your computer. The camera description file on your computer and the information in the camera are synchronized when the camera's features are inquired or written. For some cameras, some features (such as the camera's temperature), are also synchronized at a polling period with a regular, camera-defined interval. The polling period can be enabled or disabled using MdigControl() with M_GC_FEATURE_POLLING. Feature. A specific property of the camera (such as, the pixel depth) that can be controlled and/or inquired through the camera's device description file. Features are subdivided into several different types, such as features of type category (a group of features), command (an action), and features that store data (including features of type string, boolean, and double). Note that while the feature types that store data have a value that can be inquired, category type features and command type features do not. Steps to view your GenICam-compliant camera's features MIL provides several ways to access your GenICam-compliant camera's features. When dealing with an existing MIL program, or an application written to work regardless of the camera and frame-grabber combination, you can create a generalized application by using the traditional MIL control and inquire types (for example, those in MdigControl() and MdigInquire()) that are listed as supported for your MIL system). When dealing with an application written only for acquisition with GenICam-compliant cameras, you can access specific features of your camera directly using MdigControlFeature() and MdigInquireFeature(). Not all GenICam features can be accessed using MdigControl() and MdigInquire(). In MdigControl() and MdigInquire(), if a standard GenICam SFNC concept is similar to an existing MIL concept, the traditional MIL constant is used. All other constants that are used to access GenICam SFNC-specific features start with M_GC.... These constants are only supported with GenICam SFNC-compliant products. Your application can parse the camera's device description file (XML) and return a list of all the features of the camera, starting from the highest level feature of the XML structure (the root) or from a specified feature name (provided the specified feature name is a category containing additional features). To accumulate a complete list of the GenICam features available on your camera, inquire the list of features, by performing the following: Count the number of features in the camera's device description file at the highest-level feature of the XML structure (using MdigInquireFeature() with MIL_TEXT(\"Root\") and M_SUBFEATURE_COUNT). For each feature, inquire the feature's name (using M_SUBFEATURE_NAME + n), and feature type (using M_SUBFEATURE_TYPE + n). If the feature type is not a category or a command, inquire the feature's value (using M_FEATURE_VALUE). Print the result to the screen. Optionally, inquire additional attributes of each subfeature, using MIL_TEXT(\"FeatureName\"). Note that features that are commands and categories do not have a feature value (that is, M_FEATURE_VALUE); all other attributes of the feature can be inquired (for example, its description, size, tooltip, and type). Repeat this process for each category found, replacing Root with the name of the category. The following example lists the available features, starting from the root. enumfeatures.cpp Other ways to view the GenICam-compliant camera's features include: Opening Matrox Feature Browser using MdigControl() with M_GC_FEATURE_BROWSER. Opening Matrox Feature Browser from Matrox Capture Assistant. In Matrox Capture Assistant, select your camera from the tree of available cameras, and then click on the Feature Browser button . Opening Matrox Feature Browser from Matrox Intellicam. In Matrox Intellicam, select your camera from the drop-down list of available cameras, and then click on the Feature Browser button . Using GenICam with Camera Link cameras MIL supports using the GenICam CLProtocol to allow you to communicate with your Camera Link camera using the GenICam standard. Within your application, you must enable this functionality for each camera individually. If your camera is not GenCP-compliant, you must also first install the third-party, vendor-supplied, standard-complaint GenICam CLProtocol library for that camera. Note that using the GenICam CLProtocol will occupy your Camera Link connection's COM port until GenICam CLProtocol is disabled. To configure your digitizer to use the GenICam CLProtocol, you must select and enable the device ID used by the installed GenCP or third-party CLProtocol library to identify your Camera Link camera. To do so: Determine the number of device IDs supported by installed GenICam CLProtocol libraries (including GenCP), using MdigInquire() with M_GC_CLPROTOCOL_DEVICE_ID_NUM. Determine the maximum string length required to store the device IDs using MdigInquire() with M_GC_CLPROTOCOL_DEVICE_ID_SIZE_MAX. Generate a listing of the device IDs using MdigInquire() with M_GC_CLPROTOCOL_DEVICE_ID for each device ID. Using this listing, identify the device ID that matches your camera. If your camera is GenCP, select the device ID that matches the required GenCP version. Specify the appropriate device ID using MdigControl() with M_GC_CLPROTOCOL_DEVICE_ID with the device ID. Enable the GenICam CLProtocol for your camera by using MdigControl() with M_GC_CLPROTOCOL. The following example enumerates the device IDs supported by both the MIL-supplied and third-party GenICam CLProtocol libraries installed on your computer, and provides some additional information relating to each device ID to help you select the right one. In addition, it initializes GenICam feature values and opens a Feature Browser, populated with those values. It also inquires the camera's name and model (using MdigInquireFeature()) and prints this information to screen. clprotocol.cpp Once the GenICam CLProtocol is enabled for use with your Camera Link camera, you can use any of the GenICam interfaces to control and inquire your camera's settings. You can do this using MdigControlFeature() and MdigInquireFeature(). You can also use many MIL constants in MdigControl() and MdigInquire() (provided that these constants are marked as supported for your board). These MIL constants typically start with M_GC.... You can also use the Matrox Feature Browser using MdigControl() with M_GC_FEATURE_BROWSER, as described in the previous section. Alternatively, you can select the appropriate GenICam CLProtocol library for your camera using the MILConfig utility or Matrox Intellicam. In MILConfig, select the Boards Camera Link item in the tree structure of the MILConfig utility, and then select the Enable GenICam for Camera Link (CLProtocol) option from the presented page. A list of GenICam CLProtocol libraries and the device IDs they support are presented. In Matrox Intellicam, enable the Enable GenICam for Camera Link (CLProtocol) feature browser setting in System Options tab of the Preferences dialog (accessible using the Options Preferences menu item. When you access the Matrox Feature Browser in Intellicam a list of GenICam CLProtocol libraries and the device IDs they support are presented. Using MIL with GenICam Basic concepts when dealing with GenICam Steps to view your GenICam-compliant camera's features Using GenICam with Camera Link cameras ",
      "wordCount": 1264,
      "subEntries": []
    }
  ]
}]
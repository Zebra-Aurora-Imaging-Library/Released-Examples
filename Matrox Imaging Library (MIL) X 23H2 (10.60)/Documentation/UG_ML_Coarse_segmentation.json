[{
  "id": "UG_ML_Coarse_segmentation",
  "version": "2024020714",
  "title": "Segmentation",
  "subTitles": null,
  "location": "MIL UG P08: Machine learning tasks",
  "pageURL": "content\\UserGuide\\ML_Coarse_segmentation\\ChapterInformation.htm",
  "text": " Chapter 52: Segmentation This chapter explains how to perform segmentation using machine learning with the MIL Classification module. Segmentation overview Steps to perform segmentation Perform all required allocations Build and populate your dataset Train your classifier context Predict with your classifier Save your classification contexts Free your allocated objects Building a dataset for segmentation Add class definitions and entries to the source dataset context Add regions Splitting your dataset Augmentation and other data preparations Data preparation settings Augmenting images Exporting and importing a dataset Export Import Classifier and training settings for segmentation Training objects and folders Training related settings Training modes Class weights Training and analysis for segmentation Hook functions Results Training analysis IOU Confusion matrix Prediction for segmentation Prepare for prediction Predict Results Drawing results Assisted labeling Segmentation example ",
  "wordCount": 132,
  "subEntries": [
    {
      "id": "UG_ML_Coarse_segmentation_Segmentation_overview",
      "version": null,
      "title": "Segmentation overview",
      "subTitles": null,
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Coarse_segmentation\\Segmentation_overview.htm",
      "text": " Segmentation overview This chapter explains how to perform segmentation using machine learning with the MIL Classification module. Note that this chapter expands on topics previously discussed in Machine learning fundamentals. It is recommended to review these topics if you have not already done so. For the most recent documentation of this chapter, particularly as it relates to statistical analysis (MclassStatCalculate()), check for an updated version of the MIL Help online at zebra.com/aurora-imaging-library-help. Segmentation allows you to a classify regions within an image at a somewhat coarse pixel level; it typically requires a predefined segmentation classifier context that was defined by Matrox (a CSNet), and that must be trained with an images dataset context. To train a classifier, you must supply it with many images that are representative of the real-world problem the classifier will solve, along with a label identifying the class for each relevant region in the images. The classifier will learn from these training images how to differentiate the various classes, and let you predict the class of similar regions within similar images. An example of segmentation is detecting the presence of defects that are difficult to distinguish from the surfaces on which they occur, such as minor scratches or pits on steel, as shown here. Segmentation is often performed when the classification is based on a small area (for example, a defect like a small scratch), relative to the size of the entire scene, and you cannot otherwise delimit the area (for example, with fixturing). In such cases, using a local view of the data can improve the classification's robustness. Simply put, rather than being interested in the image as a whole, you are interesting in regions that can potentially occur in that image. You can use segmentation to predict the class to which regions within an entire image belong (multiple predicted class results per image). For example, rather than classifying the whole image as good or bad, you want to predict the class to which the defects within the image belong (such as Circle, Rectangle, or Triangle). Segmentation can also prove useful for other reasons, such as, if the approximate locations of the classes (for example, the defects) are important for the application. If you are only interested in locating regions, and you do not require the pixel information within, you might want to consider performing object detection. For more information, see Object detection versus segmentation. Segmentation overview ",
      "wordCount": 404,
      "subEntries": []
    },
    {
      "id": "UG_ML_Coarse_segmentation_Steps_to_perform_segmentation",
      "version": null,
      "title": "Steps to perform segmentation",
      "subTitles": [
        "Perform all required allocations",
        "Build and populate your dataset",
        "Train your classifier context",
        "Predict with your classifier",
        "Save your classification contexts",
        "Free your allocated objects"
      ],
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Coarse_segmentation\\Steps_to_perform_segmentation.htm",
      "text": " Steps to perform segmentation The following steps provide a basic methodology to perform segmentation with the MIL Classification module: Perform all required allocations. Build and populate your dataset. Train your classifier context. Predict with your classifier. If necessary, Save your classification contexts. Free your allocated objects. Perform all required allocations These allocations are required to perform segmentation. In many cases, some of these allocations will be imported or restored from previous work, using MclassImport() or MclassRestore(). Allocate a predefined segmentation classifier context, using MclassAlloc() with M_CLASSIFIER_SEG_PREDEFINED and M_CSNET_.... Note, MclassTrain() can automatically allocate a classifier context if none is specified. Allocate an images dataset context to hold all of your data (source dataset), using MclassAlloc() with M_DATASET_IMAGES. Allocate a segmentation training context, using MclassAlloc() with M_TRAIN_SEG. A training context holds the settings with which to train a classifier context. Allocate a segmentation training result buffer to hold training results, using MclassAllocResult() with M_TRAIN_SEG_RESULT. Allocate a segmentation result buffer to hold the prediction results, using MclassAllocResult() with M_PREDICT_SEG_RESULT. Object allocation for segmentation is summarized in the table below: MIL allocation Classifier context M_CLASSIFIER_SEG_PREDEFINED Specific predefined classifier context M_CSNET_... Dataset context M_DATASET_IMAGES Data preparation context M_PREPARE_IMAGES_SEG Training context M_TRAIN_SEG Training result buffer M_TRAIN_SEG_RESULT Prediction result buffer M_PREDICT_SEG_RESULT Build and populate your dataset It is recommended that you build and manage your datasets interactively, using MIL CoPilot. For example, MIL CoPilot lets you interactively create, label, modify, import, and export datasets. When using MIL CoPilot, ensure that you have installed all related MIL updates. For more information, see the Requirements, recommendations, and troubleshooting section of Chapter 47: Machine learning with the MIL Classification module. If you restore or import a dataset that is fully built, you can skip this step. If you do not use MIL CoPilot, you can follow the steps below to build and populate the source dataset: Add class definitions to the source dataset, using MclassControl() with M_CLASS_ADD. Note, the number of classes with which to categorize your data is a key decision to make when performing segmentation. Optionally, you can specify settings to help manage class definitions, using MclassControl(). For example, you can assign a color (M_CLASS_DRAW_COLOR) and an icon image (M_CLASS_ICON_ID) to class definitions. This allows you to draw and visually identify them with MclassDraw(). Add entries to the source dataset, using MclassControl() with M_ENTRY_ADD. Entries in an images dataset require image data (paths to where images are stored). To specify the location from which to get an entry's image data, use MclassControlEntry() with M_ENTRY_IMAGE_PATH. For each entry, add regions and specify the class definitions (ground truths) that are represented, using MclassEntryAddRegion() with the ClassIndexGroundTruth parameter. This step is known as labeling your data. You must label your data before training (labeled data is a prerequisite to using the module). The quality, quantity, and proportionality of correctly labeled data is critical to building a good dataset, and developing a properly trained classifier. Split your source dataset into the training, development, and testing datasets. You can split datasets manually, using MclassSplitDataset(), or you can pass a single dataset to MclassTrain(), and let MIL split it into the training and development datasets. If you want to use a testing dataset, you must first split a portion of the source dataset into a testing dataset using MclassSplitDataset(). Optionally, prepare your data by cropping or resizing images, and adding augmented images to a dataset. If you are letting MclassTrain() split your source dataset, data preparation is managed by the training contexts' internally defined data preparation context. You can prepare your data by calling MclassControl() with the identifier of the internal data preparation context (which you can inquire with M_PREPARE_DATA_CONTEXT_ID) and specifying the required data preparation controls (for example, M_PRESET_CROP and M_PRESET_NOISE_SALT_PEPPER). These preparations will be applied when you call MclassTrain(). You can use MclassPrepareData() to prepare data outside of MclassTrain(). To do this you must allocate a data preparation context (M_PREPARE_IMAGES_SEG). You should not use MclassPrepareData() on a dataset if you are letting MclassTrain() split the dataset. After you build a dataset, you can export it using MclassExport() so you can train a classifier with it at a later time. It is recommended that you export it with M_IMAGE_DATASET_FOLDER. You can use MclassImport() to import previously defined and exported datasets (for example, from a folder or a CSV file). It is recommended that you familiarize yourself with how dataset information (such as images) is stored on disk, and how you can ensure that information is well organized and portable. For more information, see the Guidelines for managing an images dataset section of Chapter 48: Datasets. Train your classifier context To train your classifier context on your datasets, perform the following: Modify training settings, using MclassControl() and MclassControlEntry(). Optionally, hook functions to training events, using MclassHookFunction(). Preprocess the training context, using MclassPreprocess(). Perform the training operation, using MclassTrain(). Optionally, get information about training events that caused the hook-handler function to execute, using MclassGetHookInfo(). If results indicate that the current training operation will be unsuccessful, stop the training, and modify your training settings and if necessary your dataset, and re-train. Optionally, get training results, using MclassGetResult(). As indicated in the previous step, if results indicate an unsuccessful training (for example, the classifier performed poorly on the development dataset), modify your training settings and if necessary your dataset, and re-train. Copy the classification result buffer that MclassTrain() produced into a classifier context, using MclassCopyResult(). Once copied, the classifier context is considered trained. If your training results are unsatisfactory, adjust training settings, contexts, and datasets as required, and call MclassTrain() with the trained classifier context. Predict with your classifier To predict with the trained classifier context, perform the following: Optionally, set the target image size, using MclassControl() with M_TARGET_IMAGE_SIZE_X and M_TARGET_IMAGE_SIZE_Y. Preprocess the trained classifier context, using MclassPreprocess(). Perform the prediction operation with the trained classifier context and the target data that you want to classify, using MclassPredict(). If your training images were prepared, then the target image must also be prepared in the same way. This maintains consistency between the training data and the target data. If you are predicting with a test dataset and the predicted classes are not what you expect, when compared to the actual classes in the test dataset (the ground truth), you can adjust your training setup, and continue the training process. Optionally, you can perform the prediction operation with a dataset as your target. For segmentation, you must set the M_SEGMENTATION_FOLDER control for the target dataset. Segmentation prediction scores are saved at this location. When predicting with a dataset as your target, you can hook functions to prediction events, using MclassHookFunction(). Retrieve the required results from the prediction result buffer, using MclassGetResult(). You can also draw prediction results, using MclassDraw(). Save your classification contexts If necessary, save your classification contexts, using MclassSave() or MclassStream(). Free your allocated objects Free all your allocated objects, using MclassFree(), unless M_UNIQUE_ID was specified during allocation. Steps to perform segmentation Perform all required allocations Build and populate your dataset Train your classifier context Predict with your classifier Save your classification contexts Free your allocated objects ",
      "wordCount": 1183,
      "subEntries": []
    },
    {
      "id": "UG_ML_Coarse_segmentation_Building_a_dataset_for_segmentation",
      "version": null,
      "title": "Building a dataset for segmentation",
      "subTitles": [
        "Add class definitions and entries to the source dataset context",
        "Add regions",
        "Splitting your dataset",
        "Augmentation and other data preparations",
        "Data preparation settings",
        "Augmenting images",
        "Exporting and importing a dataset",
        "Export",
        "Import"
      ],
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Coarse_segmentation\\Building_a_dataset_for_segmentation.htm",
      "text": " Building a dataset for segmentation This section discusses specific options for building a dataset for segmentation. For an overview of the many considerations you should make when building a dataset, see Chapter 48: Datasets. It is recommended that you use MIL CoPilot to create, label, modify, augment, and export your dataset interactively. Add class definitions and entries to the source dataset context Entries in an images dataset for segmentation have multiple regions (images), which are typically much smaller than the target image being predicted. Each region identifies a class. Although you can use an images dataset for both image classification, segmentation, and object detection simultaneously, it is generally recommended to have a dataset that is exclusively for one task or the other. To add class definitions to the source dataset, call MclassControl() with M_CLASS_ADD. These are the classes that will be represented in your dataset. Optionally, you can specify settings to help manage class definitions, using MclassControl(). For example, you can assign a color (M_CLASS_DRAW_COLOR) and an icon image (M_CLASS_ICON_ID) to class definitions. This allows you to draw and visually identify them with MclassDraw(). Add entries to the source dataset, using MclassControl() with M_ENTRY_ADD. Every entry must refer to an image, along with a label identifying the class for each region in the image. To specify the location from which to get an image entry's data, use MclassControlEntry() with M_ENTRY_IMAGE_PATH. Add regions To add regions to an entry in an images dataset context, call MclassEntryAddRegion(). Each time you call this function, one or more regions (classes) are added to the specified entry. Entry regions must properly and proportionally represent all the different classes (including the background class). For regions that are defined by images, the region image must be the same size as the entry image. When adding regions based on images, region masks are saved on disk at the location specified by M_REGION_MASKS_FOLDER. You must first set this location for the dataset context using MclassControl(). The number and type of regions that are added is determined by the specified region's descriptor. Typically, you would add one or more regions from a mask image (M_DESCRIPTOR_TYPE_MASK). You can also add regions by specifying polygons in a graphics list (M_DESCRIPTOR_TYPE_POLYGON), or the ground truth indices or colors in an image (M_GROUND_TRUTH_IMAGE or M_GROUND_TRUTH_IMAGE_COLOR). Each added region must have a ground truth (class) assigned to it (just like each entry image for image classification has a ground truth). For example, when using mask regions (M_DESCRIPTOR_TYPE_MASK), the pixels in the entry image that correspond to the non-zero pixels (masked) in the specified mask (image) are considered part of the ground truth, which you must set with the ClassIndexGroundTruth parameter. The table below illustrates the components of an image dataset for segmentation. Entry Image Region Label Entry 0 Image 0 Region 0 Class 0 Region 1 Class 1 Region 2 Class 2 Entry 1 Image 1 Region 0 Class 0 Region 1 Class 1 Entry 2 Image 2 Region 0 Class 0 Region 1 Class 2 To indicate the masked regions, it is recommended to brush over the pixels that represent the classes. MIL CoPilot provides this functionality (you can also use any simple graphics editor to do this). Conventionally, the brushing color should represent the class label. For example, if your DefectPit class is Class1 and your DefectScratch class is Class2, you should identify them using a color value of 1 and 2. Once you have brushed the pixels, you can save a version of these images where every brushed pixel value is maintained, and every other pixel value (not brushed) is considered region-less. You can specify whether you want to interpret these region-less values as having no class or being part of a specific class by calling&nbsp; MclassControl() with M_NO_REGION_PIXEL_CLASS. If you want them to represent the background, then you would set M_NO_REGION_PIXEL_CLASS to the class representing the background. Conventionally, the background class label is 0. All pixels in an entry image must be labeled, and each pixel can only belong to one class. You can control and inquire about regions, by calling MclassControlEntry() and MclassInquireEntry(). To draw region information, call MclassDrawEntry(). When you are finished adding your data, you should ensure that all of the images in your source dataset are the same size. This is a requirement for training a segmentation classifier. To do this, call MclassPrepareData() and specify your source dataset context. To specify how to resize images, use the M_SIZE_MODE and M_RESIZE_SCALE_FACTOR controls. Resizing can be performed automatically or with explicit image width and height values. Note, that using smaller images will allow for faster training. Splitting your dataset You can call MclassSplitDataset() to split a dataset into two smaller datasets. You can do this to create a training, development, or testing dataset out of your source dataset. If you want to use a testing dataset, then you should first split a portion of the source dataset into a testing dataset by calling MclassSplitDataset(). For more information about the different datasets, see the Different datasets and how they are split section of Chapter 48: Datasets. Augmentation and other data preparations Optionally, prepare your data and add augmented images to a dataset using MclassPrepareData(). You can call MclassPrepareData() with an images dataset, or an individual image. You should not use MclassPrepareData() on a dataset if you are letting MclassTrain() split the dataset, since MIL will automatically augment your data with the internally defined data preparation context. You can call MclassControl() with the identifier of the internal data preparation context (which you can inquire with M_PREPARE_DATA_CONTEXT_ID) and specify the required data preparation controls (for example, M_PRESET_CROP and M_PRESET_NOISE_SALT_PEPPER). These preparations will be applied when you call MclassTrain(). For more information, see the Data augmentation and other data preparations section of Chapter 48: Datasets. Data preparation settings MclassPrepareData() requires that you allocate a data preparation context by calling MclassAlloc() with M_PREPARE_IMAGES_SEG. The data preparation context holds the settings with which to modify the specified source (the images in a dataset or an individual image). Preprocess the data preparation context by calling MclassPreprocess(). You must also specify the location in which prepared images will be stored by setting M_PREPARED_DATA_FOLDER. For more information, see the Data augmentation and other data preparations section of Chapter 48: Datasets. Augmenting images Some augmentations will affect the regions and labels for the image. For example, if you rotate an image, the label masks will also be rotated. In some cases, this will add M_DONT_CARE_CLASS regions. This is done automatically during augmentation. The rotation augmentation shown below adds white regions near the perimeter of the image, which will not contribute to training loss. Augmented entries, and the entries used to augment them, must only be in the training dataset. Augmented entries in the development dataset can cause errors. For more information, see the Data augmentation and other data preparations section of Chapter 48: Datasets. Exporting and importing a dataset Exporting and importing your datasets allows you to make your datasets portable and organized. This will allow you to easily reuse or combine your datasets. For more information, see the Guidelines for managing an images dataset section of Chapter 48: Datasets. Export After you build a dataset, export it using MclassExport(). It is recommended that you export it with M_IMAGE_DATASET_FOLDER to create an organized folder than can be easily imported and reused. Import You can use a folder or CSV file to define data for a dataset. To import it to a dataset context, use MclassImport() and specify what to import (for example, M_COMPLETE, M_ENTRIES, M_AUTHORS, and M_CLASS_DEFINITIONS). For more information about importing data, see the Importing data from a folder or CSV file section of Chapter 48: Datasets. You can also use MclassRestore() to restore a dataset that was previously saved to a file using MclassSave() or MclassStream(). Building a dataset for segmentation Add class definitions and entries to the source dataset context Add regions Splitting your dataset Augmentation and other data preparations Data preparation settings Augmenting images Exporting and importing a dataset Export Import ",
      "wordCount": 1338,
      "subEntries": []
    },
    {
      "id": "UG_ML_Coarse_segmentation_Classifier_and_training_settings_for_segmentation",
      "version": null,
      "title": "Classifier and training settings for segmentation",
      "subTitles": [
        "Training objects and folders",
        "Training related settings",
        "Training modes",
        "Class weights"
      ],
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Coarse_segmentation\\Classifier_and_training_settings_for_segmentation.htm",
      "text": " Classifier and training settings for segmentation Before you can start training, you need to allocate a classifier context and training objects, and set training related settings. For more information on training settings, see the Fundamental decisions and settings section of Chapter 49: Training. Training objects and folders Allocate a training context, using MclassAlloc() with M_TRAIN_SEG. A training context holds the settings with which to train a classifier context, such as training modes. You will also need to allocate a training result buffer to hold training results, using MclassAllocResult() with M_TRAIN_SEG_RESULT. Set the destination folder that will store prepared images and segmentation scores using MclassControl() with M_TRAIN_DESTINATION_FOLDER. Training related settings Set your training related settings as required. Once you have established your training settings for your training context, you must preprocess the context by calling MclassPreprocess() with the identifier of the training context. Training modes Depending on the problem definition, different training modes exist for your classifier. To set these training modes for your training context, call MclassControl() with M_RESET_TRAINING_VALUES. Complete (M_COMPLETE). Typically, this is for completely restarting the training of a CNN or segmentation classifier context, or for training a CNN or segmentation classifier context that is not trained. Transfer learning (M_TRANSFER_LEARNING). Typically, this is for a CNN or segmentation classifier context that was already trained on a specific classification problem, and that you must train on a similar (but new) problem. Fine tuning (M_FINE_TUNING). Typically, this is for a CNN or segmentation classifier context that was already trained on a specific classification problem, and that you must train with additional data. When you specify a training mode, MIL automatically sets the related training mode controls to the required settings. You can also modify these controls yourself to adjust the training process by calling MclassControl(). The training mode controls let you adjust the: Learning rate (M_INITIAL_LEARNING_RATE and M_LEARNING_RATE_DECAY). For segmentation, the default learning rate is 0.001 and the default learning rate decay is 0.05. Maximum number of epochs (M_MAX_EPOCH). For segmentation, the default maximum number of epochs is 60. Mini-batch size (M_MINI_BATCH_SIZE). For segmentation, the default mini-batch size is 4. Schedule type (M_SCHEDULER_TYPE). For segmentation, the default schedule type is, M_CYCLICAL_DECAY. Such training mode controls are also known as hyperparameters. If your classifier is not performing as expected, you can adjust the hyperparameters and retrain. Class weights Class weights allow you to influence your classifier's training by placing more or less importance (weight) on certain class definitions. Class weights are a particularly useful setting in segmentation, because there are multiple classes represented in each image. In an image, 90% of the pixels might represent class 0 (the background class), and only 10% represent class 1 (the object of interest). Without setting class weights, the classifier could learn to always predict class 0 and be correct 90% of the time, but this would be a useless classifier. You can control how the weights are set in your training context by calling MclassControl() with M_CLASS_WEIGHT_MODE. The default setting is M_INVERSE_CLASS_FREQUENCY, which will set weights for each class such that the classifier will place more weight on the less frequent classes (class 1). The strength of this effect is controlled by M_CLASS_WEIGHT_STRENGTH. This might make your classifier less accurate than the \"always predict class 0\" method, but it will be much more useful for predicting class 1 (your objects of interest). If you need more control over the class weights, you can manually set the weight factor for each class using M_USER_DEFINED and M_CLASS_WEIGHT. For more information, see the Class weights subsection of the Fundamental decisions and settings section of Chapter 49: Training. Classifier and training settings for segmentation Training objects and folders Training related settings Training modes Class weights ",
      "wordCount": 619,
      "subEntries": []
    },
    {
      "id": "UG_ML_Coarse_segmentation_Training_and_analysis_for_segmentation",
      "version": null,
      "title": "Training and analysis for segmentation",
      "subTitles": [
        "Hook functions",
        "Results",
        "Training analysis",
        "IOU",
        "Confusion matrix"
      ],
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Coarse_segmentation\\Training_and_analysis_for_segmentation.htm",
      "text": " Training and analysis for segmentation When you call MclassTrain(), the classifier context will be trained with the settings specified in the training context, and trained on the data in the training and development datasets. The results from training will be stored in the classification result buffer. These must all be specified when you call MclassTrain(). For more information on the training process, and training analysis, see the Analysis, adjustment, and additional settings section of Chapter 49: Training. Hook functions You can save time and improve the training process by using MclassHookFunction() to hook a function to a training event. Call MclassGetHookInfo() to get information about the event that caused the hook-handler function to be called. Training can take a long time, and you can use hook functions to monitor the training process. It is recommended that you call a hook function at the end of each mini-batch (M_MINI_BATCH_TRAINED), and also at the end of each epoch (M_EPOCH_TRAINED), to ensure that training is developing in the correct direction. While your classifier is training, you can get the information from the events that caused the hook-handler function to execute by using MclassGetHookInfo(). Typically, you should expect to monitor the training process for proper convergence, and to make modifications to the process. You might need to abort or restart it, if required. Results After training is completed, retrieve your training results by calling MclassGetResult() with the training result buffer that MclassTrain() produced. You will often get results related to accuracy, such as the accuracy of the training dataset (M_TRAIN_DATASET_ACCURACY) and the development dataset (M_DEV_DATASET_ACCURACY). You can also get training results from a specific entry in either the training or development dataset by calling MclassGetResultEntry(). To do this, you must first copy the dataset results from the result buffer that MclassTrain() produces to a dataset using MclassCopyResult(). If your training results are not as you expected (for example, if the classifier is clearly performing poorly by over-fitting to the development dataset), then you can make adjustments and train again. Copy the classification result buffer into a classifier context, using MclassCopyResult() with M_TRAINED_CLASSIFIER. Once copied, the classifier context is considered trained. If necessary, you can always continue to adjust training settings and contexts, and call MclassTrain() with the trained classifier context. Training analysis Analyzing your training results is critical to building a well-performing classifier. IOU IOU (Intersection over union) is a metric for evaluating the performance a segmentation classifier. It compares the area and location of the predicted region to the ground truth region, relying on two concepts: Intersection is the area of overlap between the predicted and ground truth regions. Union is the total area covered by the combined predicted and ground truth regions. Intersection and union are demonstrated in the following image: The IOU is equal to the intersection divided by the union. In MIL, this value is given as a percentage from 0 (the two regions have no overlap) to 100 (the two regions have complete overlap and are the same size). Ideally, the IOU should be as close to 100 as possible. You can call MclassGetResult() to get the mean IOU after each epoch for the training and development datasets. The mean IOU can also be retrieved with MclassGetHookInfo(). Confusion matrix For segmentation, the confusion matrix tells you how many pixels were correctly and incorrectly classified during training. It will include all pixels that are assigned to a region that is not M_NO_CLASS or M_DONT_CARE_CLASS. The confusion matrix is available by calling MclassGetResult() with M_TRAIN_DATASET_CONFUSION_MATRIX or M_DEV_DATASET_CONFUSION_MATRIX. You can use the confusion matrix to identify classes that are poorly predicted in your datasets. The IOU can also be calculated explicitly from a confusion matrix. Training and analysis for segmentation Hook functions Results Training analysis IOU Confusion matrix ",
      "wordCount": 628,
      "subEntries": []
    },
    {
      "id": "UG_ML_Coarse_segmentation_Prediction_for_segmentation",
      "version": null,
      "title": "Prediction for segmentation",
      "subTitles": [
        "Prepare for prediction",
        "Predict",
        "Results",
        "Drawing results",
        "Assisted labeling"
      ],
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Coarse_segmentation\\Prediction_for_segmentation.htm",
      "text": " Prediction for segmentation MclassPredict() uses a trained classifier context to make class predictions on a target. For a trained segmentation classifier, your target is either an image or a dataset of images. For more information about prediction, see the Prediction settings, results, and drawings section of Chapter 50: Prediction. Prepare for prediction Allocate a classification result buffer to hold the prediction results, using MclassAllocResult() with M_PREDICT_SEG_RESULT. When predicting with a segmentation classifier, the target image size can be smaller or bigger than the images used to train the classifier. Some CSNet classifiers perform better with smaller or bigger images, so you should set the target image size accordingly by calling MclassControl() with M_TARGET_IMAGE_SIZE_X and M_TARGET_IMAGE_SIZE_Y. When predicting with a dataset as your target, you must set the control M_SEGMENTATION_FOLDER for the target dataset. Segmentation prediction scores are saved at this location. Before predicting, preprocess your trained classifier context using MclassPreprocess(). Predict Perform the prediction operation with the trained classifier context and the target data that you want to classify using MclassPredict(). If your training images were prepared by cropping and resizing, then the target image must also be prepared in the same way. This maintains consistency between the training data and the target data. By default, MIL assumes that the target images are the same size as the training images, which you can retrieve using MclassInquire() with M_SIZE_X and M_SIZE_Y. The target images can also be smaller or bigger than the images used to train your classifier. This requires calling MclassControl() with M_TARGET_IMAGE_SIZE_X and M_TARGET_IMAGE_SIZE_Y. Typically, cropping is the only valid method for predicting on different sized images. Resized images should not be used for prediction unless your classifier was trained using similarly resized images. When performing the prediction operation with a dataset as your target, you can hook functions to prediction events, using MclassHookFunction(). Results Retrieve the required results from the classification result buffer, using MclassGetResult(). You can also get results from a specific entry in a dataset using MclassGetResultEntry(). Typically, the most important prediction results to retrieve are the best predicted class (M_BEST_CLASS_INDEX) and its score (M_BEST_CLASS_SCORE). Drawing results To draw prediction results, call MclassDraw(). You can also draw prediction results from a specific entry in a dataset using MclassDrawEntry(). You can perform drawing operations to, for example, illustrate the best class result (M_DRAW_BEST_INDEX_IMAGE or M_DRAW_BEST_INDEX_CONTOUR_IMAGE) or the resulting class scores (M_DRAW_BEST_SCORE_IMAGE and M_DRAW_CLASS_SCORES). To draw the icon image related to the class, use M_DRAW_CLASS_ICON. Note, this image is not a result; you specify it using MclassControl() with M_CLASS_ICON_ID, and you draw it from a classifier context or a dataset context (not a result buffer). By drawing the class' icon image, you are able to visually identify the class for which you are getting results. Similarly, you can also specify and draw a color related to a class, to help visually identify it (M_DRAW_CLASS_COLOR_LUT). Prediction results for segmentation can prove complex to decipher, given that you can have multiple results for each training image. For example, a segmentation prediction can identify two classes in the following target image. By calling MclassDraw(), you can draw a contour around the classes found, as well as the corresponding color of every class, including the background. Such drawings help you better understand the results of the prediction. To perform these drawing operations, specify M_DRAW_BEST_INDEX_CONTOUR_IMAGE + M_PSEUDO_COLOR (for the image on the left) and M_DRAW_BEST_INDEX_IMAGE + M_PSEUDO_COLOR (for the image on the right). Assisted labeling You can perform assisted labeling for image classification by adding prediction results to your dataset. For more information about assisted labeling, see the Assisted labeling subsection of the Advanced techniques section of Chapter 50: Prediction. Prediction for segmentation Prepare for prediction Predict Results Drawing results Assisted labeling ",
      "wordCount": 621,
      "subEntries": []
    },
    {
      "id": "UG_ML_Coarse_segmentation_Segmentation_example",
      "version": null,
      "title": "Segmentation example",
      "subTitles": null,
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Coarse_segmentation\\Segmentation_example.htm",
      "text": " Segmentation example The example ClassSegmentationCompleteTrain.cpp demonstrates how to train a predefined (Matrox defined) segmentation classifier context to detect the presence of scratch and pit defects on the surface of steel. To run this example, use the Matrox Example Launcher in the MIL Control Center. To view this example, refer to the following: classsegmentationcompletetrain.cpp Segmentation example ",
      "wordCount": 57,
      "subEntries": []
    }
  ]
}]
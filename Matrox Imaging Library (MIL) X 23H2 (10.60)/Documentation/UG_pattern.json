[{
  "id": "UG_pattern",
  "version": "2024020714",
  "title": "Pattern matching",
  "subTitles": null,
  "location": "MIL UG P03: 2D processing and analysis",
  "pageURL": "content\\UserGuide\\pattern\\ChapterInformation.htm",
  "text": " Chapter 7: Pattern matching This chapter explains how to use the MIL Pattern Matching module to perform normalized grayscale correlation (NGC) pattern matching. Pattern matching - in general Steps to performing a pattern search Basic concepts for the MIL Pattern Matching module Defining and adding models to your Pattern Matching context Automatic models Regular models Drawing models Finding models when they are at an angle Setting the angle of search Determining the rotation tolerance of a model Masking the model Search constraints Specifying the search mode Specifying the number of matches Setting the acceptance level Setting the certainty level Redefining the model's reference position Selecting the search region Positional accuracy Selecting the speed setting Preprocessing the Pattern Matching context Speeding up the search Choose the appropriate model Adjust the search speed setting Effectively choose the search region and search angle Pattern matching algorithm (for advanced users) Normalized correlation Hierarchical search Search region Search heuristics Subpixel accuracy Pattern matching examples ",
  "wordCount": 161,
  "subEntries": [
    {
      "id": "UG_pattern_Pattern_matching_in_general",
      "version": null,
      "title": "Pattern matching - in general",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\pattern\\Pattern_matching_in_general.htm",
      "text": " Pattern matching - in general To find occurrences of patterns (models) in your target image, you can use the MIL Pattern Matching module. This module finds the degree of similarity between the specified model and the neighborhood of the pixels in the target image, on a pixel-by-pixel basis, using normalized grayscale correlation (NGC). The module then returns the position of pixels whose neighborhoods have the highest degree of similarity and meet other specified criteria. When dealing with non-rotated models or models that have broken edges, the Pattern Matching module offers a faster search than the MIL Model Finder module (Mmod...()). The Pattern Matching module allows you to search for any number of different models simultaneously, through a range of angles, but only when using 8-bit, grayscale, unsigned image buffers. The module provides some support for camera calibration. Although the results are calculated in pixels, if the target image has been associated with a camera calibration context, positional results can be returned in calibrated real-world units. The module also allows you to restore a Pattern Matching context from a file or memory stream, or save a Pattern Matching context to a file or memory stream. Pattern matching - in general ",
      "wordCount": 201,
      "subEntries": []
    },
    {
      "id": "UG_pattern_Steps_to_performing_a_pattern_search",
      "version": null,
      "title": "Steps to performing a pattern search",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\pattern\\Steps_to_performing_a_pattern_search.htm",
      "text": " Steps to performing a pattern search The following steps provide a basic methodology for using the MIL Pattern Matching module: Allocate a Pattern Matching context, using MpatAlloc(). Load or grab an image from which to define a model. This image is known as the model's source image. This must be an 8-bit unsigned grayscale image that should be (or contain) a good example of a typical model. Define and add your model to this Pattern Matching context, using MpatDefine() with the model's source image. Optionally, repeat the two previous steps as many times as necessary to grab, define, and add your models to this Pattern Matching context. All models in the same context must be the same size and have the same resolution, but models can come from different model's source images. Optionally, mask any irrelevant areas of the model using MpatMask(). Optionally, specify your required search settings for both the context and the individual model(s), using successive calls to MpatControl(). When searching for several models, the search settings can either be set for the first model (when M_SEARCH_MODE is set to M_FIND_BEST_MODELS) or in each model (when M_SEARCH_MODE is set to M_FIND_ALL_MODELS). Note that, with M_FIND_BEST_MODELS, all models must be of the same size, and with M_FIND_ALL_MODELS, all models must use the same search region. Preprocess the Pattern Matching context, using MpatPreprocess(). Allocate a result buffer to store the results of your search, using MpatAllocResult(). Grab a target image. Optionally, process it to improve its quality. Search the target image for occurrences of the models in your Pattern Matching context, using MpatFind(). Retrieve the required results from the result buffer, using MpatGetResult(). You can use MpatDraw() to draw the occurrences' bounding box and/or a cross at the occurrences' reference position. The coordinates resulting from a search return the reference position of the model, relative to the center of the top-left pixel of the target image. To determine what were the equivalent coordinates of the model's reference position in the model's source image, use MpatInquire() with M_ORIGINAL_X and M_ORIGINAL_Y. Optionally, save your Pattern Matching context, using MpatSave() or MpatStream(). Free all your allocated Pattern Matching objects, using MpatFree(), unless M_UNIQUE_ID was specified during allocation. In general, the first seven steps are performed once, while steps 8 through 11 are repeated as required. Note, in practice, models are usually saved on disk, using MpatSave(); therefore steps 1 through 6 are often replaced by a single step that restores a saved model from disk, using MpatRestore(). Steps to performing a pattern search ",
      "wordCount": 422,
      "subEntries": []
    },
    {
      "id": "UG_pattern_Basic_concepts",
      "version": null,
      "title": "Basic concepts for the MIL Pattern Matching module",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\pattern\\Basic_concepts.htm",
      "text": " Basic concepts for the MIL Pattern Matching module The basic concepts and vocabulary conventions for the MIL Pattern Matching module are: Normalized grayscale correlation (NGC). A technique that finds a pattern by looking for a similar spatial distribution of intensity. Model image. The model, when drawn, is a model image. Model occurrence. The model, when found in a target image. Model's source image. The image from which the model is defined/extracted. Reference position. A point that is relative to the model origin, that indicates which position to return when an occurrence of the model is found in the image. The position of an occurrence is the model's reference position transformed at the model occurrence (that is, relative to the target image's origin). The reference position does not have to be in the model, but it is, by default, located at the model's center. Search region. The area in which to find the model's reference position of a model occurrence within the target image. Target image. The image being searched. Basic concepts for the MIL Pattern Matching module ",
      "wordCount": 179,
      "subEntries": []
    },
    {
      "id": "UG_pattern_Defining_a_model",
      "version": null,
      "title": "Defining and adding models to your Pattern Matching context",
      "subTitles": [
        "Automatic models",
        "Regular models",
        "Drawing models"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\pattern\\Defining_a_model.htm",
      "text": " Defining and adding models to your Pattern Matching context Once you have allocated a Pattern Matching context, using MpatAlloc(), you can begin to add models to your Pattern Matching context, using MpatDefine(). There are two types of models: models defined from a specified location in the model's source image, and models that MIL automatically defines from the model's source image. These models can be mixed in the same Pattern Matching context, using MpatDefine(). Models of both types should be distinct and not defined from a flat region (consistent pixel values with no edges present). Also, the model's source image should be of the best quality possible. The model can be defined when it is at an angle. When defining multiple models, they must be the same size and at the same resolution. If noisy, try to clean the image, using the image processing techniques discussed elsewhere in this user guide. Automatic models To have MIL define one or more distinct models from a model's source image, use MpatDefine() with M_AUTO_MODEL. Specify the width and height of the model(s), as well as the maximum displacement expected between the position of the model in the source image and the position of the model when found in the target image. These four measurements define an area in which MIL expects to find a model in the source image. The reference position (used to identify the start of the area) is placed at half the width and height of the extracted model. To change the reference position, use MpatControl() with M_REFERENCE_X and M_REFERENCE_Y. You can only define M_AUTO_MODEL models that respect the following condition: (MaxPixelValue 2 * ModelWidth x ModelHeight) &lt; 2 63 , where MaxPixelValue is the maximum pixel value (typically, 255) in the target image and the model. This restriction is imposed to avoid overflows in the internal 64-bit accumulators. The total area of the model must be greater or equal to 4 pixels. Regular models To define a model from a specified location in a model's source image, use MpatDefine() with M_REGULAR_MODEL; this is referred to as a regular model. To create a regular model, pass M_REGULAR_MODEL, the model's required offset from the top-left corner of the model's source image, width, and height. A regular model is typically used in cases where the target image contains an inconsistent surrounding (such as an image of loose nuts and bolts lying on a metal sheet). If the eventual model can appear at an angle, enable angular search, using MpatControl() with M_SEARCH_ANGLE_MODE and set the range of angles at which it can appear using M_SEARCH_ANGLE.... If the model has a background that will be consistently found in the target image (such as when searching for a chip on an integrated circuit board), combine M_REGULAR_MODEL with M_CIRCULAR_OVERSCAN when calling MpatDefine(). To define a model from a rotated region of the model's source image, you can specify a source image that is associated with a rotated rectangular ROI (that is, a rectangular ROI oriented at a non-zero angle). To define the ROI, add a rectangle at the required angle to a 2D graphics list, using MgraRectAngle(). To set that rectangle as the ROI of an image buffer, call MbufSetRegion() with M_RASTERIZE or M_NO_RASTERIZE and with M_FILL_REGION. Drawing models Upon definition, the model is extracted from the selected region in the model's source image buffer and stored in a non-displayable model buffer. The model's source image buffer is then no longer needed. To view the portion of the image from which the model was extracted (referred to as the model image), use MpatDraw() with M_DRAW_IMAGE. MpatDraw() can also draw the various model features (such as M_DRAW_POSITION). Use the ControlFlag parameter to specify where in the destination buffer to draw the model information. You can start drawing at the top-left pixel of the destination buffer (M_DEFAULT) or at the same offset as was used to extract the model from the model's source image buffer (M_ORIGINAL). You can use a previously allocated 2D graphics context (see Chapter 26: Generating graphics) to control the drawing color, or use the default 2D graphics context (M_DEFAULT). You can also choose to draw in the display's overlay buffer. By drawing into the display's overlay buffer, you can annotate an image non-destructively. For more information, see Chapter 25: Displaying an image. Defining and adding models to your Pattern Matching context Automatic models Regular models Drawing models ",
      "wordCount": 734,
      "subEntries": []
    },
    {
      "id": "UG_pattern_Rotation",
      "version": null,
      "title": "Finding models when they are at an angle",
      "subTitles": [
        "Setting the angle of search",
        "Determining the rotation tolerance of a model"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\pattern\\Rotation.htm",
      "text": " Finding models when they are at an angle There are two ways to search for a model that can appear at different angles using the MpatFind() function: Define and then search for a rotated model. Search for models taken from the same region in rotated images. To implement the first type of search, define a model using MpatDefine() with M_REGULAR_MODEL or M_AUTO_MODEL. Then, enable the angular search using MpatControl() with M_SEARCH_ANGLE_MODE and set the angular range using M_SEARCH_ANGLE.... When you call MpatPreprocess(), it will internally create different models by rotating the original model at the required angles, assigning \"don't care\" pixels to regions that do not have corresponding data in the original model. You should implement this technique when the pixels surrounding the model follow no predictable pattern (for example, when searching for loose nuts and bolts lying on a conveyor). To implement the second type of search, define a regular or automatic model using MpatDefine() with M_REGULAR_MODEL + M_CIRCULAR_OVERSCAN or MpatDefine() with M_AUTO_MODEL + M_CIRCULAR_OVERSCAN, respectively; this will extract the model as well as circular overscan data from the model's source image (specifically, MIL extracts the region enclosed by a circle which circumscribes the model). Then, enable angular search using MpatControl() with M_SEARCH_ANGLE_MODE and set the angular range using M_SEARCH_ANGLE.... When you call MpatPreprocess(), it will extract different orientations of the model from the overscanned model. This technique should only be used when the model's distinct features lie in the center of the region, so that they are included in all models when rotated. Therefore, it is recommended that a M_CIRCULAR_OVERSCAN model be as square as possible: the longer the rectangle, the smaller the number of consistent central pixels in every model. As mentioned, a larger region than the one defined will be fetched from the model's source image. Therefore, the model must not be extracted from a region too close to the edge of the model's source image. The pixels surrounding the model should be relevant to the positioning of the pattern (that is, the model should appear in the target image with the same overscan data). For example, taking a model from an integrated circuit board, you can expect that the model will always be found in the same area of the board, with the same background. Both techniques find the position and match score of the model in a target image. Finally, it should be noted that MIL's implementation of MpatFind() with an M_CIRCULAR_OVERSCAN type model is significantly faster than that of a model without circular overscan when performing an angular search. Setting the angle of search By default, the nominal angle of the search is 0°. However, using MpatControl() with M_SEARCH_ANGLE_MODE enabled, you can then change the starting angle (using M_SEARCH_ANGLE) and specify an angular range (M_SEARCH_ANGLE...). You can also specify the required precision of the resulting angle and the interpolation mode used to rotate the model. These settings can influence the speed of the search significantly. When an angular range has been specified using MpatControl() with M_SEARCH_ANGLE..., MpatPreprocess() creates a model for every x degrees within the range, where x is determined by the specified rotational tolerance (M_SEARCH_ANGLE_TOLERANCE). Rotational tolerance defines the full range of degrees within which the pattern in the target image can be rotated from a model at a specific angle and still meet the acceptance level. The rotational tolerance can also be automatically determined according to the angular correlation of the model with versions of itself at specific angles when the model is preprocessed; to do so, set M_SEARCH_ANGLE_TOLERANCE to M_AUTO. Note that M_SEARCH_ANGLE_INTERPOLATION_MODE sets the type of interpolation to use when rotating the model. After the approximate location is found, MIL fine-tunes the search, according to the specified angle accuracy (M_SEARCH_ANGLE_ACCURACY). It searches within one rotational tolerance before and after the approximate location, at an angle refinement step determined by the specified angle accuracy. The greater the angle refinement step, the faster the search; however, your results will be less accurate. Note that you must set the angle accuracy (angle refinement step) to a value smaller than that of the rotation tolerance. If you don't know the best search angle accuracy to set, you can have MIL automatically select the angle refinement step. To do so, set M_SEARCH_ANGLE_ACCURACY to M_DISABLE. Then, enable angle refinement mode and set the acceptable minimum score for a rotated model, both by setting MpatControl() with M_ROTATED_MODEL_MINIMUM_SCORE to the minimum acceptable score. Angle refinement mode improves accuracy without unnecessarily sacrificing performance. Similar to automatically determining rotational tolerance, during preprocessing, angle refinement mode determines the full range of degrees within which a rotated version of the model can be rotated from the model at a specific angle and still achieve the specified score. The specified score must be set to a value higher than the acceptance level. The higher this score is set, the smaller the angle refinement step will be. If you need a minimum accuracy, you can set M_SEARCH_ANGLE_ACCURACY to the required minimum accuracy and then enable angle refinement mode by setting the minimum score for a rotated model. The lowest value between the determined angle refinement step and the specified angle accuracy is the angle refinement step used when searching for the pattern in the target image. You can set the method for scanning the specified angular range for multiple occurrences, or for one occurrence with circular overscan data, using MpatControl() with M_SEARCH_ANGLE_ACCURACY_MODE. By default, MIL returns the closest occurrence to the approximate location found within the angular range, but you can set M_SEARCH_ANGLE_ACCURACY_MODE to M_ALL to scan through all possible occurrences found at the angle refinement steps and return the best one. Doing so might reduce performance, so you should only set M_SEARCH_ANGLE_ACCURACY_MODE to M_ALL when the target image has many potential false positive occurrences. When searching within a range of angles, you should use as narrow a range, as high a tolerance, and as large an angle accuracy (angle refinement step) as possible, since the operation can take a long time to perform. Determining the rotation tolerance of a model Every model has its own particular rotation tolerance. This tolerance is dependent on the individual model characteristics and surrounding target image features. If you don't want MIL to automatically calculate the rotational tolerance (using MpatControl() with M_SEARCH_ANGLE_TOLERANCE set to M_AUTO), you must specify the required precision of the resulting angle as a value from 0 to 180.0 degrees. To determine the rotation tolerance of a model: Set the search angle of a model to the same angle as the sought for pattern in a sample target image. However, set the positive and negative delta values (M_SEARCH_ANGLE_DELTA...) to zero since you want to test by how much a pattern in an image can be rotated and still correlate with a model at a specific angle. Use the MimRotate() function to rotate the image in very small, positive increments (for example, 0.5°), and perform a MpatFind() operation at every angle. Make sure that the image's center of rotation is the same as that of the model; otherwise, the resulting tolerance will not be accurate. Note, when rotating the image, always set the angle from the image's original position to avoid interpolating the image more than once. Check the results for the greatest angle that produces an acceptable score. Repeat steps 1 and 2, rotating the image in negative increments. Take the minimum of the absolute value of these angles. Double this angle and set it as the rotation tolerance for the angular search. Finding models when they are at an angle Setting the angle of search Determining the rotation tolerance of a model ",
      "wordCount": 1272,
      "subEntries": []
    },
    {
      "id": "UG_pattern_Masking_the_model",
      "version": null,
      "title": "Masking the model",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\pattern\\Masking_the_model.htm",
      "text": " Masking the model Often your model contains regions that you need MIL to ignore when searching for the model in the target image. These regions might be noise pixels or simply regions that have nothing to do with what you are searching. For instance, in a machine guidance application, a mechanical device might need to know where mounting holes are located on a circuit board so that screws can be properly inserted. In this case, a mounting hole would be the model and the circuit board would be the target image. In the above image, the model contains too much of the actual board; it might not match holes in different areas of the board. In such cases, parts of the model can be masked by setting pixel values in certain regions to \"don't care\" pixels. MIL then ignores these regions when searching for occurrences of the model. In our example, you would need to mask the edges of the model, as follows: The unmasked region of the model now more closely resembles the pattern for which to search; it is more circular in shape and contains little of the actual board. To create a mask: Allocate an image that is of the same size as the model image. This new image is the mask image. Clear the mask image to zero. Set the \"don't care\" pixels in the mask image to a non-zero value. There are numerous ways of doing this. For example, you can use one of the Mgra...() functions. Call MpatMask(), specifying the mask image. This function sets the model's \"don't care\" pixels. View the mask using MpatDraw() with M_DRAW_DONT_CARE. When you change the \"don't care\" pixels of a model, you should preprocess the search model again. Masking the model ",
      "wordCount": 295,
      "subEntries": []
    },
    {
      "id": "UG_pattern_Search_constraints",
      "version": null,
      "title": "Search constraints",
      "subTitles": [
        "Specifying the search mode",
        "Specifying the number of matches",
        "Setting the acceptance level",
        "Setting the certainty level",
        "Redefining the model's reference position",
        "Selecting the search region",
        "Positional accuracy",
        "Selecting the speed setting"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\pattern\\Search_constraints.htm",
      "text": " Search constraints When a model is defined (whether manually or automatically), it is assigned a set of default search constraints. You can change the following constraints: The search mode. The number of occurrences to find. The threshold for acceptance and certainty. The model's reference position. The region to search in the target image. The positional accuracy. The search speed. Specifying the search mode When searching for multiple models, you can specify whether to use the search settings from each model, or from the first model in the context, using MpatControl() with M_SEARCH_MODE. When searching for multiple occurrences of multiple models in your target image, selecting one search mode over the other could provide better results in your target application. Significant testing is required for identifying when one mode will improve the results of your find operation over the other. When M_SEARCH_MODE is set to M_FIND_ALL_MODELS, the search is performed using each model's current search settings. Note that, all models must use the same search region in the target image. The M_FIND_ALL_MODELS mode is best used when searching at an angle or when searching for an unknown number of occurrences in a region. For example, you must process an image of coins dumped on a tray. While the coins are spread out (so there is very little overlap), you cannot guarantee the change is not rotated nor can you know in advance how many coins are present. When M_SEARCH_MODE is set to M_FIND_BEST_MODELS, the search is performed using the first model's current search settings. The settings of all other models are ignored. In addition, you cannot use M_FIND_BEST_MODELS to perform an angular search (M_SEARCH_ANGLE...). The M_FIND_BEST_MODELS mode is best used when searching for a maximum known number of occurrences in a region without knowing how many instances of each model are in the target image. For example, you know there will be 8 coins, but you don't know the exact amount of money (that is, you don't know how many of each coin exists in your target image). Specifying the number of matches You can specify how many matches to try to find, using MpatControl() with M_NUMBER. If all you need is one good match, set the required number of occurrences to one (the default value) and avoid unnecessary searches for further matches. If a correlation has a match score greater than or equal to the certainty level, it is automatically considered an occurrence (default 80%), the remaining occurrences will be the best of those greater than or equal to the acceptance level. When you ask for a specific number of matches (using MpatControl() with M_NUMBER), the MpatFind() function might not find that number; you should always call MpatGetResult() with M_NUMBER to see how many occurrences were actually found. When multiple results are found (in M_FIND_BEST_MODELS), they are returned in decreasing order of match score (that is, best match first). When using M_FIND_ALL_MODELS, the occurrences of each model are grouped together in decreasing order of match scores from the first model to the last. To get the model index associated with a specific model occurrence, use MpatGetResult() with M_INDEX. Setting the acceptance level The level at which the correlation (match score) between the model and the pattern in the image is considered a match is called the acceptance level. You can set the acceptance level for the specified model, using MpatControl() with M_ACCEPTANCE. If the correlation between the target image and the model is less than this level, they are not considered a match. A perfect match is 100%, a typical match is 80% or higher (depending on the image), and no correlation is 0%. If your images have considerable noise and/or distortion, you might have to set the level below the default value of 70%. However, keep in mind that such poor-quality images increase the chance of false matches and will probably increase the search time. Note, perfect matches are generally unobtainable because of noise introduced when grabbing images. Setting the certainty level The certainty level is the match score above (or equal to) which the algorithm can assume that it has found a very good match and can stop searching the rest of the image for a better one. The certainty level is very important because it can greatly affect the speed of the search. To understand why, you need to know a little about how the search algorithm works. Since a brute force correlation of the entire model, at every point of the image, would take several minutes, it is not practical. Therefore, the algorithm has to be intelligent. It first performs a rough but quick search to find likely match candidates, and then checks out these candidates in more detail to see which are acceptable. A significant amount of time can be saved if several candidate matches never have to be examined in detail. This can be done by setting a certainty level that is reasonable for your needs, using MpatControl() with M_CERTAINTY. A good level is slightly lower than the expected score. If you absolutely must have the best match in the image, set the level to 100%. This would be necessary if, for example, you expect the target image to contain other patterns that look similar to your model. Unwanted patterns might have a high score, but this will force the search algorithm to ignore them. Symmetrical models fall into this category. At certain angles symmetrical models might seem like an occurrence in the target image, but if the search was completed, a match with a higher score would be found. If you know that the model is unique in the image, then anything that reaches the acceptance level must be the match you want; therefore, you can set the certainty and acceptance levels to the same value. Otherwise, the certainty should be set to a value above the acceptance level. Another common case is a pattern that usually produces very good scores (say above 80%), but occasionally a degraded image produces a much lower score (say 50%). Obviously, you must set the acceptance level to 50%; otherwise you will never get a match in the degraded image. However, you cannot set the certainty level to 50% because you take the risk that it will find a false match (above 50%) in a good image before it finds the real match that scores 90%. A better value is about 80%, meaning that most of the time the search will stop as soon as it sees the real match, but in a degraded image (where nothing reaches the certainty level), it will take the extra time to look for the best match that reaches the acceptance level. Redefining the model's reference position The coordinates returned by MpatGetResult(), after a call to MpatFind(), are the coordinates of the models' reference position transformed (in pixel or real-world units, depending on whether the camera setup is calibrated). By default, this reference position is defined to be at the geometric center of the model. When using pixel units, results are returned relative to the top-left corner of the target image. When using calibration units, results are returned with respect to the relative coordinate system. If there is a particular spot from which you would like results returned, you can change the model's reference position, using MpatControl() with M_REFERENCE_X and M_REFERENCE_Y. For example, if your model has a hole and you want to find results with respect to this hole, change the reference position of the model accordingly. Note that you can define the reference position to be outside of the model's boundary. Selecting the search region Instead of searching the entire target image, you can limit the search region using MpatControl() with M_SEARCH_OFFSET... and M_SEARCH_SIZE.... The search region specifies the region in which to find the model's reference position. Therefore, the search region can even be smaller than the model. If you have redefined the model's reference position (with MpatControl() with M_REFERENCE...), make sure that the search region covers this new reference position and takes into account the angular search range of the model. Search time is roughly proportional to the region searched; always set the search region to the minimum required when speed is a consideration. Alternatively, you can limit the operation to a region of the image buffer using a rectangular region of interest (ROI), set using MbufSetRegion(). Doing so is more flexible than setting a search region using MpatControl() with M_SEARCH_OFFSET... and M_SEARCH_SIZE... because it allows the search region to be at an angle. In addition, using an ROI allows you to define your search region in world units. However, for some regions, it might increase processing time compared to using MpatControl() with M_SEARCH_OFFSET... and M_SEARCH_SIZE.... In general, you should not use a child buffer to delimit the search region to a portion of an image; this might cause the search operation to have border or edge effects (caused by incomplete neighbourhood operations and/or overscan pixels) and be less accurate as this operation does not assume that there is valid data outside of the buffer. When limiting the search region of a model defined with M_CIRCULAR_OVERSCAN, it is recommended that the search region always include the center of the model, even if the reference position has moved from its default position. Positional accuracy You can set the positional accuracy for your search. To do so, use MpatControl() with M_ACCURACY. It can be set to: M_LOW (typically ± 0.20 pixel). M_MEDIUM (typically ± 0.10 pixel). M_HIGH (typically ± 0.05 pixel). Note, the actual precision achieved is dependent on the quality of the model and of the image (the tolerances listed above are typical for high-quality, low-noise images). A less precise positional accuracy will speed up the search. Positional accuracy is also slightly affected by the search speed setting (MpatControl() with M_SPEED). Selecting the speed setting You can specify the algorithm's search speed, using MpatControl() with M_SPEED. When the search speed is set to M_VERY_HIGH or M_HIGH, the search algorithm takes more shortcuts, and the search is performed faster. However, as you increase the speed, the robustness of the search operation (the likelihood of finding a model) can decrease. For more information on search speed, see the Speeding up the search section later in this chapter. Search constraints Specifying the search mode Specifying the number of matches Setting the acceptance level Setting the certainty level Redefining the model's reference position Selecting the search region Positional accuracy Selecting the speed setting ",
      "wordCount": 1743,
      "subEntries": []
    },
    {
      "id": "UG_pattern_Preprocess_the_search_model",
      "version": null,
      "title": "Preprocessing the Pattern Matching context",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\pattern\\Preprocess_the_search_model.htm",
      "text": " Preprocessing the Pattern Matching context When you are ready to search for the allocated model (either manually or automatically), you must preprocess the Pattern Matching context. The preprocessing stage uses the known model(s) to decide on the optimal search strategy for subsequent search operations. Preprocessing should be performed after all search constraints have been set. Use the MpatPreprocess() function to preprocess the Pattern Matching context. MpatPreprocess() has a parameter that allows you to specify a typical target image. Providing a typical image is optional; you can set this parameter to M_NULL. If you provide this image, it helps MpatPreprocess() improve the search's robustness and optimize the strategy for subsequent search operations. You should only specify a typical image if the model occurrence will always appear on the same type of background. Note that when you load or restore a Pattern Matching context (using MpatRestore()), you must preprocess it, even if it was preprocessed before saving. Preprocessing the Pattern Matching context ",
      "wordCount": 162,
      "subEntries": []
    },
    {
      "id": "UG_pattern_Speeding_up_the_search",
      "version": null,
      "title": "Speeding up the search",
      "subTitles": [
        "Choose the appropriate model",
        "Adjust the search speed setting",
        "Effectively choose the search region and search angle"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\pattern\\Speeding_up_the_search.htm",
      "text": " Speeding up the search To ensure the fastest possible search, you should: Choose an appropriate model. Set the search speed to the highest possible setting for your application. Set the search region to the minimum required. Search time is roughly proportional to the region searched, so don't search the whole image if you don't need to. Search the smallest range of angles required. Select the lowest positional accuracy that you need. Set the certainty level to the lowest reasonable value (so that the search can stop as soon as a good match is found). Search for multiple models at the same time, if possible. If you can't attain the required speed using these techniques, you can use some more advanced techniques discussed in the Pattern matching algorithm (for advanced users) section later in this chapter. Choose the appropriate model The size of a model affects the search speed. In general, small models take longer to find than larger ones, although very large models can also be time-consuming. In general, the optimal size is approximately 128x128 pixels if you are searching a large region (for example, most of the image). Small models are found quickly when the search region is not too large. Adjust the search speed setting For any model, it is possible to set the speed of the search. Increasing the search speed reduces the search time, however, as you increase the speed, the robustness of the search operation (the likelihood of finding a model) can decrease. When you call MpatPreprocess(), MIL analyzes the pattern in the model, and determines appropriate shortcuts; only shortcuts that are considered safe for a particular model are taken. This also means that higher search speeds might not be any faster for certain models, depending on the particular pattern. Higher search speeds can reduce the positional accuracy very slightly. You adjust the search speed setting, using MpatControl() with M_SPEED. This function has five settings: M_VERY_HIGH. M_HIGH. M_MEDIUM. M_LOW. M_VERY_LOW. As expected, the M_VERY_HIGH and M_HIGH speed settings allow the search to take all possible shortcuts, performing the search as fast as possible. Higher speed settings are recommended when searching on a good quality image or when using a simple model. Note, the search might have a lower tolerance for rotation when using this setting. The M_MEDIUM speed setting is the default setting and is recommended for medium quality images or more complex models. A search with this setting is capable of withstanding up to approximately 5° of rotation for typical models. Use the M_LOW or the M_VERY_LOW speed settings only if the image quality is particularly poor and you have encountered problems at higher speeds. The speed settings are discussed further in the algorithm description at the end of this chapter. Effectively choose the search region and search angle You can improve performance by not searching the whole image unnecessarily. Search time is roughly proportional to the region searched; set the search region to the minimum required, using MpatControl() with M_SEARCH_OFFSET... and M_SEARCH_SIZE..., or using a rectangular region of interest (ROI) set using MbufSetRegion(). The latter method is more flexible but has its own limitations, as outlined in the Selecting the search region subsection of the Search constraints section earlier in this chapter. You can also improve performance by selecting the lowest positional accuracy. In addition, for an angular search, select lowest angular accuracy (MpatControl() with M_SEARCH_ANGLE_ACCURACY) and range required, in combination with the highest tolerance possible. Speeding up the search Choose the appropriate model Adjust the search speed setting Effectively choose the search region and search angle ",
      "wordCount": 596,
      "subEntries": []
    },
    {
      "id": "UG_pattern_Pattern_matching_algorithm_for_advanced_users",
      "version": null,
      "title": "Pattern matching algorithm (for advanced users)",
      "subTitles": [
        "Normalized correlation",
        "Hierarchical search",
        "Search region",
        "Search heuristics",
        "Subpixel accuracy"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\pattern\\Pattern_matching_algorithm_for_advanced_users.htm",
      "text": " Pattern matching algorithm (for advanced users) Normalized grayscale correlation is widely used in industry for pattern matching applications. Although in many cases you do not need to know how the search operation is performed, an understanding of the algorithm can sometimes help you pick an optimal search strategy. Normalized correlation The correlation operation can be seen as a form of convolution, where the pattern matching model is analogous to the convolution kernel (see the Custom spatial filters section of Chapter 4: Advanced image processing). Ordinary (un-normalized) correlation is exactly the same as a convolution: In other words, for each result, the N pixels of the model are multiplied by the N underlying image pixels, and these products are summed. Note, the model does not have to be rectangular because it can contain \"don't care\" pixels that are completely ignored during the calculation. When the correlation function is evaluated at every pixel in the target image, the locations where the result is largest are those where the surrounding image is most similar to the model. The search algorithm then has to locate these peaks in the correlation result, and return their positions. Unfortunately, with ordinary correlation, the result increases if the image gets brighter. In fact, the function reaches a maximum when the image is uniformly white, even though at this point it no longer looks like the model. The solution is to use a more complex, normalized version of the correlation function (the subscripts have been removed for clarity, but the summation is still over the N model pixels that are not \"don't cares\"): With this expression, the result is unaffected by linear changes (constant gain and offset) in the image or model pixel values. The result reaches its maximum value of 1 where the image and model match exactly, gives 0 where the model and image are uncorrelated, and is negative where the similarity is less than might be expected by chance. In our case, we are not interested in negative values, so results are clipped to 0. In addition, we use r2 instead of r to avoid the slow square-root operation. Finally, the result is converted to a percentage, where 100% represents a perfect match. So, the match score returned by MpatGetResult() is actually: Score = max(r, 0) 2 x 100% Note, some of the terms in the normalized correlation function depend only on the model, and hence can be evaluated once and for all when the model is defined. The only terms that need to be calculated during the search are: This amounts to two multiplications and three additions for each model pixel. The sums used to compute the correlation function can be retrieved using MpatGetResult(). The retrievable sums are the following: The number of pixels N is also available using MpatGetResult(). The above sums are only available if they are saved in the result buffer using MpatControl() with M_SAVE_SUMS set to M_ENABLE. These sums can be used, for example, to assess the model or target intensity level and contrast. Note that the correlation function calculated with the retrieved sums, will probably differ from the score returned by MpatGetResult(). Although the score is derived from the correlation function, it also depends on the first and last subsampling levels chosen, optimization schemes and subpixel interpolation. On a typical computer, the multiplications alone account for most of the computation time. A typical application might need to find a 128x128-pixel model in a 512x512-pixel image. In such a case, the total number of multiplications needed for an exhaustive search is 2x512 2 x128 2 , or over 8 billion. On a typical computer, this would take a few minutes, much more than the 5 msec or so the search actually takes. Clearly, MpatFind() does much more than simply evaluate the correlation function at every pixel in the search region and return the location of the peak scores. Hierarchical search A reliable method of reducing the number of computations is to perform a so-called hierarchical search. Basically, a series of smaller, lower-resolution versions of both the image and the model are produced, and the search begins on a much-reduced scale. This series of subsampled images is sometimes called a resolution pyramid, because of the shape formed when the different resolution levels are stacked on top of each other. Each level of the pyramid is half the size of the previous one, and is produced by applying a low-pass filter before subsampling. If the resolution of an image or model is 512x512 at level 0, then at level 1 it is 256x256, at level 2 it is 128x128, and so on. Therefore, the higher the level in the pyramid, the lower the resolution of the image and model. The search starts at low resolution to quickly find likely match candidates. It proceeds to higher and higher resolutions to refine the positional accuracy and make sure that the matches found at low resolution actually were occurrences of the model. Because the position is already known from the previous level (to within a pixel or so), the correlation function need be evaluated only at a very small number of locations. Since each higher level in the pyramid reduces the number of computations by a factor of 16, it is usually best to start at as high a level as possible. However, the search algorithm must trade off the reduction in search time against the increased chance of not finding the pattern at very low resolution. Therefore, it chooses a starting level according to the size of the model and the characteristics of the pattern. In the application described earlier (128x128 model and 512x512 image), it might start the search at level 4, which would mean using an 8x8 version of the model and a 32x32 version of the target image. To determine the default first and last resolution levels, use MpatInquire() with M_PROC_FIRST_LEVEL and M_PROC_LAST_LEVEL, respectively. If required, you can change the first and last resolution levels, using MpatControl() with M_FIRST_LEVEL and M_LAST_LEVEL to either a specific or the maximum possible level, repetitively. You can set the first resolution level to a specific value or you can have MIL automatically set the best first level, based on the model's size or its content. When dealing with images without much fine detail, you should have MIL automatically select the first resolution level from the model's size (using M_AUTO_SIZE_BASED). This can provide a faster search, but fine details can be lost when dealing with larger images. In cases where fine details are present in larger target images, you should have MIL automatically select a first resolution level from the model's content (using M_AUTO_CONTENT_BASED). This intelligent analysis is typically slower, but can provide more robust results. The logic of a hierarchical search accounts for a seemingly counter-intuitive characteristic of MpatFind(): large models tend to be found faster than small ones. This is because a small model cannot be subsampled to a large extent without losing all detail. Therefore, the search must begin at fairly high resolution (low level), where the relatively large search region results in a longer search time. Thus, small models can only be found quickly in fairly small search regions. Note that the pyramidal representation of the buffer is generated each time MpatFind() is called. However, you can save the pyramidal representation of the buffer (generated when MpatFind() is called) in the result buffer, using MpatControl() with M_TARGET_CACHING. This pyramidal representation is re-used by consecutive calls to MpatFind() as long as the same result buffer is used and the image, search region, and model size are not modified. Search region A zero-overscan technique is used to search for partially occluded occurrences near the border of the target image. However, this can lead to lower score and lower accuracy. In the example below, the left-most occurrence, which overlaps the border of the target image, would yield a lower score than the right-most occurrence. Search heuristics Even though performed at very low resolution, the initial search still accounts for most of the computation time if the correlation is performed at every pixel in the search region. On most models, match peaks (pixel locations where the surrounding image is most similar to the model, and correlation results are largest) are several pixels wide. These can be found without evaluating the correlation function everywhere. MpatPreprocess() analyzes the shape of the match peak produced by the model, and determines if it is safe to try to find peaks faster. If the pattern produces a very narrow match peak, an exhaustive initial search is performed. The search algorithm tends to be conservative; if necessary, force fast peak finding, using MpatControl() with M_FAST_FIND. Using MpatControl() with M_EXTRA_CANDIDATES, you can set the number of extra peaks to consider. Normally, the search algorithm considers only a limited number of (best) scores as possible candidates to a match when proceeding at the most subsampled stage. You can add robustness to the algorithm, by considering more candidates, without compromising too heavily on search speed. In addition, you can use MpatControl() with M_COARSE_SEARCH_ACCEPTANCE to set a minimum match score, valid at all levels except the last level, to be considered as an occurrence of the model. This ensures that possible models are not discarded at lower levels, yet maintains the required certainty during the final level. At the last (high-resolution) stage of the search, the model is large, so this stage can take a significant amount of time, even though the correlation function is evaluated at only a very few points. To save time, you can select a high search speed, using MpatControl() with M_SPEED. For most models, this has little effect on the score or accuracy, but does increase speed. Subpixel accuracy The highest match score occurs at only one point, and drops off around this peak. The exact (subpixel) position of the model can be estimated from the match scores found on either side of the peak. A surface is fitted to the match scores around the peak and, from the equation of the surface, the exact peak position is calculated. The surface is also used to improve the estimate of the match score itself, which should be slightly higher at the true peak position than the actual measured value at the nearest whole pixel. The actual accuracy that can be obtained depends on several factors, including the noise in the image and the particular pattern of the model. However, if these factors are ignored, the absolute limit on accuracy, imposed by the algorithm itself and by the number of bits and precision used to hold the correlation result, is about 0.025 pixels. This is the worst-case error measured in X or Y when an image is artificially shifted by fractions of a pixel. In a real application, accuracy better than 0.05 pixels is achieved for low-noise images. These numbers apply if you select high-accuracy search, using MpatControl() with M_ACCURACY, in which case the search always proceeds to resolution level 0. If you select medium accuracy (the default), the search might stop at resolution level 1, and hence the accuracy is half of what can be attained at level 0. Selecting low accuracy might cause the search to stop at level 2, so the accuracy is reduced by an additional factor of two (to about 0.2 pixel). Pattern matching algorithm (for advanced users) Normalized correlation Hierarchical search Search region Search heuristics Subpixel accuracy ",
      "wordCount": 1901,
      "subEntries": []
    },
    {
      "id": "UG_pattern_Pattern_matching_examples",
      "version": null,
      "title": "Pattern matching examples",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\pattern\\Pattern_matching_examples.htm",
      "text": " Pattern matching examples The following example demonstrates how to configure a find operation to locate a model within a target image. mpat.cpp To run this example, use the Matrox Example Launcher in the MIL Control Center. The example mpat.cpp does the following: Finds a CPU chip on a board, using a pattern matching context containing a model defined using MpatDefine() with M_REGULAR_MODEL that has its accuracy and speed set to high (using MpatControl() with M_ACCURACY and M_SPEED both set to M_HIGH). Finds the same chip within a range of angles, using a pattern matching context containing a model defined using MpatDefine() with M_REGULAR_MODEL + M_CIRCULAR_OVERSCAN. Finds a rotated wafer model occurrence within a rotated target image, using a pattern matching context containing a model defined using MpatDefine() with M_REGULAR_MODEL. In addition, the example uses MimRotate() to rotate both the source and target image. Automatically defines a model within a target image, using a pattern matching context containing a model defined using MpatDefine() with M_AUTO_MODEL. Pattern matching examples ",
      "wordCount": 169,
      "subEntries": []
    }
  ]
}]
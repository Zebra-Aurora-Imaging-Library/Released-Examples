[{
  "id": "UG_3D_Image_processing",
  "version": "2024020714",
  "title": "3D image processing",
  "subTitles": null,
  "location": "MIL UG P05: 3D processing and analysis",
  "pageURL": "content\\UserGuide\\3D_Image_processing\\ChapterInformation.htm",
  "text": " Chapter 35: 3D image processing This chapter describes how to perform 3D image processing. 3D Image Processing module overview Basic concepts for 3D image processing Moving or scaling a point cloud or 3D geometry Translation Rotation Scaling Transforming points using a transformation matrix Changing the working coordinate system Finding the transformation between 2 sets of points with a known pairing Working with points in a point cloud Cropping or masking points Cropping using a 3D geometry Cropping using a mask image Specifying the organization type Fixturing Invalidating outlier points Defining which points to use Specifying a bounding box excluding outliers Specifying the extent box Removing invalid points Copying points to user-defined arrays Smoothing a point cloud Merging point clouds Calculating unit normal vectors Surface reconstruction Sampling a point cloud or a 3D geometry Subsampling a point cloud Surface sampling a meshed point cloud Surface sampling a 3D geometry Organized and unorganized point clouds Calculating statistics on a point cloud, depth map, or 3D geometry Steps to performing a statistics calculation Determining the bounding box Determining the centroid Determining distances between points Determining surface variation Determining numbers of points Determining principal component analysis (PCA) statistics Moments Analyzing the profile of a 3D cross-section Taking a profile Setting the slicing plane according to a reference object Analyzing a profile Generating fully corrected depth and intensity maps Steps to create a depth map Changing a depth map by rotating or transforming 3D points Defining the points to use Estimating a recommended depth map image buffer size Establishing the world-to-pixel scale and the world-to-gray level scale of the depth map How to calibrate the image buffer Specifying data placement in the destination image buffer Automatically setting the pixel aspect ratio Specifying the depth map intensity scale Saturating or ignoring points outside the depth map grayscale range Remapping the depth map Generating the intensity map Adding color to a depth map for better visualization Gradually adding data to the depth map and intensity map image buffers Point cloud projection example Filling missing data points (gaps) Filling mode Filling operations Linear interpolation Propagation of one of the boundary values Phenomena causing missing data Examples Performing arithmetic operations on depth maps Using arithmetic for defect detection Subtraction considering neighborhood pixel distances Constraints on depth map operands and how to handle destination calibration ",
  "wordCount": 387,
  "subEntries": [
    {
      "id": "UG_3D_Image_processing_3D_Image_processing_overview",
      "version": null,
      "title": "3D Image Processing module overview",
      "subTitles": null,
      "location": "MIL UG P05: 3D processing and analysis",
      "pageURL": "content\\UserGuide\\3D_Image_processing\\3D_Image_processing_overview.htm",
      "text": " 3D Image Processing module overview The 3D Image Processing module supports processing and basic analysis of 3D data. Using this module, you can perform arithmetic and statistical operations on point clouds, fully corrected depth maps, and 3D geometries. You can scale, rotate, and translate 3D data, compute 3D profiles to examine a cross-section of data, and sample a subset of points or reconstruct surfaces for point clouds. You can also merge multiple point clouds, crop or mask points, and calculate normal vectors for each point based on neighboring points. Additionally, the 3D image processing module allows you to project point cloud or 3D geometry data into a fully corrected image buffer, thereby generating a depth map. To operate on depth maps, they must be stored in an image buffer and be fully corrected (that is, if you call McalInquire() with M_DEPTH_MAP, the function returns M_TRUE). To operate on point clouds, they must be stored in a MIL 3D-processable format (that is, if you call MbufInquireContainer() with M_3D_PROCESSABLE, the function returns M_PROCESSABLE). All 3D-processable point cloud containers have the 2 components M_COMPONENT_RANGE and M_COMPONENT_CONFIDENCE. The range component stores coordinates for 3D points and the confidence component stores a confidence score for each point, where a confidence score of 0 indicates an invalid point. If a confidence is not 0, the point is valid. Note that, for source containers, points in the M_COMPONENT_RANGE component whose corresponding confidence score is 0 have no effect on the computation performed. For destination containers, the M_COMPONENT_RANGE component value of a point with 0 confidence can be any value, and that value might change when the range component is copied, or when read on a different system. Some 3D image processing functions create and add new components to the destination container, or copy source components into the destination. Typically, previously existing components in the destination container are overwritten. Some functions might free and reallocate components, which can change a component's MIL identifier. In-place processing is supported. A temporary internal container is used if necessary. 3D Image Processing module overview ",
      "wordCount": 344,
      "subEntries": []
    },
    {
      "id": "UG_3D_Image_processing_Basic_concepts",
      "version": null,
      "title": "Basic concepts for 3D image processing",
      "subTitles": null,
      "location": "MIL UG P05: 3D processing and analysis",
      "pageURL": "content\\UserGuide\\3D_Image_processing\\Basic_concepts.htm",
      "text": " Basic concepts for 3D image processing The basic concepts and vocabulary conventions for 3D image processing are: Bounding box. The smallest axis-aligned box containing all the valid points in a point cloud. For a depth map, the bounding box encompasses real world coordinates that correspond to valid depth map pixels. Depth map. An image where the gray value of a pixel represents its depth in the world. Extent box. The maximum box that a depth map spans, irrespective of its data. This is the minimum/maximum X-, Y-, and Z-coordinates that would be representable in the depth map image buffer, according to its size and calibration information. Invalid point. A data value that cannot be resolved to a valid point in a point cloud or a valid pixel in a depth map. For a point cloud container, invalid data correspond to a confidence score of 0. For a depth map, missing or otherwise unresolvable pixel data are referred to as invalid points. Mesh. A triangulated surface generated for a point cloud. Organized point cloud. A point cloud whose data is stored in a container with 2D components. This 2D grid organization reflects spatial proximity between nearby elements; that is, points in the point cloud that are near each other are also stored near each other in the 2D component of the container. Processing of an organized point cloud is usually more efficient; some 3D image processing operations require organized point clouds. Unorganized point cloud. A point cloud whose data is stored in a container with 1D components. There is no spatial correspondence between nearby elements in the buffer. Working coordinate system. The implicit real world coordinate system used to express the coordinates of 3D points in a point cloud or 3D geometry object. Each point cloud container has its own working coordinate system. You might need to transform the points in the point cloud so that they are expressed in the same working coordinate system as another point cloud. Basic concepts for 3D image processing ",
      "wordCount": 336,
      "subEntries": []
    },
    {
      "id": "UG_3D_Image_processing_Manipulating_a_point_cloud_or_3D_geometry_object",
      "version": null,
      "title": "Moving or scaling a point cloud or 3D geometry",
      "subTitles": [
        "Translation",
        "Rotation",
        "Scaling",
        "Transforming points using a transformation matrix",
        "Changing the working coordinate system"
      ],
      "location": "MIL UG P05: 3D processing and analysis",
      "pageURL": "content\\UserGuide\\3D_Image_processing\\Manipulating_a_point_cloud_or_3D_geometry_object.htm",
      "text": " Moving or scaling a point cloud or 3D geometry With the 3D Image Processing module, you can translate, rotate, and scale a point cloud or 3D geometry, using M3dimTranslate(), M3dimRotate(), and M3dimScale(), respectively. You can also apply these transformations or a combination of these, using a transformation matrix; in this case, you can use M3dgeoMatrixSetTransform() or M3dgeoMatrixSetWithAxes() to generate the required transformation matrix, and then use M3dimMatrixTransform() to apply it. Translation To displace a point cloud or 3D geometry, use M3dimTranslate(). Specify the required displacements along each axis direction, in world units. The translation applies to all points in the source object. The following image shows the translation of a point cloud. If you want to apply additional transformations, or if you need to specify the required translation to a function that takes a transformation matrix object, you should instead generate a transformation matrix. To define a transformation matrix to perform a translation, use M3dgeoMatrixSetTransform() with the M_TRANSLATION transformation type and the required coordinate displacements along each of the axes of the working coordinate system. Refer to the Transforming points using a transformation matrix subsection of this section for more information. Rotation To rotate a point cloud or 3D geometry, use M3dimRotate(). Specify the center of rotation (the point around which to rotate) and the type of rotation to perform. Note that the default center of rotation is the origin (0,0,0). You can rotate around one axis (for example, M_ROTATION_X) or around all three axes (for example, M_ROTATION_XYZ). The three rotations about the axes of the working coordinate system are also known as roll, pitch, and yaw. You can also specify a vector around which to rotate (M_ROTATION_AXIS_ANGLE). For each of these rotation types, you must specify the amount of rotation in degrees. You can also specify a quaternion rotation (M_ROTATION_QUATERNION). The following images show the rotation of a point cloud, around the X-axis (left) and around a specified vector (right), using the default center of rotation (0,0,0). You can specify a custom center of rotation or, for 3D geometry objects, you can choose to center the rotation around the volumetric center of the object (M_GEOMETRY_CENTER). To rotate around the volumetric center of a point cloud, you can establish its centroid using M3dimStat() with M_STAT_CONTEXT_CENTROID. If you displace the center of rotation, the rotation occurs around a line that is parallel to the axis of rotation and runs through the center point. The following image shows the rotation of a point cloud, around a line that is parallel to the X-axis and runs through the point (0,3,4). If you want to apply additional transformations, or if you need to specify the required rotation to a function that takes a transformation matrix object, you should instead generate a transformation matrix. To define a transformation matrix to perform a rotation, use M3dgeoMatrixSetTransform() and specify one of several rotation operations (M_ROTATION_...). Refer to the Transforming points using a transformation matrix subsection of this section for more information. Scaling To scale the distance between points in a point cloud or 3D geometry, use M3dimScale(). Specify the required factor by which to scale in each axis direction. The scaling is applied to each point's distance from the origin or specified center point, along X, Y, or Z. For example, if you specify a 0.5 scale factor along each axis, the resulting point cloud is half the size. That is, each point's distance from the center point (along X, Y, and Z) is half the distance as that of points in the source point cloud. As with rotation, you can specify to scale with respect to the volumetric center of a 3D geometry so that the geometry's center does not move; the geometry just changes in size. Note that applying a non-uniform scale (that is, a scale with a different factor along each axis) changes the shape of the point cloud or 3D geometry. For this reason, you cannot apply a non-uniform scale to a box or sphere 3D geometry because the result would no longer be a box or sphere. To obtain a mirror-image of your source object, you can set negative scale factors. However, a mirrored object can be problematic if you intend to perform additional analysis. For example, any text or symbols on the object will be mirrored. The following image contains the original point cloud that will be scaled in the table below. The following table shows the effect of different scale factors on the given point cloud. Scale factor Effect Scaled point cloud (positive scale factor) Scaled point cloud (negative scale factor) ScaleFactor &gt; 1 or ScaleFactor &lt; -1 Each coordinate's distance from the origin increases. The point cloud or 3D geometry becomes larger. If the scale factor is negative, the point cloud or 3D geometry is reflected on the opposite side of the origin. -1 &lt; ScaleFactor &lt; 1 Each coordinate's distance from the origin decreases. The point cloud or 3D geometry becomes smaller. If the scale factor is negative, the point cloud or 3D geometry is reflected on the opposite side of the origin. If you want to apply additional transformations, or if you need to specify the required scale to a function that takes a transformation matrix object, you should instead generate a transformation matrix. To define a transformation matrix to perform a scaling operation, use M3dgeoMatrixSetTransform() with the M_SCALE transformation type and a scale factor for which coordinate distances will be multiplied. Refer to the Transforming points using a transformation matrix subsection of this section for more information. Scaling a point cloud is useful for numeric stability. For example, if you scan a scene with a 3D sensor that outputs a point cloud scaled in microns, the dimensions of your point cloud can be millions of microns long. If you need to perform calculations using the dimensions of a point cloud, or the objects therein, it is useful to scale down the point cloud so that all measurements are of order unity. If you have performed moments statistics calculations on your point cloud (M3dimControl() with M_MOMENTS and then M3dimStat()), you can use a transformation matrix to scale the point cloud to dimensions at or near one unit. This means that the centroid is positioned at (0,0,0) and the point cloud is transformed so that it has a unit variance along each axis. To do so, you must first allocate a transformation matrix object, using M3dgeoAlloc() with M_TRANSFORMATION_MATRIX. Copy the results of the moments statistics calculations to the transformation matrix, using M3dimCopyResult() with M_STANDARDIZATION_MATRIX. You can then apply the transformation matrix to the 3D points using M3dimMatrixTransform(). Note that the moments results must be of order 2 or greater. Transforming points using a transformation matrix To apply an arbitrary linear transformation to a point cloud or 3D geometry, apply the following: Allocate a transformation matrix object using M3dgeoAlloc(). Generate the required transformation coefficients using M3dgeoMatrixSetTransform(). Apply the transformation matrix to the 3D points using M3dimMatrixTransform(). Note that you can also apply the transformation matrix to an array of 3D coordinates using M3dimMatrixTransformList(). If you apply multiple transformations consecutively (using M3dimTranslate(), M3dimRotate(), and/or M3dimScale()), MIL computes a new point cloud for each transformation. For better performance, you can generate a transformation matrix that combines all the transformations into one, and then apply it as a single transformation. To do so, use M3dgeoMatrixSetTransform() with M_COMPOSE_WITH_CURRENT, which will compose the coefficients of a specified transformation with the existing coefficients of the specified transformation matrix object. Alternatively, you can use M_COMPOSE_TWO_MATRICES to combine the transformation defined by two matrices into one matrix. Note that the order of the matrices matters. The matrix composition follows the equation AB = C, where A is the first source matrix, B is the second source matrix, and C is the resulting transformation matrix. Note that transformation matrix B is applied before transformation matrix A. Therefore, transformation matrix C describes the transformation obtained by applying transformation matrix B followed by transformation matrix A. The following images show the result of applying a translation and then a rotation, versus applying a rotation and then a translation to the same point cloud. The image on the left depicts a point cloud undergoing a translation through X, Y, and Z followed by a rotation about the Z-axis. You can use M_COMPOSE_TWO_MATRICES and specify the rotation matrix as matrix A and the translation matrix as matrix B to compose a new transformation matrix that can obtain the same effect in one single action. The image on the right depicts a point cloud undergoing the same two transformations, but in the opposite order (rotation followed by translation). To compose the transformation matrix that has the effect of applying a rotation followed by a translation, use M_COMPOSE_TWO_MATRICES and specify the translation matrix as matrix A and the rotation matrix as matrix B. The following code snippet is an example of the steps required to transform a point cloud. The snippet shows how you can fixture to the origin a point cloud that has been translated and rotated by known amounts. To do so, it takes the inverse of the composition of the point cloud's translation and rotation, and applies it to the point cloud. Note that the snippet combines the translation and rotation transformations into one matrix, using M3dgeoMatrixSetTransform() with M_COMPOSE_WITH_CURRENT. /* Amount of translation and rotation of an object in a scene is established, for example, using 3D model finder. */ /* Set transformation coefficients. */ M3dgeoMatrixSetTransform(MilMatrix, M_TRANSLATION, Translation_X, Translation_Y, Translation_Z, M_DEFAULT, M_DEFAULT); M3dgeoMatrixSetTransform(MilMatrix, M_ROTATION_XYZ, Rotation_X, Rotation_Y, Rotation_Z, M_DEFAULT, M_COMPOSE_WITH_CURRENT); /* Find inverse matrix. */ M3dgeoMatrixSetTransform(MilMatrix, M_INVERSE, MilMatrix, M_DEFAULT, M_DEFAULT, M_DEFAULT, M_DEFAULT); /* Apply transformation to point cloud. */ M3dimMatrixTransform(MilPointCloud, MilDstPointCloud, MilMatrix, M_DEFAULT); You can also define a transformation matrix to transform points using M3dgeoMatrixSetWithAxes() with M_POSITION_TRANSFORMATION. This automatically generates the transformation matrix required to offset your 3D points by a specified position and orientation. When applied using M3dimMatrixTransform(), point coordinates are actively transformed and expressed relative to the original origin and axes. In the example below, the point P (3, 0, 2) is moved to P' (4, 0, 3). Note that P's offset from the original coordinate system is equivalent to P''s offset from the new coordinate system. Changing the working coordinate system You can change coordinates such that your 3D points are expressed relative to a new coordinate system (fixtured), using either M3dgeoMatrixSetTransform() or M3dgeoMatrixSetWithAxes(). You can use these functions to automatically generate the transformation matrices required to fixture point clouds or 3D geometries to the specified object or position and orientation. Using M3dgeoMatrixSetTransform(), you can specify a transformation matrix that can move the working coordinate system to a 3D geometry (M_FIXTURE_TO_GEOMETRY), or you can specify a transformation that can move the XY (Z=0) plane of the working coordinate system to a specified plane (M_FIXTURE_TO_PLANE). Alternatively, you can define a transformation matrix using M3dgeoMatrixSetWithAxes() with M_COORDINATE_SYSTEM_TRANSFORMATION. This allows you to set a new origin and axes. When applied using M3dimMatrixTransform(), point coordinates are passively transformed and expressed relative to the new origin and axes. In the example below, the point P (3, 0, 2) is expressed as P' (2, 0, 1) in the new coordinate system. For more information on fixturing in 3D, see the How to fixture in 3D section of Chapter 45: Fixturing in 3D. Moving or scaling a point cloud or 3D geometry Translation Rotation Scaling Transforming points using a transformation matrix Changing the working coordinate system ",
      "wordCount": 1902,
      "subEntries": []
    },
    {
      "id": "UG_3D_Image_processing_Finding_the_transformation_between_2_sets_of_points",
      "version": null,
      "title": "Finding the transformation between 2 sets of points with a known pairing",
      "subTitles": null,
      "location": "MIL UG P05: 3D processing and analysis",
      "pageURL": "content\\UserGuide\\3D_Image_processing\\Finding_the_transformation_between_2_sets_of_points.htm",
      "text": " Finding the transformation between 2 sets of points with a known pairing You can use M3dimFindTransformation() to find the best rigid or affine transformation between two sets of points, if a minimum pairing between the points is known. A rigid transformation can be a combination of translation and/or rotation transformations. An affine transformation can be a combination of translation and/or rotation and/or scale transformations; the scale can be uniform or non-uniform. Note that if the pairing is not known, or if no real pairing exists, you should use M3dregCalculate() to calculate the optimal pairwise registration. You can then obtain the transformation matrix that aligns the points, using M3dregCopyResult() with M_REGISTRATION_MATRIX. For more information, refer to the 3D Registration module. If you know there is a minimum pairing between the points, you can find point pairs in several ways. For example, you can compute an object's ideal corner points to create a target set of points. Then, find the same corners in the point cloud of the object; to do so, use M3dmetFit() to fit planes to the object's faces, and then use M3dmetFeatureEx() to find the planes' intersecting corner points. Once you have your source and target sets of points, you can pass them to M3dimFindTransformation(), which calculates the transformation required to fixture source points to the target positions. Note that in this example, the target points are computed independently, based on known dimensions of an ideal object, and are not calculated using MIL. Another way to find point pairs is to locate common points between two views of the same scene (for example, from two 3D sensors set up at different angles). You can extract points from the same object located in both views (for example, a calibration grid or a code). To do so, pass the acquired point cloud's reflectance component to McalGrid() or McodeRead() and use the respective Calibration or Code modules to identify and extract common 2D points. Then, pass the world coordinates of the extracted points to M3dimGetList(), which returns corresponding 3D coordinates. Repeat as required to establish sets of points that you can pass to M3dimFindTransformation(), which calculates the required transformation between the source and the target. Note that, in this example, established points are usually coplanar, so this approach is suitable for rigid, and not affine, transformations. To find a transformation between two sets of points, apply the following: Find point pairings so that you have a target set and a source set for which to find the transformation. You can store sets of points in two containers, two MIL array buffers, or any combination of container and array buffer. Allocate a result object, using either M3dimAllocResult() with M_FIND_TRANSFORMATION_RESULT, or M3dgeoAlloc() with M_TRANSFORMATION_MATRIX. Find the transformation between the source and target sets of points, using M3dimFindTransformation(). If you passed a result buffer to M3dimFindTransformation(), verify the success of the operation and the acceptable root-mean-square (RMS) error, using M3dimGetResult() with M_STATUS and M_RMS_ERROR, respectively. If you passed a result buffer to M3dimFindTransformation(), and the operation was successful, allocate a transformation matrix object (using M3dgeoAlloc() with M_TRANSFORMATION_MATRIX) and copy the transformation into it using M3dimCopyResult() with M_TRANSFORMATION_MATRIX. If you passed a transformation matrix object to M3dimFindTransformation(), verify the success of the operation using M3dgeoInquire() with M_RIGID or M_AFFINE, and ensure that the function returns M_TRUE. Finding the transformation between 2 sets of points with a known pairing ",
      "wordCount": 564,
      "subEntries": []
    },
    {
      "id": "UG_3D_Image_processing_Working_with_points_in_a_point_cloud",
      "version": null,
      "title": "Working with points in a point cloud",
      "subTitles": [
        "Cropping or masking points",
        "Cropping using a 3D geometry",
        "Cropping using a mask image",
        "Specifying the organization type",
        "Fixturing",
        "Invalidating outlier points",
        "Defining which points to use",
        "Specifying a bounding box excluding outliers",
        "Specifying the extent box",
        "Removing invalid points",
        "Copying points to user-defined arrays",
        "Smoothing a point cloud",
        "Merging point clouds",
        "Calculating unit normal vectors",
        "Surface reconstruction",
        "Sampling a point cloud or a 3D geometry",
        "Subsampling a point cloud",
        "Surface sampling a meshed point cloud",
        "Surface sampling a 3D geometry",
        "Organized and unorganized point clouds"
      ],
      "location": "MIL UG P05: 3D processing and analysis",
      "pageURL": "content\\UserGuide\\3D_Image_processing\\Working_with_points_in_a_point_cloud.htm",
      "text": " Working with points in a point cloud Beyond moving or scaling the coordinates of existing points in a point cloud (see the Moving or scaling a point cloud or 3D geometry section earlier in this chapter), you can crop or mask points, invalidate outlier points, merge point clouds, or construct a meshed surface for a point cloud. You can also sample points to reduce the number of points (subsampling) or, for a meshed point cloud, increase the number of points (surface sampling). Cropping or masking points To crop or mask points in a point cloud, use M3dimCrop(). Source components are copied into the destination container, where the destination container's confidence component determines a point's viability after the crop operation. Specifically, rejected points (those cropped out) are assigned a 0 confidence value in the destination's confidence component. You can crop using a specified 3D geometry object, or mask points using an image buffer. You can also apply a transformation matrix on your point cloud to fixture it before performing the cropping operation. Cropping using a 3D geometry When cropping using a 3D geometry, you can use a closed geometry to define a region in 3D space. Points inside the region are kept; points outside the region are cropped out. Specify M_INVERSE to reverse which points are kept. The following animation shows cropping using a 3D cylinder geometry. Play speed: Default (x1.0) Quarter (x0.25) Half (x0.5) Double (x2.0) Start position: Current image First image Looping: Continuous play Single iteration 1 of 5 Besides other geometries, you can crop using a plane geometry. By definition, points on one side of the plane are kept, and points on the other side are cropped out. Specify M_INVERSE to reverse which points are kept. Note that the direction of the plane's normal vector determines the default side for which points are kept. The following animation shows cropping using a 3D plane geometry. Play speed: Default (x1.0) Quarter (x0.25) Half (x0.5) Double (x2.0) Start position: Current image First image Looping: Continuous play Single iteration 1 of 4 Cropping using a mask image You can crop using a mask image, where pixels with a 0 value determine points to crop, after projection onto the mask image buffer. M3dimCrop() supports three types of mask image buffers: Uncalibrated image masks. Uniformly-calibrated image masks. 3D-calibrated image masks. With an uncalibrated image mask, M3dimCrop() maps points to the same location in the mask image buffer as the point's location in the range component of its point cloud container. The image buffer's size in X and Y must be the same as the source container's range component. If you perform an operation that returns a mask image buffer, you can use it as an uncalibrated image mask, provided its size matches the source container's range component. For example, you can create an uncalibrated image mask using the background of a point cloud. You can fit a 3D plane geometry object to the background of your point cloud, using M3dmetFit() with M_PLANE. Next, retrieve the size of the source point cloud's range component, using M3dmetGetResult() with M_RESULT_IMAGE_SIZE_X and M_RESULT_IMAGE_SIZE_Y, and allocate an image buffer of this size using MbufAlloc2d(). You can then use M3dmetCopyResult() with M_INLIER_MASK to copy the mask constructed by the points that were considered inliers during the fit into the buffer. You can then pass the buffer as an uncalibrated image mask to M3dimCrop(); if you specify M_INVERSE when calling M3dimCrop(), you will remove only those points that fit your background plane from the point cloud, leaving your foreground points untouched. You can also create an uncalibrated image mask using M3dimOutliers(). In this case, your point cloud contains anomalous points, known as outliers, which might represent noise introduced during point cloud acquisition. Use M3dimOutliers() to create a mask image in which pixels that correspond to outlier points are set to a non-zero value, while the rest of the pixels are set to zero. When passing this mask to M3dimCrop(), specify M_INVERSE to crop the outlier points, since these are identified with non-zero values in the mask. See the Invalidating outlier points subsection of this section. With a uniformly-calibrated image mask, M3dimCrop() projects points orthogonally onto the mask, which means that points are projected parallel to the Z-axis, onto the XY-plane. To create such a mask, you can start by generating a fully corrected depth map of the point cloud using M3dimProject(), which orthogonally projects points onto the XY (Z=0) plane. Next, you can use the Model Finder module to locate the target object in the resulting depth map, and then clear to 0 unwanted pixels as required. Alternatively, you can use a graphical user interface to interactively block out or mask areas in the depth map image, before passing the mask image to M3dimCrop(). The following image shows cropping using a uniformly-calibrated mask image. You can also crop using a 3D-calibrated image mask. In this case, M3dimCrop() projects points along paths that follow the frustum of the camera's view (instead of orthogonally), as represented by the mask image's associated camera calibration information. To do so, use an image of the scene that is associated with a camera calibration of type M_TSAI_BASED or M_3D_ROBOTICS. This is useful when selecting points using a graphical user interface, where you can select or block out points on screen, thereby defining the mask image. Since the camera image is 3D-calibrated, the region selected from the display specifies points within a 3D volume. To apply the crop, pass the calibrated mask image buffer to M3dimCrop(). Specifying the organization type When calling M3dimCrop(), you can specify how to organize the resulting point cloud. The following settings are available: Unorganized (M_UNORGANIZED). Specifies to store results in an unorganized point cloud, removing any invalid points in the process. This option typically uses less memory, at the cost of losing organization. Shrink (M_SHRINK). Specifies to copy, from source to destination, the smallest region that holds all valid points. With this option, point cloud organization is kept. This is useful when cropping a small, localized area of a larger, organized point cloud. Same (M_SAME). Specifies to keep the source point cloud's organization, and to modify only the M_COMPONENT_CONFIDENCE component, invalidating the cropped points by setting their confidence to 0; all other source components are copied unchanged into the destination container. M_SAME is a fast option if cropping in place, since no separate allocation or copy operations are required. The M_SAME option is also useful if the relation to the original point positions are needed in the resulting point cloud. M_SAME is the default option. For information on point cloud organization, see the Organized and unorganized point clouds subsection of this section. Fixturing You can optionally apply a transformation matrix on your point cloud to fixture it before performing the cropping operation. To do so, use M3dimCrop() with a Transformation matrix object identifier. The matrix object must have been previously allocated using M3dgeoAlloc() with M_TRANSFORMATION_MATRIX. The transformation matrix must be any valid affine transformation matrix, where the last row is (0,0,0,1). If you do not want to transform your point cloud, you can instead use M_IDENTITY_MATRIX to specify an identity matrix, which will not transform or rotate your point cloud in any way. Note that passing a transformation matrix to M3dimCrop() is the equivalent to first calling M3dimMatrixTransform(), and then calling M3dimCrop() on the resulting transformed point cloud. Typically, you store the resulting transformed points to a different container if your source point cloud has multiple instances of regions that need to be analyzed. For more information, refer to the How to fixture in 3D section of Chapter 45: Fixturing in 3D. Invalidating outlier points You can use M3dimOutliers() to invalidate outlier points. Point clouds or depth maps originating from 3D scans of a scene often contain a certain amount of noise, in the form of extraneous points (outliers). M3dimOutliers() determines outliers based on their distance from other points, the density of neighboring points, or by using statistical calculations. For example, you can specify to determine outliers based on the mean and standard deviation of the local average distance distribution, which considers the average distance of all points to their closest neighbors. M3dimOutliers() sets the confidence of outlier points to zero; the points will still exist but are invalidated. This function can also create a mask image that you can use to crop points. See the Cropping using a mask image subsection of this section. Alternatively, you can smooth a point cloud to reduce noise. See the Smoothing a point cloud subsection of this section. Defining which points to use When working with a point cloud, you can typically specify whether to use a 3D box geometry object to delimit which points to use for processing and analysis operations. You can also remove invalid points. By default, all valid points are used. You can use M3dimGet() to retrieve the component values that correspond to the valid points in the source point cloud container. You can define the size of the 3D box geometry by specifying coordinates and/or dimensions, such as X, Y, or Z lengths. See the Steps to defining and using a 3D geometry object subsection of the Defining 3D geometries section of Chapter 44: Using the 3D Geometry module for more information. Alternatively, you can use 3D box geometry results from other modules. The following code snippet is an example of the steps required to generate a fully corrected depth map from a box region of a point cloud: /* Crop the point cloud. */ M3dimCrop(MilPointCloud, MilCroppedPointCloud, MilBox, M_NULL, M_DEFAULT, M_DEFAULT); M3dimCalibrateDepthMap(MilBox, MilDepthmap, M_NULL, M_NULL, M_DEFAULT, M_POSITIVE, M_DEFAULT); /* Project the point cloud on the depth map. */ M3dimProject(MilCroppedPointCloud, MilDepthmap, M_NULL, M_POINT_BASED, M_MAX_Z, M_DEFAULT, M_DEFAULT); Specifying a bounding box excluding outliers To exclude outliers, you can use M3dimStat() to calculate the bounding box of a point cloud excluding outliers. In this case, M3dimStat() calculates the smallest axis-aligned box that contains all the valid points in a point cloud except those suspected to be outliers. You can then copy the resulting bounding box into a 3D box geometry, using M3dimCopyResult() with M_BOUNDING_BOX. For more information, see the Determining the bounding box subsection of the Calculating statistics on a point cloud, depth map, or 3D geometry section later in this chapter. Specifying the extent box To include only points in the maximum area that a depth map can span given the image buffer size and the calibration information of that buffer, you can copy the depth map's extent box into a 3D geometry object. In this case, the resulting 3D box geometry's dimensions are calculated to represent this maximum area. To do so, use M3dimCopy() with M_EXTENT_BOX. M3dimCopy() uses the following information from the depth map: the position of the origin of the pixel coordinate system in the relative coordinate system, the three dimensional scales (M_PIXEL_SIZE_X, M_PIXEL_SIZE_Y, and M_GRAY_LEVEL_SIZE_Z), and the dimensions of the depth map image buffer. Removing invalid points You can remove invalid points from a point cloud. For example, if you require only valid points for a processing operation, you can call M3dimGet(), which will retrieve a component's values that correspond only to valid points. The following code snippet shows how to retrieve the coordinates of only valid points from a container: std::vector&lt;MIL_FLOAT&gt; X; std::vector&lt;MIL_FLOAT&gt; Y; std::vector&lt;MIL_FLOAT&gt; Z; /* Retrieve valid points. */ M3dimGet(MilPointCloud, M_COMPONENT_RANGE, M_DEFAULT, M_DEFAULT, X, Y, Z); You can also call M3dimRemovePoints() to remove invalid points from the point cloud before performing another operation. For example, the following code snippet shows how to keep only points whose normals point upward: /* Remove invalid points and points whose normal vector values are zero. */ M3dimRemovePoints(MilPointCloud, MilPointCloud, M_INVALID_NORMALS, M_DEFAULT); /* Retrieve the confidence values and normals information for each point. */ MIL_ID Confidence = MbufInquireContainer(MilPointCloud, M_COMPONENT_CONFIDENCE, M_COMPONENT_ID, M_NULL); MIL_ID Normals = MbufInquireContainer(MilPointCloud, M_COMPONENT_NORMALS_MIL, M_COMPONENT_ID, M_NULL); /* Make points whose normals point roughly upward valid, and all other points invalid. */ auto NormalsZ = MbufChildColor(Normals, 2, M_UNIQUE_ID); MimBinarize(NormalsZ, Confidence, M_FIXED + M_GREATER, 0.9, M_NULL); Note that the above snippet assumes that the normals are of unit length, and considers a point's normal to point upward if its Z-component has a value greater than or equal to 0.9. To remove points whose confidence is equal to zero, use M3dimRemovePoints() with M_INVALID_POINTS_ONLY. To remove both these points and points whose normal vector values are zero, use M_INVALID_NORMALS instead. Note that, when using M3dimRemovePoints(), the resulting point cloud is always unorganized. Copying points to user-defined arrays You can use M3dimGetList() to copy a source container's values into destination arrays, while also choosing to keep or exclude invalid points. When calling M3dimGetList() with a point cloud container, you must specify from which component to copy values. The function has options for casting, storing the data in packed or planar format, and returns the number of points written to the arrays. You can also pass a depth map to M3dimGetList(). Smoothing a point cloud If the point cloud is noisy, you can also apply a smoothing filter to it, using M3dimFilter(). This function reduces noise that can arise during point cloud acquisition. Point cloud noise can make subsequent processing less accurate (for example, reconstructing a surface using M3dimMesh()). The following image shows a noisy point cloud that has been smoothed with a moving least squares (MLS) filter (M_SMOOTH_MLS) and a bilateral filter (M_SMOOTH_BILATERAL) (M3dimControl() with M_FILTER_MODE). Although both results are similar, the bilateral filter result maintains sharper edges (visible in the image where the top surface meets the front surface). The MLS filter applies smoothing equally to surface and edge points. The bilateral filter, on the other hand, smooths points on faces, leaving edge points much less affected. Deciding which filter to use depends on your particular point cloud and the requirements of your application. If edge preservation is not a concern, use the MLS filter since it is easier to set, requiring less fine-tuning with the control types mentioned above. To preserve edges or folds, use the bilateral filter. With the bilateral filter, if you set the edge-preserving control (M_NORMALS_WEIGHT_FACTOR) to a value that is too low, little smoothing occurs, even to non-edge points; if set too high, results will resemble an MLS filter output. The bilateral filter uses the source point cloud's normal values. If a normals component does not already exist in your source point cloud, the M3dimFilter() function will calculate point normals and create a normals component. In this case, use M3dimControl() with M_USE_SOURCE_NORMALS set to M_FALSE. Note that this is more efficient than separately calculating a normals component using M3dimNormals(). Nevertheless, the method used for computing normals could improve filter results. Typically, the bilateral filter works best with normals that were calculated using the M_MESH or M_TREE neighbor search mode (M3dimControl() with M_NEIGHBOR_SEARCH_MODE). However, normals created using the M_ORGANIZED mode can give faster filtering results. When using the bilateral filter, you can adjust how well edges are preserved with M_NORMALS_WEIGHT_FACTOR. A smaller value results in more edge preservation. You can also set the M_WEIGHT_MODE control type (available for either the bilateral or MLS filter). M_WEIGHT_MODE sets how the Gaussian weights are determined for the smoothing operation. Typically, for bilateral smoothing, the weight mode is best left at the default setting (M_RELATIVE), since the M_ABSOLUTE setting is dependent on point cloud resolution, which can make it difficult to accurately configure the other filter context control settings. Note that both the bilateral and MLS filters are expensive in terms of processing time. However, applying a smoothing filter to reduce noise results in a more accurate point cloud representation. For example, in the image above, the original point cloud was acquired from a box with flat sides. Despite the smooth-sided source, the acquisition process introduced significant noise. Once filtered, the resulting smoothed point cloud is a superior representation of the original object that you can use for further processing. Smoothing a point cloud using M3dimFilter() changes the positions of points, but does not eliminate them. If you want to invalidate outlier points, use M3dimOutliers(). Merging point clouds You can use M3dimMerge() to combine multiple point clouds into a single point cloud. The function assumes the working coordinate systems of the point clouds are aligned. Optionally, you can pass M3dimMerge() a subsample context (discussed below) when merging, to reduce the number of points in the resulting point cloud. Note that, if you pass a subsample context that specifies an organized destination point cloud, the resulting merged point cloud will be organized. Without such a context, the resulting point cloud is always unorganized when using M3dimMerge(). For information on point cloud organization, see the Organized and unorganized point clouds subsection of this section. To align the working coordinate systems of multiple point clouds and then merge, use M3dregMerge() instead. See the Merging point clouds, retrieving specific results, and drawing results section of Chapter 40: 3D registration for more information. Calculating unit normal vectors You can calculate each point's unit normal vector, which is a vector perpendicular to a surface estimated from the positions of surrounding points. Normal vectors are useful, for example, when reconstructing a surface for the point cloud using M3dimMesh() (for certain surface reconstruction modes only), or for registration operations using the 3D Registration module. To calculate the normal vectors, use M3dimNormals(). This function stores the unit normal vector of each point in the normals component of the operation's destination container, creating or reallocating the component if necessary. Note that, to increase calculation speed, M3dimNormals() can use either existing organizational information in the point cloud or an existing mesh component. Once you have calculated the normals of a point cloud, you can draw the normal vectors as lines in a 3D display. To do so, use M3dimGet() to retrieve the coordinates of points from the point cloud's range component; then, call M3dimGet() again to retrieve the X-, Y-, and Z-components of the normal vectors from the normals component. Finally, pass the coordinates and vector components to M3dgraLines() while also specifying M_POINT_AND_VECTOR. Surface reconstruction You can pass a point cloud to M3dimMesh() to generate a mesh, which is a reconstructed triangulated surface for the point cloud. Creating a mesh can improve results for some 3D operations. For example, when using M3dimProject() to generate a depth map from an organized point cloud, the source point cloud can have widely varying distances between points, depending on the position of objects and surfaces during acquisition. This can happen when using a snapshot type of 3D sensor, which captures the point cloud from a single point of view. If you first generate a mesh for the point cloud before projecting it, the resulting depth map is often more satisfactory, and eliminates the need to fill excessively large gaps. In other cases, you might have a point cloud generated from a 3D profile sensor, where you have regular gaps in one dimension. If you first create a mesh, projecting the mesh-based point cloud typically yields better results. In general, a depth map produced from a meshed point cloud results in a more natural-looking reconstructed surface. When displaying point clouds, a meshed point cloud typically shows surface texture better than non-meshed point clouds that are displayed as individual points. The M3dimMesh() function generates a mesh according to the specified mesh 3D image processing context (M3dimAlloc() with M_MESH_CONTEXT). Use M3dimControl() to set up the context. Choose the reconstruction mode (M_MESH_MODE) with which to create the mesh. You can choose from the following modes: Organized mode (M_MESH_ORGANIZED). Connects existing points according to organizational information, and can result in a mesh with holes, due to invalid points in the source point cloud. Local plane mode (M_MESH_LOCAL_PLANE). Connects existing points, using local planes computed from each point's neighborhood to establish which neighbors to connect. With this mode, the source point cloud can be organized or unorganized and must have a normals component. The resulting mesh can have holes. Smoothed mode (M_MESH_SMOOTHED). Generates a smooth surface even when the source point cloud is noisy; missing data is approximated when possible, resulting in significantly fewer holes. This mode requires a normals component. Note that when using the smoothed mode, there will probably be more points in the resulting point cloud and the points will probably not be the same as that of the source point cloud. The following images show the result of applying the organized, local plane, and smoothed mesh modes to the same point cloud. Note the regularity of the mesh produced using the organized mode. This is especially evident on flat or smoothly transitioning surfaces; however, where there are sharper transitions and edges, the mesh is not as smooth, since the operation is simply connecting neighboring points. The hole above the eye is due to the specified step interval (M_MESH_STEP_...), which in this case is smaller than the point spacing in the original point cloud. Increasing the step interval might remove a few holes, at the cost of losing detail, since the triangular planes that form the mesh would become larger. Regardless, a mesh with holes remains a possibility, since the specified step interval can still land on a point whose confidence is 0 (an invalid point), preventing the formation of a mesh triangle at that point. For the local plane mesh, the surface triangles are less regular. This is due in large part to the relatively broad settings of the M_SURFACE_ANGLE_MAX and M_TRIANGLE_ANGLE_... control types specified for this example. Holes in the mesh appear where the specified angle constraints were not met (or where the resulting surface triangle would be too large). You can set stricter values for these control types to obtain a higher quality mesh; however, more holes are likely. Typically for the local plane mode, there is a trade-off between mesh quality and number of holes. The smoothed mesh is much smoother than the others, since noise reduction was applied to the point cloud before reconstructing the surface. In this case, the operation has filled holes, although hole filling is not guaranteed when using the smoothed mesh mode. The suitability of this mode's smoothing properties depends on your application. M3dimMesh() stores the generated mesh in the mesh component of the destination container. Once you have generated a mesh, you can use M3dimRemovePoints() with M_NOT_MESH_POINTS to remove from the point cloud all points whose confidence is equal to zero, as well as points that are not part of the mesh. Note that the resulting point cloud after an M3dimRemovePoints() operation is always unorganized. Sampling a point cloud or a 3D geometry You can use the M3dimSample() function to reduce the number of points in a point cloud (subsampling), or increase the number of points in a meshed point cloud (surface sampling). You can also surface sample a 3D geometry to generate a point cloud. Subsampling is useful, for example, to maintain an approximately similar distance between points after a merge operation, or when compressing point cloud data. You can also subsample an unorganized point cloud to obtain an organized point cloud. Subsampling a point cloud To subsample, allocate a subsample 3D image processing context using M3dimAlloc() with M_SUBSAMPLE_CONTEXT and set the type of subsampling operation to perform using M3dimControl() with M_SUBSAMPLE_MODE. Choose from the following modes: Decimate mode (M_SUBSAMPLE_DECIMATE). Selects points at regular intervals (set with M_STEP_SIZE_X and M_STEP_SIZE_Y). This is the default mode and is typically used with organized point clouds. This mode is typically faster than the other modes. The following code snippet is an example of the steps required to subsample a point cloud in decimate mode: /* Allocate a subsample context and set it to decimate mode. */ MilSubsampleContext = M3dimAlloc(MilSystem, M_SUBSAMPLE_CONTEXT, M_DEFAULT, M_NULL); M3dimControl(MilSubsampleContext, M_SUBSAMPLE_MODE, M_SUBSAMPLE_DECIMATE); /* Set the interval size and specify to keep the organization type. */ M3dimControl(MilSubsampleContext, M_STEP_SIZE_X, DECIMATION_STEP); M3dimControl(MilSubsampleContext, M_STEP_SIZE_Y, DECIMATION_STEP); M3dimControl(MilSubsampleContext, M_ORGANIZATION_TYPE, M_ORGANIZED); /* Apply sampling. */ M3dimSample(MilSubsampleContext, MilPointCloud, MilDstPointCloud, M_DEFAULT); Note that M_SUBSAMPLE_DECIMATE can sometimes generate undesirable regular patterns. To avoid such patterns, use random mode (M_SUBSAMPLE_RANDOM) instead. Random mode (M_SUBSAMPLE_RANDOM). Selects points using an operation that randomly selects a specified fraction of points from the source point cloud. Set the seed for the random number generator with M_SEED_VALUE; set the fraction with M_FRACTION_OF_POINTS. Geometric mode (M_SUBSAMPLE_GEOMETRIC). Selects points using a geometrically stable subsampling operation. This mode is intended to help 3D registration converge faster and avoid local minima. The underlying algorithm identifies and rejects points that are likely to cause registration to slide away from convergence, while retaining points that constrain rotational and translational transforms during registration. You can set the fraction of points that are retained from the source point cloud with M_FRACTION_OF_POINTS. Grid mode (M_SUBSAMPLE_GRID). Selects points using a grid operation, which divides the 3D space into cells. For each cell, M3dimSample() selects one point: either the point closest to the cell's center, or the selection is determined using the mode specified with M_POINT_SELECTED. Set the cell size with M_GRID_SIZE_.... You can use grid mode to organize an unorganized point cloud (set M_ORGANIZATION_TYPE to M_ORGANIZED). Grid mode is also useful after merging point clouds (using M3dimMerge()) to ensure an approximately constant spacing between points. Note that if you want the grid operation to select a particular fraction of points from the source point cloud, you can use M3dimLattice() to calculate the cell size required. Set the fraction of points with M_FRACTION_OF_POINTS and M_FRACTION_OF_POINTS_TOLERANCE before calling M3dimLattice(). You can store the lattice results in a subsample 3D image processing context, or copy the lattice results into a subsample 3D image processing context, using M3dimCopyResult() with M_SUBSAMPLE_CONTEXT. This sets the M_SUBSAMPLE_MODE control type of the subsample 3D image processing context to M_SUBSAMPLE_GRID, its M_GRID_SIZE_... control types to the calculated lattice's cell sizes, and, if 1 of the cell dimensions is infinite, its M_ORGANIZATION_TYPE control type to M_ORGANIZED. The following code snippet is an example of the steps required to subsample an unorganized point cloud using the grid mode, and generate a new, organized point cloud: static const MIL_DOUBLE GRID_SIZE = 1.0; /* Allocate a subsample context and set it to grid mode. */ MilSubsampleContext = M3dimAlloc(MilSystem, M_SUBSAMPLE_CONTEXT, M_DEFAULT, M_NULL); M3dimControl(MilSubsampleContext, M_SUBSAMPLE_MODE, M_SUBSAMPLE_GRID); /* Organize the unorganized source point cloud with resolution in X and Y equal to the grid size. */ M3dimControl(MilSubsampleContext, M_ORGANIZATION_TYPE, M_ORGANIZED); M3dimControl(MilSubsampleContext, M_GRID_SIZE_Z, M_INFINITE); M3dimControl(MilSubsampleContext, M_GRID_SIZE_X, GRID_SIZE); M3dimControl(MilSubsampleContext, M_GRID_SIZE_Y, GRID_SIZE); /* Select the point with the lowest coordinate along the M_INFINITE axis for each cell. */ M3dimControl(MilSubsampleContext, M_POINT_SELECTED, M_MIN); /* Apply sampling. */ M3dimSample(MilSubsampleContext, MilPointCloud, MilDstPointCloud, M_DEFAULT); Note that, when M_ORGANIZATION_TYPE is set to M_ORGANIZED, exactly 1 of M_GRID_SIZE_X, M_GRID_SIZE_Y, or M_GRID_SIZE_Z must be set to M_INFINITE. When M_ORGANIZATION_TYPE is set to M_UNORGANIZED, you can set at most 1 of M_GRID_SIZE_... to M_INFINITE. When one of M_GRID_SIZE_... is set to M_INFINITE, the point selected for a cell depends on the mode specified with M_POINT_SELECTED, which is, by default, the highest point along the M_INFINITE axis (M_MAX). When none of M_GRID_SIZE_... is set to M_INFINITE, the point selected for a cell is the point closest to the cell's volumetric center. The following image shows options for the M_POINT_SELECTED control type. Note that the M_CENTER option selects the point closest to the center of each 2D cell, as if the point cloud is collapsed onto the plane formed by the 2 non-infinite dimensions. Normal mode (M_SUBSAMPLE_NORMAL). Selects points based on normal vector angles and neighborhood distance (set with M_DISTINCT_ANGLE_DIFFERENCE and M_NEIGHBORHOOD_DISTANCE). This mode is best used when compressing the source point cloud. Points around key features like corners and edges are kept, while the number of points on flat surfaces is reduced. The following image shows the result of using the normal mode with M_DISTINCT_ANGLE_DIFFERENCE and M_NEIGHBORHOOD_DISTANCE. The following code snippet is an example of the steps required to subsample a point cloud in normal mode. Note the specified M_DISTINCT_ANGLE_DIFFERENCE and M_NEIGHBORHOOD_DISTANCE values. The 12 degree distinct angle difference specifies that the point is kept if all neighboring points' normal vectors differ by more than 12 degrees from the examined point's normal vector angle. The neighborhood within which to consider the normal vector of surrounding points is limited to 10 world units from the examined point. /* Allocate a subsample context and set it to normal vector mode. */ MilSubsampleContext = M3dimAlloc(MilSystem, M_SUBSAMPLE_CONTEXT, M_DEFAULT, M_NULL); M3dimControl(MilSubsampleContext, M_SUBSAMPLE_MODE, M_SUBSAMPLE_NORMAL); /* Control what makes a point distinct from its neighbors. */ M3dimControl(MilSubsampleContext, M_DISTINCT_ANGLE_DIFFERENCE, 12); M3dimControl(MilSubsampleContext, M_NEIGHBORHOOD_DISTANCE, 10); /* Apply sampling. */ M3dimSample(MilSubsampleContext, MilPointCloud, MilDstPointCloud, M_DEFAULT); Note that you can specify the organizational type of the resulting point cloud (M_ORGANIZATION_TYPE). An M_ORGANIZED output point cloud is available only for M_SUBSAMPLE_DECIMATE or M_SUBSAMPLE_GRID operations. When M_ORGANIZATION_TYPE is set to M_UNORGANIZED, invalid points are removed. For information on point cloud organization, see the Organized and unorganized point clouds subsection of this section. Surface sampling a meshed point cloud You can use M3dimSample() to surface sample a meshed point cloud and generate a denser (non-meshed) point cloud. This is useful, for example, when performing 3D registration with an acquired point cloud and a CAD model. A point cloud restored from a CAD file is often sparse, with large surface triangles on flat surfaces. You can apply a surface sample operation to the CAD-sourced point cloud to increase its density and subsequently improve registration results, since both point clouds should have a similar point density. Note that you can use MbufImport() or MbufRestore() to import/restore an acquired point cloud or a CAD file; for a CAD file, the data must have been saved in the PLY or STL file format. To perform surface sampling, use M3dimSample() with a surface sample 3D image processing context, allocated using M3dimAlloc() with M_SURFACE_SAMPLE_CONTEXT. A M_COMPONENT_MESH_MIL component must exist in the source point cloud. Use M3dimControl() with M_RESOLUTION to control the density of newly added points. M3dimSample() samples the surface, and places a denser (non-meshed) point cloud into the destination container. Surface sampling a 3D geometry You can also sample a finite 3D geometry to generate a point cloud. To do so, use M3dimSample() with a surface sample 3D image processing context, allocated using M3dimAlloc() with M_SURFACE_SAMPLE_CONTEXT. You must pass a 3D geometry object that you have previously allocated using M3dgeoAlloc() with M_GEOMETRY. Note that infinite 3D geometries, such as infinite cylinders, lines, or planes are not supported. M3dimSample() generates the components M_COMPONENT_RANGE and M_COMPONENT_CONFIDENCE. The function also generates a M_COMPONENT_NORMALS_MIL component for box, cylinder, and sphere 3D geometries, but not for points or lines. Organized and unorganized point clouds A point cloud can be organized or unorganized. In an organized point cloud, points close to each other in 3D space are stored close to each other in the pixel coordinate systems of the components. Typically, you can ignore whether a point cloud is organized; most operations are supported for both organized and unorganized point clouds. However, in some cases, you can improve performance with an organized or unorganized point cloud. You can determine if a point cloud is organized using MbufInquireContainer() with M_COMPONENT_RANGE and M_3D_REPRESENTATION. If the returned value is M_CALIBRATED_XYZ_UNORGANIZED, the point cloud is unorganized; otherwise, it is organized. Setting the 3D representation of a range component to indicate that it is organized when the underlying data is unorganized can lead to unexpected results. Some operations have modes that can only be specified for an organized point cloud. For example, for a normals 3D image processing context, you can use M3dimControl() with M_NEIGHBOR_SEARCH_MODE set to M_ORGANIZED. In this case, M3dimNormals() will use the organization of the point cloud to calculate the normals more efficiently. Modes specific to organized point clouds are not available when the range component is a buffer with a single row (M_SIZE_Y is equal to 1), even if the data is organized. If you have an organized point cloud with a large number of invalid points, it might be more efficient to convert it to an unorganized point cloud with only valid points (using M3dimRemovePoints()). This reduces the memory required to store the point cloud, and improves the performance of some operations. Typically, this will only improve the performance of your application if you need to perform multiple operations with the same point cloud. Typically, 3D data grabbed from a 3D sensor is organized. A point cloud converted from a depth map using MbufConvert3d() is always organized. You can create an organized point cloud that has a subset of the points of an unorganized point cloud by subsampling it using M3dimSample() with a subsample 3D image processing context that specifies a M_SUBSAMPLE_GRID operation and an M_ORGANIZED organization type. Working with points in a point cloud Cropping or masking points Cropping using a 3D geometry Cropping using a mask image Specifying the organization type Fixturing Invalidating outlier points Defining which points to use Specifying a bounding box excluding outliers Specifying the extent box Removing invalid points Copying points to user-defined arrays Smoothing a point cloud Merging point clouds Calculating unit normal vectors Surface reconstruction Sampling a point cloud or a 3D geometry Subsampling a point cloud Surface sampling a meshed point cloud Surface sampling a 3D geometry Organized and unorganized point clouds ",
      "wordCount": 5489,
      "subEntries": []
    },
    {
      "id": "UG_3D_Image_processing_Calculating_statistics_on_a_point_cloud_or_depth_map",
      "version": null,
      "title": "Calculating statistics on a point cloud, depth map, or 3D geometry",
      "subTitles": [
        "Steps to performing a statistics calculation",
        "Determining the bounding box",
        "Determining the centroid",
        "Determining distances between points",
        "Determining surface variation",
        "Determining numbers of points",
        "Determining principal component analysis (PCA) statistics",
        "Moments"
      ],
      "location": "MIL UG P05: 3D processing and analysis",
      "pageURL": "content\\UserGuide\\3D_Image_processing\\Calculating_statistics_on_a_point_cloud_or_depth_map.htm",
      "text": " Calculating statistics on a point cloud, depth map, or 3D geometry You can extract a variety of statistical information from a point cloud, depth map, or 3D geometry. Using M3dimStat(), you can calculate: The bounding box (with or without outlier points). The centroid of a point cloud, depth map, or 3D geometry. Distances between 3D points and their neighbors. Surface variation. Number of points. Principle component analysis (PCA) statistics, including PCA statistics on a point cloud's unit normal vectors. Moments. Note that if you are extracting statistical information from a 3D geometry, only bounding box and centroid calculations are supported by M3dimStat(). If you require statistics from distance measurements between a point cloud or depth map and a reference object, use M3dmetStat(). See the Calculating distance measurements and statistics section of Chapter 39: 3D metrology for more information. Except when otherwise mentioned, everything in this section that refers to point clouds also includes depth maps. The image below shows a point cloud with six spheres, each surrounded by a bounding box. M3dimStat() is used to calculate the bounding box of each sphere. Steps to performing a statistics calculation The following steps provide a basic methodology for calculating statistics using the 3D Image Processing module. Optionally, allocate a custom statistics 3D image processing context using M3dimAlloc() with M_STATISTICS_CONTEXT. With a custom context, you can enable and calculate multiple statistics. Note that if you want to evaluate a single statistics calculation, you can call M3dimStat() with a predefined statistics 3D image processing context, without having to allocate a statistics 3D image processing context beforehand. M3dimStat() will calculate the required statistics using default settings. Allocate a statistics 3D image processing result buffer using M3dimAllocResult() with M_STATISTICS_RESULT. If required, use M3dimControl() to enable the required statistics calculation (for example, to enable bounding box statistics, set M_BOUNDING_BOX to M_ENABLE). This step is necessary if you are using a custom statistics 3D image processing context. Call M3dimStat() with an appropriate statistics context (custom or predefined), a statistics result buffer or a 3D geometry object, and the point cloud container, depth map image buffer, or 3D geometry object for which to calculate the required statistics. Retrieve results using M3dimGetResult() and M3dimCopyResult(). Determining the bounding box The bounding box of a point cloud or 3D geometry is the smallest axis-aligned box that contains, by default, either all the valid points in the point cloud or the entire 3D geometry object, respectively. To calculate the bounding box, specify the M_STAT_CONTEXT_BOUNDING_BOX predefined context, or use a custom statistics 3D image processing context with M_BOUNDING_BOX enabled. You can compute the bounding box's dimensions, as well as its minimum and maximum coordinate points. You can also calculate a semi-oriented bounding box, which is, by default, axis-aligned in Z but not in X and Y. To do so, specify the M_STAT_CONTEXT_SEMI_ORIENTED_BOX predefined context or use a custom statistics 3D image processing context with M_BOUNDING_BOX enabled and M_SEMI_ORIENTED specified. You can change the axis in which the semi-oriented bounding box is aligned, using M_BOX_SEMI_ORIENTED_ROTATION_AXIS. The default algorithm (M3dimControl() with M_BOUNDING_BOX_ALGORITHM set to M_ALL_POINTS) calculates a bounding box that includes all valid points. You can refine your bounding box, typically to limit outliers, using M3dimControl() with M_BOUNDING_BOX_ALGORITHM set to M_ROBUST. In this case, you must specify the outlier ratio with M_BOUNDING_BOX_OUTLIER_RATIO_X, M_BOUNDING_BOX_OUTLIER_RATIO_Y, and M_BOUNDING_BOX_OUTLIER_RATIO_Z. The outlier ratio is the fraction of the total number of points whose coordinates place the points in outlier positions, and therefore outside of the bounding box. For example, if you set all M_BOUNDING_BOX_OUTLIER_RATIO_... control types to 0.01, the calculation rejects the furthest 1 percent of points. You can directly write the bounding box to a 3D geometry object if you have allocated one. To do so, specify the identifier of the 3D geometry object when calling M3dimStat() instead of specifying the identifier of a statistics 3D image processing result buffer. Writing the bounding box to a 3D geometry object will establish it as a 3D box geometry object. If you passed a statistics 3D image processing result buffer to M3dimStat(), you can copy the bounding box from the result buffer to a 3D geometry object using M3dimCopyResult() with M_BOUNDING_BOX, which will establish it as a 3D box geometry object. You can also copy the coordinates of the center of the bounding box into a 3D geometry object, establishing it as a 3D point geometry object. Whereas, you can use M3dimGetResult() to retrieve the bounding box's dimensions (M_SIZE_X, M_SIZE_Y, and M_SIZE_Z), the minimum/maximum coordinates of the bounding box (M_MIN_... or M_MAX_...), and the coordinates of the center of the bounding box (M_BOX_CENTER_X, M_BOX_CENTER_Y, and M_BOX_CENTER_Z). If you want to scale your entire point cloud to dimensions at or near one unit, refer to the Transforming points using a transformation matrix subsection of the Moving or scaling a point cloud or 3D geometry section earlier in this chapter. Alternatively, if you have previously allocated a transformation matrix object, using M3dgeoAlloc() with M_TRANSFORMATION_MATRIX, you can copy, into the transformation matrix, the bounding box statistics results that will transform the bounding box into a unit box. To do so, use M3dimCopyResult() with M_NORMALIZATION_MATRIX. The unit box's maximum length is 1, except in the case of a signed unit box, whose maximum length is 2. You can set whether the normalization matrix should transform the point cloud to fit a signed or unsigned unit box, using M3dimControl() with M_NORMALIZATION_MODE. You can also use M_NORMALIZATION_SCALE to set how to apply the scale values when transforming the point cloud to fit a unit box. Note that this copy operation is not available for a semi-oriented bounding box. Scaling the bounding box to a unit box provides numeric stability. For example, if your point cloud is very large and you need to perform calculations using the dimensions of the point cloud's bounding box, it can be useful to transform it into a unit box, such that all dimensions are of order unity. You can also be certain that all points are included in the scaling, since by definition the bounding box encloses all valid points. Further, if you need to discretize your data, having well-defined boundaries on all three axes of the bounding box is useful. For example, it can be easier to perform a binning operation on the points in the bounding box if the dimensions of the box are all integer values. Determining the centroid The centroid of a point cloud or 3D geometry is its center of mass. To calculate the centroid, specify the M_STAT_CONTEXT_CENTROID predefined context or use a custom statistics 3D image processing context with M_CENTROID enabled. Centroid statistics include the X-, Y-, and Z-coordinates of the point cloud or 3D geometry's centroid. To retrieve these results, use M3dimGetResult() with M_CENTROID_X, M_CENTROID_Y, and M_CENTROID_Z. You can directly write the centroid to a 3D geometry object if you have allocated one. To do so, specify the identifier of the 3D geometry object when calling M3dimStat() instead of specifying the identifier of a statistics 3D image processing result buffer. Writing the centroid to a 3D geometry object will establish it as a 3D point geometry object. If you passed a statistics 3D image processing result buffer to M3dimStat(), you can copy the centroid from the result buffer to a 3D geometry object, using M3dimCopyResult() with M_CENTROID. Copying the centroid to a 3D geometry object will also establish it as a 3D geometry point object. Determining distances between points To compute distance-to-nearest-neighbor statistics, specify the M_STAT_CONTEXT_DISTANCE_TO_NEAREST_NEIGHBOR predefined context or use a custom statistics 3D image processing context with M_DISTANCE_TO_NEAREST_NEIGHBOR enabled. When using a custom context, you must also enable the required statistical calculation(s) (M_CALCULATE_...). You can calculate the average distance, the median distance, the maximum distance, and the minimum distance of points to their nearest neighbor. You can also calculate the standard deviation and the robust standard deviation of these distances. Choose the robust calculation to minimize the influence of outlier points (for example, due to noise), since such outliers can skew data in a non-robust standard deviation calculation. To retrieve distance-to-nearest-neighbor statistics results, use M3dimGetResult() with M_DISTANCE_TO_NEAREST_NEIGHBOR_.... Determining surface variation A point cloud's surface variation is a measure of its noise level. When calculating surface variation statistics, MIL fits a local plane to points in a neighborhood and measures the perpendicular distance of the point in question to the plane. Greater distances mean a noisier point cloud. To compute surface variation statistics, use a custom statistics 3D image processing context with M_SURFACE_VARIATION enabled. You must also enable the required surface variation calculation(s) (M_CALCULATE_...). You can calculate the average distance, the median distance, the maximum distance, and the minimum distance to the local plane, for all points in the point cloud. You can also calculate the standard deviation and the robust standard deviation of these distances. Choose the robust calculation to minimize the influence of outlier points (for example, due to noise), since such outliers can skew data in a non-robust standard deviation calculation. To retrieve surface variation statistics results, use M3dimGetResult() with M_SURFACE_VARIATION_.... Determining numbers of points To compute number-of-points statistics, specify the M_STAT_CONTEXT_NUMBER_OF_POINTS predefined context or use a custom statistics 3D image processing context with M_NUMBER_OF_POINTS enabled. You can calculate the total number of points, the number of points that are missing data (invalid points), and the number of valid points. Note that the sum of valid and invalid points equals the total number of points. To retrieve these results, use M3dimGetResult() with M_NUMBER_OF_POINTS_TOTAL, M_NUMBER_OF_POINTS_MISSING_DATA, or M_NUMBER_OF_POINTS_VALID. Determining principal component analysis (PCA) statistics The principal components of a dataset (such as 3D points distributed in a point cloud) are the directions along which there is the most variance, or spread, in the data. The first principal component (or principal axis) represents the maximum variance; the second principal component represents the second-most variance, perpendicular to the first; and the third principal component is perpendicular to the first two components. To compute principal component analysis (PCA) statistics, specify the M_STAT_CONTEXT_PCA predefined context or use a custom statistics 3D image processing context with M_PCA enabled. You can also compute PCA statistics on the point cloud's unit normal vectors. To do so, specify the M_STAT_CONTEXT_PCA_NORMALS predefined context or use a custom context with M_PCA enabled, M_PCA_MODE set to M_ORDINARY, and M_COMPONENT_OF_INTEREST set to M_COMPONENT_NORMALS_MIL. To retrieve these results, use M3dimGetResult() and specify the required result type. You can retrieve the components of the normalized vectors along the computed principal axes (M_PRINCIPAL_AXIS_...), the related eigenvalues (M_EIGENVALUE_...), as well as corresponding moments results (for example, M_COVARIANCE_MATRIX). Additionally, you can retrieve numbers-of-points statistics for the PCA calculations (for instance, M_NUMBER_OF_POINTS_VALID). Other available results include the component for which the statistics were calculated (M_COMPONENT_OF_INTEREST) and whether the results were measured relative to the object's centroid or the origin (M_PCA_MODE). Moments Moments statistics calculations reflect the distribution of matter around a point or axis. To compute moments statistics, use a custom statistics 3D image processing context with M_MOMENTS enabled. You must also specify the order up to which moments are calculated (M_MOMENT_ORDER). Note that MIL calculates the moments of the specified order, as well as all lower order moments. For example, setting the order to 3 will calculate third order moments, but also second and first order moments. Retrieve these results using M3dimGetResult(). You can retrieve moments statistics results with respect to the origin (ordinary moments), or with respect to the centroid (central moments); use the M_MOMENT_XYZ() and M_MOMENT_CENTRAL_XYZ() result types, respectively. When setting the moment order (M_MOMENT_ORDER) before calculating these results, note that the specified order must be a value that agrees with certain restrictions as per the ordinary or central moment formula (see M3dimGetResult()). Specifically, certain ordinary or central moment results require that the sum of the powers in X, Y, and Z not exceed a specific value. For instance, to retrieve PCA moments results, PowerX + PowerY + PowerZ must not exceed 2. Beyond the M_MOMENT_XYZ() and M_MOMENT_CENTRAL_XYZ() result types, you can retrieve several moments-based results, provided the specified moment order is sufficient. For example, to retrieve M_CENTROID_... or M_COMPONENT_OF_INTEREST results, the specified moment order must be at least 1. Most other moments statistics results require a minimum moment order of 2. These include M_COVARIANCE_MATRIX, M_EIGENVALUE_..., M_PCA_MODE, M_PRINCIPAL_AXIS_..., and M_STANDARD_DEVIATION_... results. Note that number-of-points statistics are also available for moments calculations (for example, M_NUMBER_OF_POINTS_VALID), and do not require a specific moment order. Note that enabling some other statistics calculations is equivalent to enabling moments statistics calculations with a particular order. For example, using M3dimControl() to enable centroid calculations is the same as enabling moments calculations and setting the moment order to 1. Similarly, enabling PCA calculations is the same as enabling moments and setting the moment order to 2. Calculating statistics on a point cloud, depth map, or 3D geometry Steps to performing a statistics calculation Determining the bounding box Determining the centroid Determining distances between points Determining surface variation Determining numbers of points Determining principal component analysis (PCA) statistics Moments ",
      "wordCount": 2153,
      "subEntries": []
    },
    {
      "id": "UG_3D_Image_processing_Analyzing_the_profile_of_a_3D_cross_section",
      "version": null,
      "title": "Analyzing the profile of a 3D cross-section",
      "subTitles": [
        "Taking a profile",
        "Setting the slicing plane according to a reference object",
        "Analyzing a profile"
      ],
      "location": "MIL UG P05: 3D processing and analysis",
      "pageURL": "content\\UserGuide\\3D_Image_processing\\Analyzing_the_profile_of_a_3D_cross_section.htm",
      "text": " Analyzing the profile of a 3D cross-section MIL allows you to take the profile of a point cloud, 3D geometry, mesh, or depth map, along a specified slicing plane. This can be useful, for example, to analyze and verify the contour of an object's surface along a slice. Taking a profile To take the 3D profile of a point cloud, 3D geometry, or mesh along a slicing plane, use M3dimProfile() with M_PROFILE_POINT_CLOUD, M_PROFILE_GEOMETRY, or M_PROFILE_MESH, respectively. M3dimProfile() allows you to limit the size of the slicing plane, so it requires an oriented plane (a plane with an origin and two in-plane axes). Therefore, instead of a geometry object, the function takes a transformation matrix that defines a coordinate system whose XY-plane (Z=0) is used as the slicing plane. You can establish this new coordinate system, using M3dgeoMatrixSetWithAxes() with M_COORDINATE_SYSTEM_TRANSFORMATION; for more information, refer to the Defining an oriented plane using a transformation matrix subsection of the Defining 3D geometries section of Chapter 44: Using the 3D Geometry module. When taking the 3D profile of a mesh, the source container of the mesh must have an M_COMPONENT_MESH_MIL component. The image below shows a slicing plane being used to take the profile of a point cloud. Note that the slicing plane is oriented; it is defined by the XY-plane of a new coordinate system and thus has a set origin and its X and Y axes are explicitly defined. Using an oriented plane when taking the profile allows you to customize the slice. For example, if you want a partial profile without completely slicing through the point cloud, you can specify a limit on the X-dimension of the oriented plane. When taking the 3D profile of a 3D geometry or mesh, MIL samples and extracts points along the intersection of the specified slicing plane and the geometry or mesh. For point clouds, however, the points in the profile do not necessarily fall exactly on the slicing plane. Instead, MIL subdivides the slicing plane into a grid and establishes the dimension of each grid's cell according to specified values, known as the sampling distance. For each cell, the point closest to the slicing plane is selected for the profile, as long as it is within a certain distance, known as the cutoff distance, from the plane. To take the 3D profile of a depth map, use M3dimProfile() with M_PROFILE_DEPTH_MAP. You must pass the X- and Y-coordinates of the start and end points that define the line along which to extract points, represented in either world units (M_WORLD) or pixel units (M_PIXEL). The line is extended downwards, effectively defining a slicing plane, with the pixels' intensity values indicating depth (Z-values). M3dimProfile() does not return points directly; instead, it stores the sampled points in a 3D image processing result buffer (M_PROFILE_RESULT). To retrieve these coordinates, call M3dimGetResult(). You can retrieve the X-, Y-, and Z-coordinates relative to the working coordinate system with M_WORLD_X, M_WORLD_Y, and M_WORLD_Z. Alternatively, you can retrieve the X- and Y- coordinates along the slicing plane, with respect to the origin of the plane, with M_PROFILE_PLANE_X and M_PROFILE_PLANE_Y. Note that for depth map profiles, since points are extracted along a specified line rather than an actual slicing plane, M_PROFILE_PLANE_X will return the real world distance along the specified line for each extracted point and M_PROFILE_PLANE_Y will return values that correspond to the depth at each pixel. To draw the result of a profile operation, use M3dimDraw3d(). This function draws the profile points into a 3D graphics list. Setting the slicing plane according to a reference object To take the profile of an object along a slicing plane that is at a known location on a reference object, you can: Take a depth map of the object's point cloud using M3dimProject(). See the Generating fully corrected depth and intensity maps section later in this chapter for more information. Find a model of a distinguishing feature on the object using MmodFind(). Note that since the Model Finder module works in 2D, the object is assumed to rotate only around its Z-axis, meaning it is always resting on the same side. Retrieve the calibration fixture matrix that contains the transformation coefficients required to fixture to the found occurrence, using McalFixture() with M_MOVE_RELATIVE, the Model Finder result buffer, and a previously allocated transformation matrix object. Apply the calibration fixture matrix to the object's point cloud so that it is at the same known position from the working coordinate system as the reference object is from its working coordinate system. To do so, use M3dimMatrixTransform(). Take the profile along the slicing plane using M3dimProfile() with M_PROFILE_POINT_CLOUD. Pass the transformation matrix that defines the slicing plane for the reference object. Note that the profile could have been taken directly from the depth map. However, if vertical surfaces have been scanned, they will not appear in the depth map; they will only appear in the point cloud. In addition, if the point cloud includes both the top view and the bottom view of the object, the profile would include points along the bottom view. The following is an example of taking the profile of a serpentine belt tensioner along a slicing plane. The X- and Y-axes in the cross-sectional profile refer to the X- and Y-axes of the profile plane (M_PROFILE_PLANE_X and M_PROFILE_PLANE_Y), not the working coordinate system. 3dprofilemetrology.cpp Analyzing a profile To analyze the surface of the object along the profile, you can fit one or more geometric shapes to it, such as a line and an arc, using the Metrology module. To add the profile points to a Metrology context, use MmetPut(). Since the points are not necessarily an ordered and connected chain, do not have MmetPut() automatically interpolate the gradient angle for the points (that is, set the ControlFlag parameter to M_DEFAULT). For more information, see the Using external edgels and points with constructed features section of Chapter 21: Metrology. Note that the Metrology module can only analyze 2D points along the same plane (the X- and Y-coordinates). Therefore, to analyze the profile of an object along the plane, extract the coordinates of the points along the plane with respect to the plane's origin (M3dimGetResult() with M_PROFILE_PLANE_X and M_PROFILE_PLANE_Y), instead of with respect to the working coordinate system. The following animation demonstrates how to define a 3D profile and then use the Metrology module to obtain certain measurements for analysis. Play speed: Default (x1.0) Quarter (x0.25) Half (x0.5) Double (x2.0) Start position: Current image First image Looping: Continuous play Single iteration 1 of 22 Analyzing the profile of a 3D cross-section Taking a profile Setting the slicing plane according to a reference object Analyzing a profile ",
      "wordCount": 1112,
      "subEntries": []
    },
    {
      "id": "UG_3D_Image_processing_Generating_a_fully_corrected_depth_map",
      "version": null,
      "title": "Generating fully corrected depth and intensity maps",
      "subTitles": [
        "Steps to create a depth map",
        "Changing a depth map by rotating or transforming 3D points",
        "Defining the points to use",
        "Estimating a recommended depth map image buffer size ",
        "Establishing the world-to-pixel scale and the world-to-gray level scale of the depth map",
        "How to calibrate the image buffer",
        "Specifying data placement in the destination image buffer",
        "Automatically setting the pixel aspect ratio",
        "Specifying the depth map intensity scale",
        "Saturating or ignoring points outside the depth map grayscale range",
        "Remapping the depth map",
        "Generating the intensity map",
        "Adding color to a depth map for better visualization",
        "Gradually adding data to the depth map and intensity map image buffers",
        "Point cloud projection example"
      ],
      "location": "MIL UG P05: 3D processing and analysis",
      "pageURL": "content\\UserGuide\\3D_Image_processing\\Generating_a_fully_corrected_depth_map.htm",
      "text": " Generating fully corrected depth and intensity maps You can generate a fully corrected depth map from a point cloud or 3D geometry. To do so, use M3dimProject(), which projects the point cloud or 3D geometry onto the XY-plane (Z=0) of the working coordinate system, and converts 3D point information to depth map pixel values. You can also generate an intensity map from a point cloud. As an image, a depth map has pixels and a pixel coordinate system where the center of the top-left pixel is (0, 0). However, each pixel can be seen as a 3D coordinate, with the X- and Y-coordinates of a pixel in the depth map corresponding to X- and Y-coordinates of a 3D point (where pixel and world units are suitably scaled). The intensity of a pixel in a depth map corresponds to the Z-coordinate, or depth, of the point. For more information on the correspondence between pixel coordinates and world coordinates, see the Transforming coordinates from a depth map to a world coordinate system (or vice versa) subsection of the How coordinates and non-positional results are transformed section of Chapter 28: Calibrating your camera setup. In addition to a depth map, you can also generate an intensity map from a point cloud. As with a depth map, the X- and Y-pixel coordinates of the intensity map correspond to the X- and Y-world coordinates of 3D points (where pixel and world units are suitably scaled). The difference between the two maps lies with what the intensities of their pixels correspond. In a depth map, the intensity of a given pixel corresponds to the height of the object at that point. In an intensity map, the intensity of a pixel corresponds to the luminous intensity of the object at that point. For more information on intensity maps, see the Generating the intensity map subsection of this section. Except when otherwise mentioned, everything in this section that refers to depth maps also includes intensity maps. Steps to create a depth map To create a depth map from a point cloud, you need to call M3dimProject() and specify the point cloud container and a calibrated image buffer to hold the depth map. M3dimProject() uses the calibration information of the image buffer so that projected points and depth information correspond to the X-size, Y-size, and Z-scale of the depth map image buffer. Note that M3dimProject() projects 3D points onto the XY-plane (Z=0) of the point cloud's working coordinate system, which are then mapped to depth map pixels. The following steps provide a basic methodology for generating a fully corrected depth map from a point cloud: Optionally, rotate or transform the point cloud (using M3dimRotate() or M3dimMatrixTransform()) so that projecting the point cloud onto the XY-plane of its working coordinate system will result in the required depth map. See the Changing a depth map by rotating or transforming 3D points subsection of this section. Optionally, call M3dimCalculateMapSize() or M3dimLattice() to estimate a recommended depth map image buffer size from the source container's point cloud. Note that you can specify a 3D box geometry object to limit which points to include for the calculation. M3dimCalculateMapSize() makes a best estimate calculation for the depth map image buffer, based on the distance between points in X and Y in the source point cloud, such that most points map to their own pixel and with a minimum number of invalid points. M3dimLattice() calculates a lattice that divides points into cells at a specified approximate rate. When the lattice size is used for the depth map image buffer, all the points in a cell map to the same depth map pixel. If you already know the depth map image buffer size with which you want to work, you can allocate it directly, without first calling M3dimCalculateMapSize() or M3dimLattice(). Allocate an image buffer that will hold the depth map, using MbufAlloc2d(). If you are generating both a depth map and an intensity map, you must allocate a separate image buffer for each. The image buffer in which to store the intensity map must have the same dimensions (SizeX and SizeY) as the image buffer allocated for the depth map. Note that you can allocate a 3-band image buffer (using MbufAllocColor()) for the intensity map if the point cloud container's M_COMPONENT_REFLECTANCE or M_COMPONENT_INTENSITY component is also 3-band. If you used M3dimLattice() to determine the depth map image buffer size, call McalUniform() to calibrate the allocated image buffer for the depth map. Use the M_START_POINT_... and M_CELL_SIZE_... result types (M3dimGetResult()) to specify the translation and scale between pixel and world coordinate systems. You can then use McalControl() with M_GRAY_LEVEL_SIZE_Z and M_WORLD_POS_Z to set the real-world to gray level scale and Z-offset, respectively. Otherwise, call M3dimCalibrateDepthMap() to calibrate the allocated image buffer for the depth map. The calibration sets the real-world to pixel unit scale for the image buffer, and sets the real-world to gray level scale (M_GRAY_LEVEL_SIZE_Z) for depth values. Note that if you are using M3dimCalculateMapSize() and M3dimCalibrateDepthMap(), you must perform steps 2 and 4 such that the bounding boxes for each operation are the same (typically, M3dimCalculateMapSize() and M3dimCalibrateDepthMap() use the same source point cloud or 3D geometry). Note that once you have computed a calibration for the depth map image buffer (using M3dimCalibrateDepthMap() or McalUniform()), you can use the results for multiple calls to M3dimProject(), without having to recalculate a calibration each time. Call M3dimProject(), specifying the point cloud container that holds the point cloud and the image buffers that will hold the depth map and intensity map. After the projection, the resulting depth map will be fully corrected and include a valid Z-scale. Changing a depth map by rotating or transforming 3D points To generate a fully corrected depth map, M3dimProject() projects the points of the point cloud on the XY-plane (Z=0) of its working coordinate system. To obtain a different view of the point cloud in the resulting depth map, use M3dimRotate() or M3dimMatrixTransform() to rotate or transform (respectively) the 3D points before calling M3dimProject(). The following image illustrates generating 2 different depth maps from the same point cloud (the wire frame boxes are the objects from which the point clouds are created). Note that the ZSign parameter is set to M_NEGATIVE so that more negative Z-values are brighter in the depth map. Defining the points to use By default, if you pass a point cloud to M3dimCalculateMapSize(), M3dimLattice(), or M3dimCalibrateDepthMap(), the functions will use the bounding box of all points in the point cloud. If you want to exclude some points (for example, outliers), you can limit the calculation to points inside a box if you pass a 3D box geometry to M3dimCalculateMapSize() or M3dimLattice(). If applicable, you should pass the same box to M3dimCalibrateDepthMap(); doing so ensures that the calibrated image buffer passed to M3dimProject() will hold the required point data from inside the box. Optionally, you can apply a crop or mask to the point cloud using M3dimCrop() before calling M3dimProject() if you need to exclude further points. If you want to calculate a bounding box that contains most of the points, but rejects outliers, you can use M3dimStat() with an M_STAT_CONTEXT_BOUNDING_BOX context that has M_BOUNDING_BOX_ALGORITHM set to M_ROBUST. See the Determining the bounding box subsection of the Calculating statistics on a point cloud, depth map, or 3D geometry section earlier in this chapter. Estimating a recommended depth map image buffer size Before calling M3dimProject() to generate a depth map, you can use M3dimCalculateMapSize() or M3dimLattice() to estimate a recommended image buffer size in X and Y for the depth map. In the former case, the recommended size is based on the given points and is optimized so that the resulting depth map is not too small or large. That is, you will not have a depth map where too many points have been projected into the same pixel (loss of information), or a depth map where the gaps between valid pixels are too large. M3dimCalculateMapSize(), by default, does all calculations for the depth map's dimensions. If required, you can set a fixed size along 1 dimension, and the function will calculate the necessary size of the other dimension (for either pixel size or image buffer size). In the case of M3dimLattice(), the recommended size is based on a fraction of the given points, such that multiple points can map to the same pixel. Specify the required ratio of valid pixels in the depth map to valid points in the point cloud, using M3dimGetResult() with M_FRACTION_OF_POINTS and M_FRACTION_OF_POINTS_TOLERANCE. For example, if you specify a ratio of 0.25 and a tolerance of 0.05, points will be projected into the depth map at an approximate rate of 3-5 points per pixel. You can retrieve the calculated lattice's sizes to use for the depth map, using M3dimGetResult() with M_NUMBER_OF_CELLS_.... Note that using M3dimCalculateMapSize() or M3dimLattice() to estimate a recommended depth map image buffer size is optional. Your 3D sensor might provide organized point clouds with standard XY-dimensions with which you want to work. In this case, you can specify the required dimensions directly when allocating the depth map image buffer, using MbufAlloc2d(). Establishing the world-to-pixel scale and the world-to-gray level scale of the depth map You can let M3dimCalibrateDepthMap() automatically determine the world-to-pixel scale and world-to-gray level scale for the depth map or you can use McalUniform() and McalControl() with M_GRAY_LEVEL_SIZE_Z to manually set the world-to-pixel scale and world-to-gray level scale for the depth map, respectively. How to calibrate the image buffer Ultimately, you want a depth map with constant world-to-pixel and world-to-gray level scales. You can use M3dimCalibrateDepthMap() to determine the ratio between the specified point cloud's dimensions (measured in world units) and the depth map image buffer (measured in pixels), and automatically calibrate the image buffer with this information. Default settings use a square pixel size. Alternatively, if you previously called M3dimLattice() to calculate the depth map image buffer size, you can retrieve the translation and scale of the transformation between pixel and world units, using M3dimGetResult() with M_START_POINT_... and M_CELL_SIZE_..., and then pass these values to McalUniform() to calibrate the image buffer with this information. To inquire about a depth map's scale values, call McalInquire() with the identifier of the depth map image buffer and M_PIXEL_SIZE_X or M_PIXEL_SIZE_Y to inquire the real-world length in X or Y of one depth map pixel; inquire M_GRAY_LEVEL_SIZE_Z to obtain the step, in world units, along the Z-axis of the relative coordinate system represented by one gray level. Specifying data placement in the destination image buffer Depending on the source point cloud, it might not fit exactly within the destination image buffer's dimensions. When calling M3dimCalibrateDepthMap(), you can use the ControlFlag parameter to specify where to place valid values in the destination image buffer; you can set the parameter to M_DEFAULT or M_CENTER. In the M_DEFAULT case, the data aligns to the top left of the image buffer, and M3dimCalibrateDepthMap() calculates a pixel size such that the data extends at least the breadth of one dimension, and no data is lost along the other dimension. Leftover pixels are set to the invalid value, and are always located to the right or bottom of the valid data in the image buffer. In the M_CENTER case, valid data is centered in the image buffer and extends the full breadth of one dimension with no loss of data along the other dimension. Leftover pixels are located above and below or to the right and left of the valid data. Note that the source data maintains its aspect ratio, and is not affected by the destination's aspect ratio. The following illustration shows source data placement in the destination image buffer. Automatically setting the pixel aspect ratio When using M3dimCalibrateDepthMap(), you can set the PixelSizeAspectRatio parameter to M_NULL so that projected data fits exactly onto the depth map image buffer, with no invalid rows or columns. This is useful, for example, when using a laser profiler to generate the source point cloud. Data along X depends on the camera resolution, while data along Y depends on the frame rate and the speed of the conveyor. The disparity between these dimensions makes it difficult to use a 1:1 pixel aspect ratio, and doing so would result in data loss in the X-direction, or gaps in the Y-dimension if a square pixel size was enforced. To ensure a calculated map size without gaps or data loss, and a corresponding exact fit of projected data into the depth map image buffer, use M3dimControl() with M_PIXEL_ASPECT_RATIO set to M_NULL when setting up the calculate map size 3D image processing context, and then use M3dimCalibrateDepthMap() with M_NULL when calibrating the destination image buffer. Specifying the depth map intensity scale MIL automatically sets the real-world to gray level scale M_GRAY_LEVEL_SIZE_Z when you call M3dimCalibrateDepthMap(). By default, M_GRAY_LEVEL_SIZE_Z is considered positive (M_POSITIVE), which means larger (more positive) Z-values have the brightest gray levels and smaller (more negative) Z-values are darker. To invert the gray level values, set the ZSign parameter to M_NEGATIVE. If the Z-axis points downward, the object is in the negative Z-region of the working coordinate system, and more negative Z-values indicate a higher surface and are darker in the depth map (assuming ZSign is set to M_POSITIVE). The following image shows the distribution of gray values when the object is in the positive or negative Z-region, and when ZSign is set to M_POSITIVE or M_NEGATIVE. Note that calling McalUniform() does not automatically set the real-world to gray level scale. You must specify the scale using McalControl() with M_GRAY_LEVEL_SIZE_Z. Saturating or ignoring points outside the depth map grayscale range The depth map image buffer's grayscale range, along with M_GRAY_LEVEL_SIZE_Z, determines the maximum and minimum Z-values representable in the depth map. A point can be outside the depth map grayscale range. For instance, if the Z-axis scale is 0.02 cm/gray level in an 8-bit image, a point that is 1 cm high has a gray value of 50. At about 5 cm, the gray value reaches its maximum for the bit depth, 254. Out-of-range points can occur if you pass M3dimCalibrateDepthMap() a point cloud or 3D box geometry that has different bounding box dimensions from the point cloud passed to M3dimProject(). If a point's Z-coordinate is not in the depth map grayscale range, the point is ignored or it saturates the pixel to which it maps. To specify which should occur, use M3dimProject() with the Options parameter set to M_DEFAULT to ignore or M_SATURATION to saturate. By ignoring the point (M_DEFAULT), it is not included in the calculation of the depth map pixel's gray value. If there are other points that project onto this pixel, they are used to calculate the gray value. Otherwise, the pixel is rendered as invalid, and set to the buffer's maximum value (for example, 255 for an 8-bit buffer). By saturating the point (M_SATURATION), the pixel's gray level value is clipped to either the maximum possible valid value (M_MAX - 1) or the minimum possible value (M_MIN), depending on whether the point maps to a value above or below the grayscale range. Remapping the depth map You might require a depth map that contains only a specific range of depths for analysis, or the depth map has to be of a certain bit depth. For example, MmodFind() accepts only 8-bit image buffers. You can use M3dimRemapDepthMap() to remap a depth map to fit within a specified Z-range, or convert the depth map to a different data type. To remap a depth map, perform the following: Acquire or create the source depth map. Allocate a remap 3D image processing context, using M3dimAlloc() with M_REMAP_CONTEXT to hold the remap settings. Note that you can skip this step and use a default context instead. Specify the remap mode (M_REMAP_MODE) and other options, using successive calls to M3dimControl(). The remap mode determines the Z-range of the destination depth map, based on specific criteria such as the source depth map's standard deviations from the mean Z-value, its maximum and minimum pixel values, or user-specified values. Allocate an image buffer to hold the remapped depth map, using MbufAlloc2d(). The destination image buffer must have the same dimensions (SizeX and SizeY) as the source depth map image buffer. Remap the depth map, using M3dimRemapDepthMap(). Generating the intensity map In an intensity map, the gray value of each pixel represents the luminosity of the original scene at this point. You can use this information to determine, for example, whether the surface is dark or light, or if the surface is reflective. To generate the intensity map, allocate an image buffer and pass its identifier to M3dimProject(). The width and height of this buffer (the intensity map buffer) must be the same as the depth map image buffer. The number of bands of this buffer must match the number of bands of the source point cloud container's M_COMPONENT_REFLECTANCE or M_COMPONENT_INTENSITY component. Note that the image buffer for the intensity map does not have to be calibrated; M3dimProject() will calibrate it using the calibration information of the depth map image buffer. If you specify M_NULL as the identifier of the intensity map buffer, an intensity map is not generated. Adding color to a depth map for better visualization To enhance your ability to differentiate small intensity differences in your depth map, you can apply a colormap LUT to the depth map image buffer, as in the image below. The basic steps to apply a colormap to a depth map are as follows: Allocate a 3-band LUT buffer using MbufAllocColor() with M_LUT. Ensure that your LUT buffer width is at least as large as the number of different possible intensities in the depth map (256 for 8-bit depth maps, 65536 for 16-bit depth maps). Populate the LUT buffer with colors for each intensity using MgenLutFunction() with a colormap, such as M_COLORMAP_JET. Optionally, to highlight invalid pixels, add the M_LAST_GRAY combination value. When specified, the color assigned to the highest index of the LUT is replaced. Set the b parameter to specify the replacement color. Specifying a value that is not among the colors in the LUT (for instance gray) allows you to find areas with invalid points more easily. Alternatively, you can use MbufPutColor2d() to replace elements (colors) in a LUT. Associate the LUT buffer with the display using MdispLut(). This adds color to the image displayed on the screen, but does not affect the image itself. The grayscale depth map is not changed as a result of this function. You can associate a LUT buffer to a 3D display using M3dgraControl() with M_COLOR_USE_LUT set to M_TRUE. See the Applying LUTs subsection of the Color and display settings for 3D data section of Chapter 43: 3D Display and graphics for more information. For more information on modifying how a 1-band image, such as a depth map, is displayed using a LUT buffer, see the Mapping 1-band images through a LUT upon display section of Chapter 25: Displaying an image. Gradually adding data to the depth map and intensity map image buffers Normally, when you call M3dimProject(), it clears the depth map and intensity map buffers before writing to them. However, when the M_ACCUMULATE option is specified, M3dimProject() adds the data without clearing these buffers first. Before the first call to M3dimProject() and before starting to extract data of another object, do not use the M_ACCUMULATE option. Point cloud projection example For an illustration of projecting a point cloud to generate a depth map, see the following example. pointcloudprojection.cpp Generating fully corrected depth and intensity maps Steps to create a depth map Changing a depth map by rotating or transforming 3D points Defining the points to use Estimating a recommended depth map image buffer size Establishing the world-to-pixel scale and the world-to-gray level scale of the depth map How to calibrate the image buffer Specifying data placement in the destination image buffer Automatically setting the pixel aspect ratio Specifying the depth map intensity scale Saturating or ignoring points outside the depth map grayscale range Remapping the depth map Generating the intensity map Adding color to a depth map for better visualization Gradually adding data to the depth map and intensity map image buffers Point cloud projection example ",
      "wordCount": 3367,
      "subEntries": []
    },
    {
      "id": "UG_3D_Image_processing_Filling_missing_data",
      "version": null,
      "title": "Filling missing data points (gaps)",
      "subTitles": [
        "Filling mode",
        "Filling operations",
        "Linear interpolation",
        "Propagation of one of the boundary values",
        "Phenomena causing missing data",
        "Examples"
      ],
      "location": "MIL UG P05: 3D processing and analysis",
      "pageURL": "content\\UserGuide\\3D_Image_processing\\Filling_missing_data.htm",
      "text": " Filling missing data points (gaps) Gaps, or invalid values, are missing data points in a depth map, where the 3D Reconstruction module or 3D sensor could not pick up information, due to occlusion, reflections, or areas out of the camera's field of view. These missing data points form gaps in the generated depth map. There are two types of gaps: a gradual elevation gap and a sharp elevation gap. Gaps are categorized into one of the two depending on their underlying surface. If the depth of the gap's underlying surface is considered to be gradual, the gap is considered to be a gradual elevation gap; if the depth of the gap's underlying surface is considered to be sharp, the gap is considered to be a sharp elevation gap. The 3D Image Processing module makes assumptions about the type of gap based on the difference between boundary values of the gap in the depth map. If there is a small difference, the module assumes that the gap is a gradual elevation gap and if there is a large difference, it assumes that the gap is a sharp elevation gap. You can fill gaps in a depth map or intensity map. To do so, specify the image buffer(s) and gap filling context using M3dimFillGaps(). Then, if default settings are not sufficient, set required control type values using M3dimControl(). Filling mode The gap filling operation can either ignore gaps or fill them using either the X-then-Y or the Y-then-X filling mode. To specify the filling mode, use M3dimControl() with M_FILL_MODE. The X-then-Y filling mode (M_X_THEN_Y) first analyzes each depth map row and fills any missing data points it encounters in that row, according to the specified filling operation (discussed later). It then analyzes each column and fills any gaps it finds in that column, also according to the specified filling operation. Note that gaps whose boundaries touch the border of the image are not filled. The Y-then-X filling mode (M_Y_THEN_X) is the same as the X-then-Y filling mode, except that the columns and rows are filled in the reverse order. The Y-then-X filling mode first analyzes each depth map column and then it analyzes each row. Again, gaps whose boundaries touch the border of the image are not filled. For an 8- or 16-bit depth map image buffer, the gray values 255 and 65535 are used to indicate missing data points (gaps), respectively. The gray values used to fill the gaps are approximations. If you require accurate results, see the Phenomena causing missing data subsection of this section for suggestions on improving your 3D reconstruction setup to minimize occurrences of gaps. When gradually adding data to the image buffer (using M3dimProject() with M_ACCUMULATE), gap filling is not performed, since it must be done after extracting all the laser line data. However, after all necessary data have been extracted, you can fill gaps in the depth map using M3dimFillGaps(). Filling operations To fill gaps, two filling operations are available. You can either propagate the value of one of the boundaries or do a linear interpolation between the two boundaries on each row or column of the gap. The choice of filling operation is based on whether the gap is considered a gradual or a sharp elevation gap. To determine which operation to perform, you can set a threshold above which gaps are considered sharp elevation gaps and are therefore filled by boundary propagation. Below the threshold, gaps are considered gradual and are filled using linear interpolation. Default settings consider all gaps as gradual, and linear interpolation is always used. Linear interpolation A gradual elevation gap is a range of missing data where the difference of the boundary values of that gap's row or column is less than the threshold specified using M3dimControl() with M_FILL_SHARP_ELEVATION_DEPTH. The underlying surface of such a gap is considered to be part of the same surface as its boundaries, so this type of gap is always filled with linearly interpolated values. For example, in the following image, M_FILL_SHARP_ELEVATION_DEPTH is set to 75.0. The values at the gap boundaries are 42 and 60. Since the difference, 18, is less than the specified threshold of 75.0, the gap will be filled with linearly interpolated values. You can specify to always use linear interpolation to fill missing data points, regardless of the depth difference at the boundaries, by setting M_FILL_SHARP_ELEVATION_DEPTH to M_INFINITE. This treats any elevation gap as being a gradual elevation gap. Propagation of one of the boundary values A sharp elevation gap is a range of missing data where the underlying surface is considered to be discontinuous at least at one of its boundaries. The depth difference of the boundaries of the gap's row or column must be greater or equal to M_FILL_SHARP_ELEVATION_DEPTH for the gap to be considered a sharp elevation gap. For these types of gaps, you can select to either: Fill the sharp elevation gaps by propagating one of their boundaries. You should choose this fill operation if the gaps are not due to holes in the object. Leave the gaps as is by setting M_FILL_SHARP_ELEVATION to M_DISABLE because the gaps are due to holes in the object. To propagate one of the boundary values, you can choose to fill the gaps with either the minimum or maximum pixel value of the gaps' boundaries. To do so, set M_FILL_SHARP_ELEVATION to M_MIN or M_MAX, respectively. If the X-then-Y filling mode is used, then gaps in each row will be filled first by propagating one of the boundaries on either side of the gap in each row, followed by the filling of the gaps in each column. Similarly, if the Y-then-X filling mode is used, gaps in each column will be filled first by propagating one of the boundaries on either side of the gap in each column, followed by the filling of the gaps in each row. Note if one of the boundaries is invalid (in an 8-bit image, it has a value of 255), then the gap will not be filled. To use propagation of the boundary values to fill any type of gap, set M_FILL_SHARP_ELEVATION_DEPTH to 0.0. All gaps will be treated as being sharp elevation gaps. Phenomena causing missing data Laser line information might be missing in the image due to different phenomena. Depending on your setup, you might be able to reduce the amount of missing data. Camera occlusion and laser occlusion. Camera occlusion occurs when the laser line is hidden from the camera's field of view by the object's geometry. Laser occlusion occurs when the object's geometry prevents the laser line from illuminating parts of the object's surface. To fix camera occlusion problems, try to position your camera in such a way that it is able to capture the laser line at all times. If this is not possible, add a camera to grab the laser lines that are hidden from the first camera. For information on setting up multiple cameras, see the Multiple camera-laser pairs section of Chapter 46: 3D reconstruction using laser line profiling. To fix laser occlusion problems, add a second laser to the setup, whose laser plane aligns and overlaps with the plane of the first laser and whose laser line illuminates the regions that are not illuminated by the first laser. Reflectance. The object's surface can absorb enough light to prevent laser line detection. For example, dark markings might not reflect enough light back to the camera. Increase the laser line's intensity to compensate for the light being absorbed. If this is not possible, adjust the iris opening (aperture) of your camera to gain more exposure. Scaling. The aspect ratio of the object in the depth map might be maintained by filling rows with missing data (255). This occurs when the camera is not able to grab an image with the laser line for each slice of the object. Decrease the speed at which the object is moving under the laser plane or increase the frame rate or resolution of the camera to provide more 3D data to the module. You can use M3dimControl() with M_PIXEL_SIZE_X, M_PIXEL_SIZE_Y, and M_GRAY_LEVEL_SIZE_Z to adjust the scale of the pixels and obtain a smaller sized depth map. This type of gap can only appear if the 3D reconstruction context was allocated in M_CALIBRATED_CAMERA_LINEAR_MOTION 3D reconstruction mode. The following image shows an example of camera occlusion, laser occlusion, and reflectance. In the case of camera occlusion and laser occlusion, you could use propagation of one of the boundary values because the gaps might be caused by sharp elevation differences. For the gaps due to reflectance, for example, you could use linear interpolation because the underlying surface of the gap is flat. Examples The following image shows the generated depth map of a cookie with missing laser data due to scaling as well as camera and laser occlusion. To fill these gaps, you can choose linear interpolation, propagation of one of the boundary values, or a mix of both. You can use M3dimControl() with M_FILL_SHARP_ELEVATION and M_FILL_SHARP_ELEVATION_DEPTH to specify the filling operation. Filling missing data points (gaps) Filling mode Filling operations Linear interpolation Propagation of one of the boundary values Phenomena causing missing data Examples ",
      "wordCount": 1532,
      "subEntries": []
    },
    {
      "id": "UG_3D_Image_processing_3D_Arithmetic",
      "version": null,
      "title": "Performing arithmetic operations on depth maps",
      "subTitles": [
        "Using arithmetic for defect detection",
        "Subtraction considering neighborhood pixel distances",
        "Constraints on depth map operands and how to handle destination calibration"
      ],
      "location": "MIL UG P05: 3D processing and analysis",
      "pageURL": "content\\UserGuide\\3D_Image_processing\\3D_Arithmetic.htm",
      "text": " Performing arithmetic operations on depth maps You can perform arithmetic operations between depth maps and 3D geometries using M3dimArith(), to produce a new depth map. Supported operations include addition, subtraction, and minimum/maximum operations. This could be useful to, for example, subtract a fitted plane from the depth map to find the regions which deviate from the fit. M3dimArith() can also be used to replace missing values in a depth map. All image operands must be fully corrected depth maps for this operation. To determine if you have a fully corrected depth map, call McalInquire() with M_DEPTH_MAP and ensure that it returns M_TRUE. Using arithmetic for defect detection You can use arithmetic functions on depth maps for defect detection. With two aligned, fully calibrated depth maps of the same object, one depth map representing a perfect model of an object and the other representing a current instance of the object, you can subtract the intensities of the two to create a depth map that highlights where the current object differs from the perfect model, as in the following image. To use arithmetic on depth maps for defect detection, you must start with two point clouds of the object, a perfect model of the object and the object currently being tested. Then, follow this procedure: Find the transformation that would align the two point clouds, using the 3D Registration module. Align the two point clouds by applying the transformation to the point cloud of the object currently being tested, using M3dregMerge() (pass M_NULL for the point cloud of the perfect model). Generate two depth maps, one for the perfect model and the other for the object currently being tested, using M3dimProject(). Subtract the two depth maps, using M3dimArith() with M_DIST_NN(), M_DIST_NN_SIGNED(), M_SUB, or M_SUB_ABS, depending on your application and the depth maps. If the depth maps should be nearly identical, except any defects, you can use M_SUB_ABS or M_SUB to determine where the defects are. These operations simply subtract the intensities of corresponding pixels in the two depth maps. These operations are fairly quick. If the two aligned depth maps might have a small degree of difference that is not related to defects, such as shadows or incomplete alignment, you can use M_DIST_NN() or M_DIST_NN_SIGNED() to determine where the defects are. These operations are more robust versions of M_SUB_ABS and M_SUB that compensate for noise or misalignment by considering the neighborhood of pixels around the corresponding pixels. Use the resulting depth map to locate and possibly measure areas where the object being tested differs from the perfect model. These areas will likely have higher than average intensities. Subtraction considering neighborhood pixel distances When looking for substantial differences between two depth maps (or geometry objects) that have a small degree of difference due to noise, simply subtracting the intensities of the pixels could give too many false differences. In this case, you can use M3dimArith() with M_DIST_NN() or M_DIST_NN_SIGNED(). For example, if the depth map differences are due to noise, such as incomplete alignment, small shadows, or other slight camera deviations, you can use these operations. These arithmetic operations are more robust to slight alignment differences or random deviations between the depth maps, but take longer to process. You can specify the size of the neighborhood used in the algorithm; typically, the bigger the neighborhood, the more robust the operation is, but the longer it takes to process. Rather than subtracting the intensities of just the corresponding pixels in the two operands, as with M_SUB_ABS and M_SUB, M_DIST_NN() and M_DIST_NN_SIGNED() additionally considers the neighborhood of pixels around the corresponding pixels when determining the difference. The M_DIST_NN() and M_DIST_NN_SIGNED() operations use a three step process. The operations use the real-world coordinates of corresponding pixels in the depth map, with the pixel's intensity mapping to a Z-coordinate. The three step process to calculate the resulting intensity of each pixel is as follows: The operation calculates the distance between a pixel in operand 1 (considered as a 3D point) and each of the neighborhood pixels of the corresponding pixel in operand 2, including the corresponding pixel itself. The shortest of these distances is considered. Note that the distance between two pixels is calculated by first converting the X- and Y-pixel coordinates and pixel intensity of each pixel into a 3D coordinate in world units. The distance between 2 points in 3D space is calculated using Euclidean geometry. The operation calculates the 3D Euclidean distance between a pixel in operand 2 (considered as a 3D point) and each of the neighborhood pixels of the corresponding pixel in operand 1, including the corresponding pixel itself. The shortest of these distances is considered. The operation uses the larger of the two minimum distances calculated in the previous two steps. Note that the resulting distance is converted back to an intensity using the Z-scale (M_GRAY_LEVEL_SIZE_Z). The animation below illustrates this algorithm using a neighborhood size of 3x3. Specify the size of the neighborhood using M_DIST_NN() with NeighborhoodSize. The larger the size of the neighborhood, the longer the process takes, but the more robust it tends to be. Neighborhoods that are too large might return meaningless results. Constraints on depth map operands and how to handle destination calibration When two depth maps are the operands, M3dimArith() performs the selected operation point-to-point. Therefore, your settings with respect to X and Y must be equal. Specifically, both source depth maps must share the same values for the following: M_PIXEL_SIZE_X: the length, in world units, of one pixel in the corrected image, in the X-direction. M_PIXEL_SIZE_Y: the length, in world units, of one pixel in the corrected image, in the Y-direction. Both source depth maps must also share the same world position for the top-left pixel in the corrected images. This means that a M_PIXEL_TO_WORLD transformation must be the same for the top-left pixel (0,0) of both images. Equivalently, both source depth maps must share the same values for the following: M_WORLD_POS_X + (M_PIXEL_SIZE_X * M_CALIBRATION_CHILD_OFFSET_X): the X-coordinate of the center of the top-left pixel in the corrected image. M_WORLD_POS_Y + (M_PIXEL_SIZE_Y * M_CALIBRATION_CHILD_OFFSET_Y): the Y-coordinate of the center of the top-left pixel in the corrected image. Note, however, that the scale along Z (M_GRAY_LEVEL_SIZE_Z) and the location of zero gray along Z (M_WORLD_POS_Z, also called the Z-offset) can differ, since the operation is performed using the Z-values. For example, M_ADD adds together the world Z-coordinates of corresponding points. After performing the operation, each world point for the destination image buffer is transformed back into a depth map pixel. You can control what happens to the values of M_GRAY_LEVEL_SIZE_Z and M_WORLD_POS_Z of the destination buffer, using the ControlFlag parameter. The default setting is M_FIT_SCALES, which adjusts the Z-scale and Z-offset so that values can be represented in the destination image buffer without saturation. In determining these values, M3dimArith() considers all possible values that could result from the operation, and are thus dependent on the operator. A simple example: let [6, 18] and [2, 10] be the range of representable world Z-values of the first and second operands, respectively. If the operation is M_SUB, the range of possible resulting values would be [6 - 10, 18 - 2] = [-4, 16]. Note that these are the extreme possible values, and not necessarily actual source values. Note that both the M_MIN and M_MAX operations use the same range, which includes minimum and maximum extremes. This makes comparison between the two operations easier. So using the same ranges of the example operands above, the range of possible values for the destination image buffer (for either M_MIN or M_MAX) would be [min(6, 2), max(18, 10)] = [2, 18]. Once the destination range is computed, the values of M_GRAY_LEVEL_SIZE_Z and M_WORLD_POS_Z are chosen to span that range. To constrain the destination Z-scale to that of the first source image, set ControlFlag to M_SET_WORLD_OFFSET_Z. In this case, operation results are mapped to fit the Z-scale of the first source image and the Z-offset is automatically adjusted so that the destination range covers the center of the possible range of data. Saturation can occur with this setting. If you set ControlFlag to M_USE_DESTINATION_SCALES, you can explicitly set the values of M_GRAY_LEVEL_SIZE_Z and M_WORLD_POS_Z for the destination image buffer using McalControl(). In this case, M3dimArith() will use the provided destination calibration information when performing the arithmetic operation. The destination must be 3D-corrected before calling M3dimArith(), and have the same scales and offsets in X and Y as the source image(s). Use McalUniform() and McalControl() to manually set the calibration information of the destination image buffer. Alternatively, you can copy the camera calibration information from another image using McalAssociate(). You can also use the calibration information of one of the source operands by setting ControlFlag to M_USE_SOURCE1_SCALES or M_USE_SOURCE2_SCALES. Performing arithmetic operations on depth maps Using arithmetic for defect detection Subtraction considering neighborhood pixel distances Constraints on depth map operands and how to handle destination calibration ",
      "wordCount": 1485,
      "subEntries": []
    }
  ]
}]
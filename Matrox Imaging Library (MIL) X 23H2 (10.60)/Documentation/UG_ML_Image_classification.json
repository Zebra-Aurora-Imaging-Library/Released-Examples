[{
  "id": "UG_ML_Image_classification",
  "version": "2024020714",
  "title": "Image classification",
  "subTitles": null,
  "location": "MIL UG P08: Machine learning tasks",
  "pageURL": "content\\UserGuide\\ML_Image_classification\\ChapterInformation.htm",
  "text": " Chapter 51: Image classification This chapter explains how to perform image classification using machine learning with the MIL Classification module. Image classification overview Steps to perform image classification Perform all required allocations Build and populate your dataset Train your classifier context Predict with your classifier Save your classification contexts Free your allocated objects Building a dataset for image classification Populate the source dataset context Splitting your dataset Augmentation and other data preparations Data preparation settings Augmenting images Exporting and importing a dataset Export Import Classifier and training settings for image classification Training objects and folders Training modes Training and analysis for image classification Monitoring the training process Results Training analysis Confusion matrix Prediction for image classification Prepare for prediction Predict Results Drawing results Assisted labeling Image classification examples ",
  "wordCount": 130,
  "subEntries": [
    {
      "id": "UG_ML_Image_classification_Image_classification_overview",
      "version": null,
      "title": "Image classification overview",
      "subTitles": null,
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Image_classification\\Image_classification_overview.htm",
      "text": " Image classification overview This chapter explains how to perform image classification using machine learning with the MIL Classification module. Note that this chapter expands on topics previously discussed in Machine learning fundamentals. It is recommended to review these topics if you have not already done so. For the most recent documentation of this chapter, particularly as it relates to statistical analysis (MclassStatCalculate()), check for an updated version of the MIL Help online at zebra.com/aurora-imaging-library-help. Image classification allows you to classifying an entire image; it typically requires a predefined CNN classifier context that was defined by Matrox (an ICNet), and that must be trained with an images dataset context. To train a classifier, you must supply it with many images that are representative of the real-world problem the classifier will solve, along with a label for each image identifying its class. The classifier will learn from these training images how to differentiate the various classes. An example of image classification is identifying different types of images that have complex and similar features, such as different types (classes) of pasta, as shown here. To solve this problem, and identify different classes of pasta, a classifier must be trained on many images of each class of pasta (for example, hundreds of images of each class). After an ICNET classifier is trained, it can be used to predict the class to which a previously unseen image belongs. Ultimately, the classifier gives each predicted image a score, per defined class, to indicate the degree to which that class represents the image. The defined class with the highest score is the one that best represents the image. The following example shows an image and its predicted class results. The best class is highlighted. The classification of pasta above is a multiclass classification problem. Image classification can also be used to perform binary classification, predicting whether or not an image belongs to an invalid or invalid class (such as BadApple or GoodApple). Image classification overview ",
      "wordCount": 330,
      "subEntries": []
    },
    {
      "id": "UG_ML_Image_classification_Steps_to_perform_image_classification",
      "version": null,
      "title": "Steps to perform image classification",
      "subTitles": [
        "Perform all required allocations",
        "Build and populate your dataset",
        "Train your classifier context",
        "Predict with your classifier",
        "Save your classification contexts",
        "Free your allocated objects"
      ],
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Image_classification\\Steps_to_perform_image_classification.htm",
      "text": " Steps to perform image classification The following steps provide a basic methodology to perform image classification with the MIL Classification module: Perform all required allocations. Build and populate your dataset. Train your classifier context. Predict with your classifier. If necessary, Save your classification contexts. Free your allocated objects. Perform all required allocations These allocations are required to perform image classification. In many cases, some of these allocations will be imported or restored from previous work, using MclassImport() or MclassRestore(). Allocate a predefined CNN classifier context, using MclassAlloc() with M_CLASSIFIER_CNN_PREDEFINED and M_ICNET_.... Note, MclassTrain() can automatically allocate a classifier context if none is specified. Allocate an images dataset context to hold all of your data (source dataset), using MclassAlloc() with M_DATASET_IMAGES. Alternatively, you can restore an images dataset context, using MclassRestore(), or you can import data into an images dataset context, using MclassImport(). Allocate a CNN training context, using MclassAlloc() with M_TRAIN_CNN. A training context holds the settings with which to train a classifier context. Allocate a CNN training result buffer to hold training results, using MclassAllocResult() with M_TRAIN_CNN_RESULT. Allocate a CNN result buffer to hold the prediction results, using MclassAllocResult() with M_PREDICT_CNN_RESULT. Object allocation for image classification is summarized in the table below: MIL allocation Classifier context M_CLASSIFIER_CNN_PREDEFINED Specific predefined classifier context M_ICNET_... Dataset context M_DATASET_IMAGES Data preparation context M_PREPARE_IMAGES_CNN Training context M_TRAIN_CNN Training result buffer M_TRAIN_CNN_RESULT Prediction result buffer M_PREDICT_CNN_RESULT Build and populate your dataset It is recommended that you build and manage your datasets interactively, using MIL CoPilot. For example, MIL CoPilot lets you interactively create, label, modify, import, and export datasets. When using MIL CoPilot, ensure that you have installed all related MIL updates. For more information, see the Requirements, recommendations, and troubleshooting section of Chapter 47: Machine learning with the MIL Classification module. If you restore or import a dataset that is fully built, you can skip this step. If you are not importing a dataset, follow the steps below to build and populate the source dataset: Add class definitions to the source dataset, using MclassControl() with M_CLASS_ADD. Note, the number of classes with which to categorize your data is a key decision to make when performing image classification. Optionally, you can specify settings to help manage class definitions, using MclassControl(). For example, you can assign a color (M_CLASS_DRAW_COLOR) and an icon image (M_CLASS_ICON_ID) to class definitions. This allows you to draw and visually identify them with MclassDraw(). Add entries to the source dataset, using MclassControl() with M_ENTRY_ADD. Entries in an images dataset require image data (paths to where images are stored). To specify the location from which to get an entry's image data, use MclassControlEntry() with M_ENTRY_IMAGE_PATH. For each entry, specify the class definition (ground truth) that is represented, using MclassControlEntry() with M_CLASS_INDEX_GROUND_TRUTH. This step is known as labeling your data. You must label your data before training (labeled data is a prerequisite to using the module). The quality, quantity, and proportionality of correctly labeled data is critical to building a good dataset, and developing a properly trained classifier. Split your source dataset into the training, development, and testing datasets. You can split datasets manually, using MclassSplitDataset(), or you can pass a single dataset to MclassTrain(), and let MIL split it into the training and development datasets. If you want to use a testing dataset, you must first split a portion of the source dataset into a testing dataset using MclassSplitDataset(). Optionally, prepare your data by cropping or resizing images, and adding augmented images to a dataset. If you are letting MclassTrain() split your source dataset, data preparation is managed by the training contexts' internally defined data preparation context. You can prepare your data by calling MclassControl() with the identifier of the internal data preparation context (which you can inquire with M_PREPARE_DATA_CONTEXT_ID) and specifying the required data preparation controls (for example, M_PRESET_CROP and M_PRESET_NOISE_SALT_PEPPER). These preparations will be applied when you call MclassTrain(). You can use MclassPrepareData() to prepare data outside of MclassTrain(). To do this you must allocate a data preparation context (M_PREPARE_IMAGES_CNN). You should not use MclassPrepareData() on a dataset if you are letting MclassTrain() split the dataset. After you build a dataset, you can export it using MclassExport() so you can train a classifier with it at a later time. It is recommended that you export it with M_IMAGE_DATASET_FOLDER. You can use MclassImport() to import previously defined and exported datasets (for example, from a folder or a CSV file). It is recommended that you familiarize yourself with how dataset information (such as images) is stored on disk, and how you can ensure that information is well organized and portable. For more information, see the Guidelines for managing an images dataset section of Chapter 48: Datasets. Train your classifier context To train your classifier context on your datasets, perform the following: Modify training settings, using MclassControl() and MclassControlEntry(). Optionally, hook functions to training events, using MclassHookFunction(). Preprocess the training context, using MclassPreprocess(). Perform the training operation, using MclassTrain(). Optionally, get information about training events that caused the hook-handler function to execute, using MclassGetHookInfo(). If results indicate that the current training operation will be unsuccessful, stop the training, and modify your training settings and if necessary your dataset, and re-train. Optionally, get training results, using MclassGetResult(). As indicated in the previous step, if results indicate an unsuccessful training (for example, the classifier performed poorly on the development dataset), modify your training settings and if necessary your dataset, and re-train. Copy the classification result buffer that MclassTrain() produced into a classifier context, using MclassCopyResult(). Once copied, the classifier context is considered trained. If your training results are unsatisfactory, adjust training settings, contexts, and datasets as required, and call MclassTrain() with the trained classifier context. Predict with your classifier To predict with the trained classifier context, perform the following: Preprocess the trained classifier context, using MclassPreprocess(). Perform the prediction operation with the trained classifier context and the target data that you want to classify, using MclassPredict(). If your training images were prepared, then the target image must also be prepared in the same way. This maintains consistency between the training data and the target data. If you are predicting with a test dataset and the predicted classes are not what you expect, when compared to the actual classes in the test dataset (the ground truth), you can adjust your training setup, and continue the training process. Optionally, you can perform the prediction operation with a dataset as your target. In this case, you can hook functions to prediction events, using MclassHookFunction(). Retrieve the required results from the prediction result buffer, using MclassGetResult(). You can also draw prediction results, using MclassDraw(). Save your classification contexts If necessary, save your classification contexts, using MclassSave() or MclassStream(). Free your allocated objects Free all your allocated objects, using MclassFree(), unless M_UNIQUE_ID was specified during allocation. Steps to perform image classification Perform all required allocations Build and populate your dataset Train your classifier context Predict with your classifier Save your classification contexts Free your allocated objects ",
      "wordCount": 1167,
      "subEntries": []
    },
    {
      "id": "UG_ML_Image_classification_Building_a_dataset_for_image_classification",
      "version": null,
      "title": "Building a dataset for image classification",
      "subTitles": [
        "Populate the source dataset context",
        "Splitting your dataset",
        "Augmentation and other data preparations",
        "Data preparation settings",
        "Augmenting images",
        "Exporting and importing a dataset",
        "Export",
        "Import"
      ],
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Image_classification\\Building_a_dataset_for_image_classification.htm",
      "text": " Building a dataset for image classification This section discusses specific options for building a dataset for image classification. For an overview of the many considerations you should make when building a dataset, see Chapter 48: Datasets. It is recommended that you use MIL CoPilot to create, label, modify, augment, and export your dataset interactively. Populate the source dataset context Add class definitions to the source dataset, using MclassControl() with M_CLASS_ADD. These are the classes that will be represented in your dataset. Optionally, you can specify settings to help manage class definitions, using MclassControl(). For example, you can assign a color (M_CLASS_DRAW_COLOR) and an icon image (M_CLASS_ICON_ID) to class definitions. This allows you to draw and visually identify them with MclassDraw(). Add entries to the source dataset, using MclassControl() with M_ENTRY_ADD. Every entry must refer to an image and must identify its ground truth (the class label that identifies that image). To specify the location from which to get an image entry's data, use MclassControlEntry() with M_ENTRY_IMAGE_PATH. Specify the class label for each entry using MclassControlEntry() with M_CLASS_INDEX_GROUND_TRUTH. The table below illustrates the components of an image dataset for image classification. Entry Image Label Entry 0 Image 0 Class 0 Entry 1 Image 1 Class 1 Entry 2 Image 2 Class 2 Typically, the dataset holds images representing many classes. Datasets can also hold images representing just two classes. Such datasets are usually used for image classification related to a boolean type of validation; that is, to determine whether or not an image is acceptable (or, in other words, whether an image is good or bad). If this is the case, you might consider performing anomaly detection. When you are finished adding your data, you should ensure that all of the images in your source dataset are the same size. This is a requirement for training a CNN classifier. To do this, call MclassPrepareData() and specify your source dataset context. To specify how to resize images, use the M_SIZE_MODE and M_RESIZE_SCALE_FACTOR controls. Resizing can be performed automatically or with explicit image width and height values. Note, using smaller images will allow for faster training. Splitting your dataset You can call MclassSplitDataset() to split a dataset into two smaller datasets. You can do this to create a training, development, or testing dataset out of your source dataset. If you want to use a testing dataset, you should first split a portion of the source dataset into a testing dataset by calling MclassSplitDataset() with M_SPLIT_CONTEXT_DEFAULT or M_SPLIT_CONTEXT_FIXED_SEED. For more information about the different datasets, see the Different datasets and how they are split section of Chapter 48: Datasets. Augmentation and other data preparations Optionally, prepare your data and add augmented images to a dataset using MclassPrepareData(). You can call MclassPrepareData() with an images dataset, or an individual image. You should not use MclassPrepareData() on a dataset if you are letting MclassTrain() split the dataset, since MIL will automatically augment your data with the internally defined data preparation context. You can call MclassControl() with the identifier of the internal data preparation context (which you can inquire with M_PREPARE_DATA_CONTEXT_ID) and specify the required data preparation controls (for example, M_PRESET_CROP and M_PRESET_NOISE_SALT_PEPPER). These preparations will be applied when you call MclassTrain(). For more information, see the Data augmentation and other data preparations section of Chapter 48: Datasets. Data preparation settings MclassPrepareData() requires that you allocate a data preparation context by calling MclassAlloc() with M_PREPARE_IMAGES_CNN. The data preparation context holds the settings with which to modify the specified source (the images in a dataset or an individual image). Preprocess the data preparation context by calling MclassPreprocess(). You must also specify the location in which prepared images will be stored by setting M_PREPARED_DATA_FOLDER. For more information, see the Data augmentation and other data preparations section of Chapter 48: Datasets. Augmenting images To prepare your data with specific, preset augmentations, you can call MclassControl() and enable the M_PRESET... augmentation operation setting. Alternatively, you can access additional types of augmentation operations using the data preparation context's internal augmentation context. This context is automatically managed by MIL (you need not allocate it or free it) and is an internal version of the image processing context for augmentation (that is, MimAlloc() with M_AUGMENTATION_CONTEXT). Augmented entries, and the entries used to augment them, must only be in the training dataset. Augmented entries in the development dataset can cause errors. For more information on augmentation, see the Data augmentation and other data preparations section of Chapter 48: Datasets. Exporting and importing a dataset Exporting and importing your datasets allows you to make your datasets portable and organized. This will allow you to easily reuse or combine your datasets. For more information, see the Guidelines for managing an images dataset section of Chapter 48: Datasets. Export After you build a dataset, export it using MclassExport(). It is recommended that you export it with M_IMAGE_DATASET_FOLDER to create an organized folder than can be easily imported and reused. Import You can use a folder or CSV file to define data for a dataset. To import it to a dataset context, use MclassImport() and specify what to import (for example, M_COMPLETE, M_ENTRIES, M_AUTHORS, and M_CLASS_DEFINITIONS). For more information about importing data, see the Importing data from a folder or CSV file section of Chapter 48: Datasets. You can also use MclassRestore() to restore a dataset that was previously saved to a file using MclassSave() or MclassStream(). Building a dataset for image classification Populate the source dataset context Splitting your dataset Augmentation and other data preparations Data preparation settings Augmenting images Exporting and importing a dataset Export Import ",
      "wordCount": 931,
      "subEntries": []
    },
    {
      "id": "UG_ML_Image_classification_Classifier_and_training_settings_for_image_classification",
      "version": null,
      "title": "Classifier and training settings for image classification",
      "subTitles": [
        "Training objects and folders",
        "Training modes"
      ],
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Image_classification\\Classifier_and_training_settings_for_image_classification.htm",
      "text": " Classifier and training settings for image classification Before you can start training, you need to allocate a classifier context and training objects, and set training related settings. For more information on training settings, see the Fundamental decisions and settings section of Chapter 49: Training. Training objects and folders Allocate a training context, using MclassAlloc() with M_TRAIN_CNN. A training context holds the settings with which to train a classifier context, such as training modes. You will also need to allocate a training result buffer to hold training results, using MclassAllocResult() with M_TRAIN_CNN_RESULT. Set the destination folder that will store prepared images using MclassControl() with M_TRAIN_DESTINATION_FOLDER. Training modes Depending on the problem definition, different training modes exist for your classifier. To set these training modes for your training context, call MclassControl() with M_RESET_TRAINING_VALUES. Complete (M_COMPLETE). Typically, this is for completely restarting the training of a CNN, segmentation, or object detection classifier context, or for training a CNN, segmentation, or object detection classifier context that is not trained. Transfer learning (M_TRANSFER_LEARNING). Typically, this is for a CNN or segmentation classifier context that was already trained on a specific classification problem, and that you must train on a similar (but new) problem. Fine tuning (M_FINE_TUNING). Typically, this is for a CNN or segmentation classifier context that was already trained on a specific classification problem, and that you must train with additional data. When you specify a training mode, MIL automatically sets the related training mode controls to the required settings. You can also modify these controls yourself to adjust the training process by calling MclassControl(). The training mode controls let you adjust the: Learning rate (M_INITIAL_LEARNING_RATE and M_LEARNING_RATE_DECAY). For image classification, the default learning rate is 0.005 and the default learning rate decay is 0.1. Maximum number of epochs (M_MAX_EPOCH). For image classification, the default maximum number of epochs is 60. Mini-batch size (M_MINI_BATCH_SIZE). For image classification, the default mini-batch size is 32. Schedule type (M_SCHEDULER_TYPE). For image classification, the default schedule type is M_CYCLICAL_DECAY. Such training mode controls are also known as hyperparameters. If your classifier is not performing as expected, you can adjust the hyperparameters and retrain. Once you have established your training mode controls (and all other settings for your training context), you must preprocess the context by calling MclassPreprocess() with the identifier of the training context before training. Classifier and training settings for image classification Training objects and folders Training modes ",
      "wordCount": 403,
      "subEntries": []
    },
    {
      "id": "UG_ML_Image_classification_Training_and_analysis_for_image_classification",
      "version": null,
      "title": "Training and analysis for image classification",
      "subTitles": [
        "Monitoring the training process",
        "Results",
        "Training analysis",
        "Confusion matrix"
      ],
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Image_classification\\Training_and_analysis_for_image_classification.htm",
      "text": " Training and analysis for image classification When you call MclassTrain(), the classifier context will be trained with the settings specified in the training context, and trained on the data in the training and development datasets. The results from training will be stored in the classification result buffer. These must all be specified when you call MclassTrain(). For more information on the training process, and training analysis, see the Analysis, adjustment, and additional settings section of Chapter 49: Training. Monitoring the training process You can save time and improve the training process by using MclassHookFunction() to hook a function to a training event. Call MclassGetHookInfo() to get information about the event that caused the hook-handler function to be called. Training can take a long time, and you can use hook functions to monitor the training process. It is recommended that you call a hook function at the end of each mini-batch (M_MINI_BATCH_TRAINED), and also at the end of each epoch (M_EPOCH_TRAINED), to ensure that training is developing in the correct direction. While your classifier is training, you can get the information from the events that caused the hook-handler function to execute by using MclassGetHookInfo(). Typically, you should expect to monitor the training process for proper convergence, and to make modifications to the process. You might need to abort or restart it, if required. Results After training is completed, retrieve your training results by calling MclassGetResult() with the training result buffer that MclassTrain() produced. You will often get results related to accuracy, such as the accuracy of the training dataset (M_TRAIN_DATASET_ACCURACY) and the development dataset (M_DEV_DATASET_ACCURACY). You can also get training results from a specific entry in either the training or development dataset by calling MclassGetResultEntry(). To do this, you must first copy the dataset results from the result buffer that MclassTrain() produces to a dataset using MclassCopyResult(). If your training results are not as you expected (for example, if the classifier is clearly performing poorly by over-fitting to the development dataset), then you can make adjustments and train again. Copy the classification result buffer into a classifier context, using MclassCopyResult() with M_TRAINED_CLASSIFIER. Once copied, the classifier context is considered trained. If necessary, you can always continue to adjust training settings and contexts, and call MclassTrain() with the trained classifier context. Training analysis Analyzing your training results is critical to building a well-performing classifier. Confusion matrix A confusion matrix tells you information about how many entries were correctly and incorrectly classified during training in the training or development dataset. It can be used to identify weaknesses in your classifier, particularly when dealing with an unbalanced dataset. The confusion matrix is available by calling MclassGetResult() with M_TRAIN_DATASET_CONFUSION_MATRIX or M_DEV_DATASET_CONFUSION_MATRIX. Training and analysis for image classification Monitoring the training process Results Training analysis Confusion matrix ",
      "wordCount": 463,
      "subEntries": []
    },
    {
      "id": "UG_ML_Image_classification_Prediction_for_image_classification",
      "version": null,
      "title": "Prediction for image classification",
      "subTitles": [
        "Prepare for prediction",
        "Predict",
        "Results",
        "Drawing results",
        "Assisted labeling"
      ],
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Image_classification\\Prediction_for_image_classification.htm",
      "text": " Prediction for image classification MclassPredict() uses a trained classifier context to make class predictions on a target. For a trained CNN classifier, your target is either an image or a dataset of images. For more information about prediction, see the Prediction settings, results, and drawings section of Chapter 50: Prediction. Prepare for prediction Allocate a classification result buffer to hold the prediction results, using MclassAllocResult() with M_PREDICT_CNN_RESULT. Image classification with ICNET classifiers requires target images that are the same size as the images used during training. The size of the training images can be inquired using MclassInquire() with M_SIZE_X and M_SIZE_Y. Before predicting, preprocess your trained classifier context using MclassPreprocess(). Predict Perform the prediction operation with the trained classifier context and the target data that you want to classify using MclassPredict(). If your training images were prepared by cropping and resizing, then the target image must also be prepared in the same way. This maintains consistency between the training data and the target data. The size of the target images must be the same as the images used to train your ICNet classifier. When performing the prediction operation with a dataset as your target, you can hook functions to prediction events, using MclassHookFunction(). Results Retrieve the required results from the classification result buffer, using MclassGetResult(). You can also get results from a specific entry in a dataset using MclassGetResultEntry(). Typically, the most important prediction results to retrieve are the best predicted class (M_BEST_CLASS_INDEX) and its score (M_BEST_CLASS_SCORE). Drawing results To draw prediction results, call MclassDraw(). You can also draw prediction results from a specific entry in a dataset using MclassDrawEntry(). You can perform drawing operations to, for example, illustrate the best class result (M_DRAW_BEST_INDEX_IMAGE or M_DRAW_BEST_INDEX_CONTOUR_IMAGE) or the resulting class scores (M_DRAW_BEST_SCORE_IMAGE and M_DRAW_CLASS_SCORES). To draw the icon image related to the class, use M_DRAW_CLASS_ICON. Note, this image is not a result; you specify it using MclassControl() with M_CLASS_ICON_ID, and you draw it from a classifier context or a dataset context (not a result buffer). By drawing the class' icon image, you are able to visually identify the class for which you are getting results. Similarly, you can also specify and draw a color related to a class, to help visually identify it (M_DRAW_CLASS_COLOR_LUT). In general, drawing operations for prediction results can prove particularly useful when performing an image classification prediction with a child buffer or a buffer with a region. Assisted labeling You can perform assisted labeling for image classification by adding prediction results to your dataset. For more information about assisted labeling, see the Assisted labeling subsection of the Advanced techniques section of Chapter 50: Prediction. Prediction for image classification Prepare for prediction Predict Results Drawing results Assisted labeling ",
      "wordCount": 453,
      "subEntries": []
    },
    {
      "id": "UG_ML_Image_classification_Image_classification_example",
      "version": null,
      "title": "Image classification examples",
      "subTitles": null,
      "location": "MIL UG P08: Machine learning tasks",
      "pageURL": "content\\UserGuide\\ML_Image_classification\\Image_classification_example.htm",
      "text": " Image classification examples The following examples demonstrate how to perform image classification with the MIL Classification module. The example ClassCNNCompleteTrain.cpp demonstrates how to train a predefined (Matrox defined) CNN classifier context to classify 3 different types of fabric. To view this example, refer to the following: classcnncompletetrain.cpp The example ClassPrintedChar.cpp demonstrates how to restore a pretrained (Matrox defined and trained) classifier context to perform a type of OCR image classification. To view this example, refer to the following: classprintedchar.cpp The example ClassSeaFoodInspect.cpp demonstrates how to restore a pretrained (Matrox defined and trained) classifier context to accept or reject mussels depending on whether they have pieces of shell on them. To view this example, refer to the following: classseafoodinspect.cpp The example Mclass.cpp demonstrates how to restore a pretrained (Matrox defined and trained) classifier context to identify and categorize different kinds of pasta. To view this example, refer to the following: mclass.cpp To run these examples, use the Matrox Example Launcher in the MIL Control Center. Image classification examples ",
      "wordCount": 169,
      "subEntries": []
    }
  ]
}]
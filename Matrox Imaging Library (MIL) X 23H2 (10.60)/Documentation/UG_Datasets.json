[{
  "id": "UG_Datasets",
  "version": "2024020714",
  "title": "Datasets",
  "subTitles": null,
  "location": "MIL UG P07: Machine learning fundamentals",
  "pageURL": "content\\UserGuide\\Datasets\\ChapterInformation.htm",
  "text": " Chapter 48: Datasets This chapter explains how to use datasets with the MIL Classification module. Datasets overview Images dataset versus features dataset Basic concepts for datasets Data collection and identification Data foresight Classes and labeling (the ground truth) UUID MIL_UUID utility macros for C users Different datasets and how they are split Source dataset Splitting data Training dataset Development dataset Testing dataset An example of distributing data among datasets Data augmentation and other data preparations Preparing for preparation Images produced Augmentation Importing data from a folder or CSV file Folder CSV General CSV file format Headers for authors Headers for class definitions Headers for entries Guidelines for managing an images dataset Data on disk Folders Data preparation and training with a single dataset Exporting and importing Consolidation Creating portable and properly structured datasets Image classification Segmentation and object detection Adding portability to an existing dataset Exporting to disk Special DATASET path alternative ",
  "wordCount": 154,
  "subEntries": [
    {
      "id": "UG_Datasets_Datasets_overview",
      "version": null,
      "title": "Datasets overview",
      "subTitles": [
        "Images dataset versus features dataset"
      ],
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\Datasets\\Datasets_overview.htm",
      "text": " Datasets overview Datasets are the lifeblood of machine learning. Regardless of the task (image classification, segmentation, object detection, or feature classification), a well trained classifier is always the goal, and this is only possible with properly constructed datasets. Investing time into building good datasets will make your classifier more robust and save you time in training. A good dataset includes data that you have collected, such as training images, that is correctly labeled and representative of the task to perform. The quality, quantity, and proportionality of accurate data is critical to building a good dataset. Having a proper set of labeled data is a prerequisite for using the MIL Classification module (and, in general, for using supervised classification technologies). It is recommended to decouple dataset creation from usage. Since constructing a dataset involves disk storage, it should be done first, properly, and on its own. Once the dataset is ready, you can use it (for example, restore it) for training or prediction. For more information, see the Guidelines for managing an images dataset section later in this chapter. For the most recent documentation of this topic, particularly as it relates to anomaly detection and statistical analysis (MclassStatCalculate()), check for an updated version of the MIL Help online at zebra.com/aurora-imaging-library-help. Images dataset versus features dataset You can allocate two types of datasets with MIL: one for image data and one for feature data. The one you use depends on the task you want to perform. Dataset Task Image classification Segmentation Object detection Feature classification Images dataset (M_DATASET_IMAGES) ? ? ? - Features dataset (M_DATASET_FEATURES) - - - ? All datasets share a common organizational structure that is like a table; each row represents one complete and representative instance of your data (an entry), and each column represents the parts you need to define that data. Since an images dataset holds images, each entry (which is like a row) should contain information (which is like a column) that identifies the path to an image. Image classification requires one whole image; segmentation and object detection should also specify multiple regions (for example, regions identifying defects). Since a features dataset holds numerical data, each entry should contain values that identify features, such as a series of blob features (for example, length, width, and height). Regardless of the information held (whole images, images and regions, or features), each entry should also hold the class label (ground truth) that identifies the data it represents. For example, if you are performing image classification to identify apples and oranges, each entry in that dataset must not only specify an image of a specific apple or orange, it must also codify that data with the corresponding class label (Apple or Orange). If you are performing segmentation and object detection, there would be multiple class labels per entry, corresponding to each region (for example, an image with 3 defects would have 3 regions, and each region would have a class label identifying the type of defect). If you are performing feature classification, there would be multiple feature values and 1 class label identifying what those values mean. Dataset entry for image classification ImageFileLocation-01 ClassLabel ImageFileLocation-02 ClassLabel Dataset entry for segmentation and object detection ImageFileLocation-01 This image has 3 regions. ClassLabel, ClassLabel, ClassLabel ImageFileLocation-02 This image has 4 regions. ClassLabel, ClassLabel, ClassLabel, ClassLabel Dataset entry for feature classification FeatureValue-01, FeatureValue-02, FeatureValue-03 ClassLabel FeatureValue-01, FeatureValue-02, FeatureValue-03 ClassLabel As discussed later, dataset entries can hold other information, such as a unique key (UUID). Although each entry represents a real world example of your data, it is the dataset as a whole, that must be representative of the entire real world environment from which the data exists. For example, if your factory (the real world) occasionally captures blurry images (approximately 1% of the time), and half your dataset contains blurry images, then your dataset as a whole is not a proper representation of the problem you are trying to solve, even though the individual entries (examples) are. Only a dataset that has proper entries and is as a whole properly populated can result is a well trained classifier. It is highly recommended to use MIL CoPilot for managing your images datasets, and to have installed all related MIL updates. For more information, see the Requirements, recommendations, and troubleshooting section of Chapter 47: Machine learning with the MIL Classification module. Datasets overview Images dataset versus features dataset ",
      "wordCount": 734,
      "subEntries": []
    },
    {
      "id": "UG_Datasets_Basic_concepts",
      "version": null,
      "title": "Basic concepts for datasets",
      "subTitles": null,
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\Datasets\\Basic_concepts.htm",
      "text": " Basic concepts for datasets The basic concepts and vocabulary conventions for datasets are: Augmentation. Creating plausible variations of a source dataset entry to increase the number of entries. Dataset entry. One row of fields in a dataset. The data defined in an entry's fields include the class (label) to which the entry belongs (the ground truth), an image (file name and path) or set of features (numerical values) representing that class, and a UUID (key) that uniquely identifies that entry. Dataset entries are also known as samples or inputs; each entry in a features dataset is also known as a set of features or a feature set. Descriptor. Specifies a region in an entry image. Descriptors can be bounding boxes (for object detection), polygons, or masks (for segmentation). A single region can contain up to 1 mask descriptor, any number of polygon descriptors and up to 1 one box descriptor. Development dataset. The dataset that evaluates the performance of the classifier's training and regulates overfitting. Entries in the development dataset, training dataset, and testing dataset should be unique to their set. Tree ensemble classifiers do not typically require a development dataset; such classifiers use out-of-bag entries, which can evaluate the training's performance. Entry image. The image specified in an entry of an images dataset. Features dataset. A dataset that contains features (lists of values) that can be used with a tree ensemble classifier. Ground truth (GT). The class to which a dataset entry belongs. You should determine this with direct human observation or, in some cases, assisted labeling. Images dataset. A dataset that holds the images with which to train a predefined CNN, segmentation, or object detection classifier. Labeling. Specifying the class definition (ground truth) that a dataset entry represents. Training requires labeled entries. For segmentation and object detection, each dataset entry should have multiple class definitions (one for each region). Source dataset. A dataset that holds all the data with which to train a classifier. Typically, a source dataset is split into a training dataset, a development dataset, and a testing dataset. Testing dataset. An optional dataset containing data that was not seen by the classifier during training. A testing dataset can serve as a final check to assess the performance of the classifier. Entries in the testing dataset, training dataset, and development dataset should be unique to their set. Training dataset. The dataset that trains the classifier (the training dataset entries update the classifier's weights). Entries in the training dataset, development dataset, and testing dataset should be unique to their set. Training image. An image that is used to train a classifier. Universally unique identifier (UUID). A unique key that identifies dataset entries, authors, and class definitions. Basic concepts for datasets ",
      "wordCount": 455,
      "subEntries": []
    },
    {
      "id": "UG_Datasets_Data_collection_and_identification",
      "version": null,
      "title": "Data collection and identification",
      "subTitles": [
        "Data foresight",
        "Classes and labeling (the ground truth)",
        "UUID",
        "MIL_UUID utility macros for C users"
      ],
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\Datasets\\Data_collection_and_identification.htm",
      "text": " Data collection and identification The data (images or features) in your datasets must properly represent what you are trying to classify. The quality and quantity of your data directly impacts the classifier's training. To help ensure proper data, collect it using the final imaging setup (the same camera, lens, and illumination) and use real samples (images) of the subject matter, or as close as possible to them. Your dataset should include all expected variations of what you want to classify, and in sufficient number, such as variations in aspect, color, intensity, rotation, and dimension. Pay special attention to variations that are difficult to obtain and, if necessary, use augmentation to synthesize such variations (for example, rotation and flip). The following example illustrates variations (images) of a single apple object (class). To classify multiple objects, each of which is a separate class (for example, apples, oranges, and pears), you must provide, for each class, numerous training images of every variation. MIL internally divides that data, which usually occupies a lot of memory, into several sets (for example, by using mini-batches or bagging). Gathering as much data as possible helps ensure it is evenly consumed by training. It is recommended to have a minimum of 500 dataset entries (images or sets of features) per class, although simple applications can require less, and complex applications can require more. The number of entries required to properly train a classifier context depends on the complexity of the problem, the number of classes, and the number of variations within the classes. A balanced dataset is key to achieving a properly trained classifier. In a balanced dataset, each class should have the same number of entries representing it and, within each class, you should have the same number of entries representing each variation. You can obtain a balanced dataset by performing data augmentation with MclassPrepareData(). Note, obtaining a perfectly balanced dataset for segmentation or object detection is not always possible because an image might contain multiple regions with different labels. Typically, the data in all your datasets come from the same distribution (the same environment and setup). However this is not always the case. For example, the training dataset might contain images from different sources or systems, such as images of fabrics from the internet and also from the target application, while the development dataset might only contain images of fabrics taken from the target application. In such cases, you can create another dataset (the training development set) which has images from the same distribution as the training dataset, and use it to determine the presence of data mismatch issues between the training dataset and the development dataset. These dataset recommendations are for a complete (default) training, which means you are training a classifier from the ground up; other types of training, such as transfer learning, require less data. For more information, see the Training modes subsection of the Fundamental decisions and settings section of Chapter 49: Training. Data foresight The data that you collect and the classes with which it is organized must represent, as much as possible, a well-posed and consistent problem that you want to train the classifier to solve. Be aware that there are usually several ways to formulate a problem and present the data to solve that problem; it is imperative that this is done in a consistent and perceptive manner. You must know the problem that you want to solve and provide the data to solve it, while at the same time being mindful of ambiguities or unwanted associations that could lead to uncertain or even mistaken classifications. Illumination variances in your data represents one of the ways a classifier can make an inadvertent association. If illumination is irrelevant to determining the class, the data with which you train should contain different illuminations to remove the bias that a specific illumination condition can introduce. For example, if your dataset has images of good parts taken near a window on a cloudy day and images of defective parts taken near that window on a sunny day, it is likely for the classifier to learn that a dark image means a good part and a bright image means a defective part. It is vital to anticipate the important variations and provide that data; this allows the classifier to properly learn how to solve the problem without unwanted bias. Classes and labeling (the ground truth) MIL uses supervised training, which requires you to label your dataset entries with the class they represent. This label is called the ground truth. Class labels must be unique. Datasets should have 2 or more class labels, depending on the type of problem you are trying to solve; for example: For a two-class problem, you could specify a Good or Bad class label for every dataset entry (every image or set of features in a dataset is identified as one of these two classes). Another set of labels to use for such problems is Defective or NotDefective. For an n-class problem, you could specify one of n-labels (for example, ClassMetal, ClassCarpet, or ClassWood) for every dataset entry (every image or set of features in a dataset is identified as one of these n-classes). Another set of labels to use for such problems is ClassMetalGood, ClassMetalWithHole, or ClassMetalWithScratch. To label images (indicate the ground truth class that they represent, such as Good or Bad), you can use MIL CoPilot. Alternatively, you can build a simple utility that calls MIL. If you have a large amount of data to label, it might be possible to label some of it, train your classifier context, call the prediction operation to label the unlabeled dataset entries, and then use those newly labeled entries in your training dataset to continue the training process. For more information, see the Assisted labeling subsection of the Advanced techniques section of Chapter 50: Prediction. UUID A UUID refers to a universally unique identifier that is used for identification purposes across unrelated systems and applications. The following is an example of an automatically generated key (UUID): 87fbb05a-b078-4389-8ad2-2a2f9707c8f4. Every dataset entry has a UUID value that uniquely identifies it; this is also known as the entry's key. Since datasets can contain numerous entries from multiple datasets constructed on unrelated systems, this universally unique key helps ensure that the entries from all the datasets are not confused with each other. MIL automatically generates a UUID when required, such as, when you add an entry to a dataset (M_ENTRY_ADD). Although you do not create you own UUIDs, you can use them to access an entry to, for example, inquire about it or modify it. The MIL custom data type for UUIDs is MIL_UUID. When using a MIL_UUID variable, MIL defines the M_DEFAULT_UUID and M_NULL_UUID constants, which allow you to specify the corresponding default UUID value or a null UUID value. MIL_UUID utility macros for C users To perform comparisons with MIL_UUID values in C, use the following macros: M_COMPARE_MIL_UUID(VarA, VarB). This macro evaluates an equal (==) comparison operation between 2 variables. M_IS_DEFAULT_UUID(VarA) or M_IS_DEFAULT_KEY(VarA). These macros evaluate whether a variable is equal to the default UUID. These macros are equivalent. M_IS_NULL_UUID(VarB) or M_IS_NULL_KEY(VarB). These macros evaluate whether a variable is equal to a null UUID. These macros are equivalent. The following are examples of how to use these macros in C: MIL_UUID VarA = M_DEFAULT_UUID; MIL_UUID VarA = M_DEFAULT_UUID; if(M_COMPARE_MIL_UUID(VarA, VarB)) { /* Do something if VarA == VarB; */ } if(M_IS_DEFAULT_UUID(VarA)) { /* Do something if VarA == M_DEFAULT_UUID; */ } if(M_IS_NULL_UUID(VarB)) { /* Do something if VarB == M_NULL_UUID; */ } Note, you can use MIL_UUID variables normally in a C++ program, since the MIL_UUID data type, in C++, implements the equal (==) and not equal (!=) comparison operators. Data collection and identification Data foresight Classes and labeling (the ground truth) UUID MIL_UUID utility macros for C users ",
      "wordCount": 1309,
      "subEntries": []
    },
    {
      "id": "UG_Datasets_Different_datasets_and_how_they_are_split",
      "version": null,
      "title": "Different datasets and how they are split",
      "subTitles": [
        "Source dataset",
        "Splitting data",
        "Training dataset",
        "Development dataset",
        "Testing dataset",
        "An example of distributing data among datasets"
      ],
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\Datasets\\Different_datasets_and_how_they_are_split.htm",
      "text": " Different datasets and how they are split The MIL Classification module makes use of the following datasets: Source dataset. Training dataset. Development dataset (not needed and usually not used for feature classification). Testing dataset. What these datasets are, and how they established (split) and used, is described below. Source dataset Your source dataset should hold all the data with which to train your classifier. This dataset typically includes data that you explicitly collect, and can also included imported data. It is important that this dataset contains only unique images (no duplicates) and does not contain augmented images. Given enough time, the training accuracy of a sufficiently large classifier that uses a single source dataset eventually converges to an accuracy of 100% (0% error). However, this does not mean that the training is successful. Several sources of errors can limit and improperly bias the performance of a classifier that you trained using a single dataset to the point where, if you use the trained classifier with similar but different data (images), it will almost surely fail. Even though it is recommended that you specify a single source dataset, internally, MIL splits that dataset into the training dataset and the development dataset, as is required by the theoretical principles of image classification, segmentation, and object detection. In so doing, the classifier not only learns to classify a specific set of images (the training dataset), but also learns the general principles with which to classify the images so you can use it to successfully classify images with which it did not train (the development dataset). You can also have a testing dataset and use it with MclassPredict() to serve as a quarantined final check for any trained classifier. Splitting data You can call MclassSplitDataset() to split a dataset into two smaller datasets. You can do this to create a training, development, or testing dataset out of your source dataset. Alternatively, you can specify a single dataset and let MclassTrain() split it into the training and development datasets, as per the related split control settings, such as M_SPLIT_PERCENTAGE and M_SPLIT_SEED_MODE (this is not done for feature classification, since only the training dataset is needed). If you want to use a testing dataset, then you should first split a portion of the source dataset into a testing dataset by calling MclassSplitDataset(). Keep in mind that, regardless of how you create your dataset contexts, the data in each of them must come from the final imaging setup (for example, you should capture all training images in all datasets using the same kind of resolution and illumination conditions, which should in turn use the same kind of resolution and illumination conditions at prediction time). Only the training dataset can contain augmented data; the development dataset and the testing dataset must not contain any augmentations. You must manually split your dataset if you want to apply custom augmentations to your training dataset. For more information, see the Data augmentation and other data preparations section later in this chapter. Training dataset The training dataset context holds the entries that update the internal parameters (weights) of the classifier. In general, it is recommended that the training dataset holds about 70% of your source data. If your source dataset is significantly large, you can increase this percentage to 80% or 90%. As previously discussed, when using tree ensemble classifiers, your training dataset typically holds 100% of your data (unless you want to use some for the testing dataset). Development dataset MIL uses the development dataset to evaluate the performance of the classifier on data that is not involved in establishing the classifier's internal parameters (weights). This is done at the end of each epoch. Once training is complete, an analysis of the training metrics might indicate several actions to take before training once more to obtain better results. In some cases, you can analyze training metrics at the end of an epoch to decide whether to continue or abort training. For more information, see the Analysis, adjustment, and additional settings section of Chapter 49: Training. In general, it is recommended that the development dataset holds about 10% to 30% of your source data. The development dataset and the testing dataset, if you have one, typically have the same percentage. For example, if your training dataset has 70% of your data, then the development dataset and testing dataset would each have 15%. Testing dataset The testing dataset contains labeled data that resembles the training data, but was completely unseen by the training process. You can use it to help ensure that your classifier is free from any bias it might have inadvertently learned, and as a way to validate the final performance of the trained classifier. Testing data is initially separated from the source dataset to populate a testing dataset. To make use of the testing dataset, you must pass it to MclassPredict(), and compare the predicted class to the ground truth class. The accuracy of prediction results from a properly trained classifier should be overwhelmingly similar to the accuracy of training results. In general, the testing dataset is about the same size as the development dataset. Note, for segmentation and object detection, you would usually use other metrics to help confirm a successful training. For example, for segmentation, you would use results related to intersection over union (such as M_DEV_DATASET_EPOCH_IOU_MEAN and M_TRAIN_DATASET_EPOCH_IOU_MEAN), and for object detection, you would use results related to bounding box coordinates (such as M_BOX_4_CORNERS). An example of distributing data among datasets The following is an example of how data is typically distributed among datasets containing image data (for image classification, segmentation, or object detection). Note, 30% of the data in class A, B, and C is split among the development and testing datasets (this amounts to 15% for each class in each of these datasets). Different datasets and how they are split Source dataset Splitting data Training dataset Development dataset Testing dataset An example of distributing data among datasets ",
      "wordCount": 994,
      "subEntries": []
    },
    {
      "id": "UG_Datasets_Data_augmentation_and_other_data_preparations",
      "version": null,
      "title": "Data augmentation and other data preparations",
      "subTitles": [
        "Preparing for preparation",
        "Images produced",
        "Augmentation"
      ],
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\Datasets\\Data_augmentation_and_other_data_preparations.htm",
      "text": " Data augmentation and other data preparations Ideally, your source dataset is initially filled with a sufficient quantity and quality of images for a successful training. In practice, successful training often requires that you further prepare your source data for training, for example, by augmenting your training images (such as adding translated, rotated, and blurred images), or cropping and resizing images (since training images should be the same size). MIL facilitates these data modifications with MclassPrepareData(). You can call MclassPrepareData() with an images dataset or a specific image as your source (for image classification, segmentation, or object detection). You cannot call MclassPrepareData() with a features dataset (you cannot use it for feature classification). Note, MclassPrepareData() does not support preparing signed images. MclassPrepareData() only performs augmentations (such as translations, rotations, and blur) if you specify a dataset as your source; if you specify an image as your source, augmentation settings are ignored, although cropping and resizing settings are applied. MclassPrepareData() only adds augmented images to the destination dataset if at least one augmentation operation is enabled; by default; no augmentations are enabled. Augmented images should only be in your training dataset; do not allow them in your development or testing dataset, since errors can occur. Other preparations such as cropping and resizing can be made on images in any dataset, and are often required to meet image size requirements. To hook functions to data preparation events, call MclassHookFunction(). Preparing for preparation MclassPrepareData() requires a data preparation context, which you must allocate depending on your machine learning task (that is, you must call MclassAlloc() with M_PREPARE_IMAGES_CNN, M_PREPARE_IMAGES_DET or M_PREPARE_IMAGES_SEG). The data preparation context holds the settings with which to modify the source data (SrcDatasetContextOrImageBufId). To change the data preparation context settings, call MclassControl(). For example, you can use the M_SIZE_... controls to set the size of the images that MclassPrepareData() produces, or you can use the M_AUGMENT_NUMBER_ABSOLUTE control to set the number of augmented entries to add to the destination dataset. You can consider data preparation (MclassPrepareData()) as taking the specified source data (SrcDatasetContextOrImageBufId), modifying it (according to your data preparation settings), and copying the modified version into the destination (DstDatasetContextOrImageBufId). One image in your source can result in multiple modified images in your destination, depending on your data preparation settings. Source data is not modified. Class definitions and authors are copied from the source dataset to the destination, if they do not exist in the destination. To prepare your data with specific, preset augmentations, you can call MclassControl() and enable the M_PRESET... augmentation operation settings. Alternatively, you can access additional types of augmentation operations using the data preparation context's internal augmentation context. This context is automatically managed by MIL (you need not allocate it or free it) and is an internal version of the image processing context for augmentation (that is, MimAlloc() with M_AUGMENTATION_CONTEXT). Note, some preset augmentations are not available for binary and grayscale images. To specify the additional augmentations, you must first inquire the identifier of this internal augmentation context, by calling MclassInquire() with M_AUGMENT_CONTEXT_ID. You can then use that augmentation context identifier with MimControl(), and perform any of the augmentation operations within MimControl(). The augmentations are applied when you call MclassPrepareData(). Images produced The modified (cropped/resized or augmented) images that MclassPrepareData() produces are placed in the specified dataset context or image buffer (DstDatasetContextOrImageBufId), depending on the source you want to prepare (SrcDatasetContextOrImageBufId). The images are also placed in the destination folder, specified by the M_PREPARED_DATA_FOLDER control. By default, each time you call MclassPrepareData(), a new PrepareX folder is created, where X is the next available value to create a unique folder name. You can change this behavior with the M_DESTINATION_FOLDER_MODE control. All the images that MclassPrepareData() produces (and copies into the prepared data folder) are MIMs (MIL images) and named as follows: OriginalName_Prp_AugmentationNumber.mim. For example, if MclassPrepareData() produces three augmentations of Abyssinian.mim, they would be named Abyssinian_Prp_1.mim, Abyssinian_Prp_2.mim, and Abyssinian_Prp_3.mim. The suffix '0' is reserved for images that are prepared but not augmented (for example, Abyssinian_Prp_0.mim would be a cropped/resized copy of the original image). Augmentation Data augmentation is a technique to synthetically generate more samples (entries) from a limited dataset to be used during training. Therefore, if you do not have enough training data, you can use augmentation to synthesize more. The ultimate goal is to have training data that is a full representation of all expected variations and conditions. If you cannot get the actual data, augmentation can be a solution. Data augmentation can prevent overfitting and can improve accuracy as well as robustness. Various types of data augmentation are available, such as: Transformation-related. Examples include translation, rotation, flip, shearing, aspect ratio, and scale. Intensity-related. Examples include smoothing, adding noise, brightness variations. The application of a specific augmentation type depends on the problem definition. The augmentation should make sense in the context of the application. For instance, if the rotation of an object does not happen in the real application or it changes the label of the image, then rotation should not be applied as an augmentation (or, only a few degrees of rotation could be used, if necessary). The following bottle cap inspection example illustrates the point. In this case, the original image is shown first, then, a rotation of 180 degrees is shown, but this actually can never happen in the application, so it must not be included in the set of augmented images that you use. Finally, the third image shows an 10 degree rotation, which can happen (the bottle might shake and tilt on the conveyor), so this augmentation can be used. An example of an augmentation changing the label of an image is a 180 degree rotation performed on an image of the number 6. The image would then represent the number 9, which has a different label. This type of augmentation should be avoided. Other types of data augmentations might be needed in addition to just supplementing original variations. For example, over time, images might be subject to change of focus and noise. These changes could have a noticeable impact on the network's accuracy. Hence, to overcome this problem, these artifacts should also be simulated through data augmentation. In some cases, the original dataset is not balanced. This means that there is a significant difference in quantity of labeled data between the different classes. As a result, the network might give too much importance to the class with more data. One solution to an imbalanced dataset is data augmentation. For example, 1000 images are available for class A (non-defective parts) and only 50 images are available for class B (defective parts). In this situation, more data augmentation could be applied for class B to narrow the gap between the two classes by setting M_AUGMENT_BALANCING with MclassControl(). This balancing is then applied when you call MclassPrepareData(). Such augmentations, to a single class, require especially prudent considerations, since you do not want the classifier to unrealistically learn how to identify that class (the augmentations must always reflect the overall problem). This in part explains why, as previously mentioned, you must not add augmented data to the development dataset or the testing dataset, since they are used to regulate and help you identify misguided classifications. For more information about setting up, performing, and analyzing an augmentation for images, see the Augmentation section of Chapter 5: Specialized image processing. Note the following recommendations: Only allow augmented entries in the training dataset. In this case, augmented entries also refers to the entries used to augment them. All of these entries must only be in the training dataset. Augment your data after splitting it into different datasets. It is not recommended to call MclassSplitDataset() to create the development dataset or testing dataset if your source dataset contains augmented entries. Identify augmented entries by calling MclassControlEntry() with M_AUGMENTATION_SOURCE. Disregard bagging information if your training dataset has augmented entries. Keep the source entry and its augmented entries in one dataset. The source and its variations are considered part of the augmentation. Note, original images should be a little larger than those in the final application, to ensure that augmented images do not contain overscan pixels. In the final training dataset, you can crop the images to meet the application's size requirements. Data augmentation and other data preparations Preparing for preparation Images produced Augmentation ",
      "wordCount": 1386,
      "subEntries": []
    },
    {
      "id": "UG_Datasets_Importing_data_from_a_CSV_file_to_a_dataset_context",
      "version": null,
      "title": "Importing data from a folder or CSV file",
      "subTitles": [
        "Folder",
        "CSV",
        "General CSV file format",
        "Headers for authors",
        "Headers for class definitions",
        "Headers for entries"
      ],
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\Datasets\\Importing_data_from_a_CSV_file_to_a_dataset_context.htm",
      "text": " Importing data from a folder or CSV file You can use a folder or CSV (comma-separated value) file to define data for a dataset (images or features), such as authors, class definitions, and entries. You can then add that data to a dataset context, using MclassImport(), and specify what to import (for example, M_COMPLETE, M_ENTRIES, M_AUTHORS, and M_CLASS_DEFINITIONS). You will typically be importing data that was previously exported from MIL or MIL CoPilot. To export a dataset (for example, to a CSV file), call MclassExport(). Folder In general, the most useful way to import data to a dataset is to do a complete import (M_COMPLETE) from a folder (M_IMAGE_DATASET_FOLDER). This imports the entire content of that folder, including subfolders, into the dataset. This includes authors (CSV), class definitions (CSV), controls (CSV), entries (images and CSV), descriptors (images and CSV), segmentation files (MIL buffers to hold the pixel data), prediction results (CSV), and icons (images). As indicated, the folder from which you are importing must contain the proper content, in the proper format (for example, CSV), and the structure of that folder, and its subfolders, must be coherent to MIL. Typically, the folder from which you are importing was previously a complete export from a dataset (MclassExport()). The following is an example of the resulting MIL folder structure after such an export. The folder names represent what MIL uses; the file names are fabricated. Datasets for image classification do not have descriptor, segmentation, or detection data; exporting such datasets would not result in those folders and files. DstFolder +---Icons¹ ¦ +---Class1.mim ¦ +---Class2.mim +---Images ¦ +---Class1² ¦ ¦ +---Image1.mim ¦ ¦ +---Image2.mim ¦ +---Class2 ¦ ¦ +---Image1.mim ¦ ¦ +---Image2.mim ¦ +---Image1.mim³ ¦ +---Image2.mim +---Masks4 ¦ +---GUID1.mim ¦ +---GUID2.mim +---Segmentations5 ¦ +---GUID1_0.mbufi ¦ +---GUID2_0.mbufi ¦ +---GUID3_0.mbufi +---authors.csv +---class_definitions.csv +---controls.csv +---descriptor_boxes.csv +---descriptor_masks.csv +---descriptor_polygons.csv +---entries.csv +---results_segmentations.csv +---results_detection.csv ¹This folder's content can be drawn using M_DRAW_CLASS_ICON. ²Sub-folders at this level contain images labeled, with respect to image classification, for each class. ³Images labeled for segmentation and object detection are under the Images folder. 4Contains the segmentation ground truth masks. 5Contains the segmentation prediction score tensors (this can be seen as a type of image with the scores for all pixels for each class). CSV When importing data from a CSV (MclassImport()), note the following: Import authors and class definitions first, and then import entries, as entries require authors and class definitions. If you only import entries, MIL automatically creates class definitions and authors according to the information in the entries. If you import class definitions or authors afterward, MIL appends them to the already existing ones. If MIL encounters non-existing class definitions or authors when importing entries, they are automatically added to the dataset using default values. When calling MclassImport(), you must specify what to import (CSV files); this includes: authors.csv. class_definitions.csv. controls.csv. descriptor_boxes.csv. descriptor_masks.csv. descriptor_polygons.csv. entries.csv. results_segmentations.csv. results_detection.csv. You can only specifically import the files authors.csv, class_definitions.csv, and entries.csv (using M_FORMAT_CSV with M_AUTHORS, M_CLASS_DEFINITIONS, or M_ENTRIES). Other CSVs, such as controls.csv, descriptor_boxes.csv, and descriptor_masks.csv are imported from a folder (M_IMAGE_DATASET_FOLDER) using M_COMPLETE. In this case, all CVSs are completely imported (including authors, class definitions, and entries). General CSV file format To import data from a CSV file, it must adhere to formatting requirements and contain the necessary headers and corresponding data (this can depend on what you are importing). The following is an example of valid content in a CSV file from which you can import entries (M_ENTRIES). Key,FilePath,AuthorName,AugmentationSource,RegionType,ClassIdxGroundTruth,UserString 0,E:\\Images\\Class1\\0001.mim,Zebra,NOT_AUGMENTED,WholeImage,0,Any useful 1,E:\\Images\\Class1\\0002.mim,Zebra,NOT_AUGMENTED,WholeImage,0,meta information 2,E:\\Images\\Class1\\0003.mim,Zebra,NOT_AUGMENTED,WholeImage,0,can be written here 3,E:\\Images\\Class1\\0004.mim,Zebra,NOT_AUGMENTED,WholeImage,0,in the UserString field. 4,E:\\Images\\Class2\\0001.mim,Zebra,NOT_AUGMENTED,WholeImage,1,Lines that end with a comma 5,E:\\Images\\Class2\\0002.mim,Zebra,NOT_AUGMENTED,WholeImage,1,indicate that there is no 6,E:\\Images\\Class2\\0003.mim,Zebra,NOT_AUGMENTED,WholeImage,1,meta information for that entry. 7,E:\\Images\\Class2\\0004.mim,Zebra,NOT_AUGMENTED,WholeImage,1, 8,E:\\Images\\Class2\\Augmented\\0001a.mim,Zebra,4,WholeImage,1,augmented version of the image from the entry index 4. Regardless of the data you import, CSV files must contain: Header fields (cells) on the first line that are separated by a comma. You do not need to list the headers in any particular order. One or more lines below the header containing the information fields corresponding to the headers. You must separate the information fields with a comma and order them according to the order of the header fields. You can leave information fields empty but cannot omit the comma between fields. Each line below the header is one entry in a dataset. Note, if you have N header fields, you should have N fields on every line below the header, and every line should have N-1 commas. A header called 'Key'. You can leave the corresponding key information in the lines below this header empty, or you can specify a unique number. In either case, MIL generates a UUID for each key field. Note, key information depends on the data you are importing. When importing entries, the key corresponds to the entry key, when importing authors or class definitions, the key corresponds to the author key and the class key, respectively. Typically, CSV headers are PascalCase terms that correspond to an entry setting in MclassControlEntry(). For example, the header 'AugmentationSource' corresponds to the M_AUGMENTATION_SOURCE setting. The information presented here indicates how CSV files are generally set up. For details on the exact information (such as headers) required for import, see MclassImport() in the MIL Reference. Headers for authors To import authors (M_AUTHORS), use the headers below (and provide the corresponding lines of information after the header) in the CSV file: Header name Required or optional Related MIL setting Images or features dataset Key Required M_AUTHOR_KEY (MclassInquire()) Both AuthorName Required M_AUTHOR_NAME (MclassControlEntry()) Both The following is an example of the content in a CSV file that you can use to import authors: Key,AuthorName 0,SM 1,FX 2,VC ,JR 4, Headers for class definitions To import class definitions (M_CLASS_DEFINITIONS), use the headers below (and provide the corresponding lines of information after the header) in the CSV file. Header name Required or optional Related MIL setting Images or features dataset Key Required M_CLASS_KEY (MclassInquire()) Both Name Required M_CLASS_NAME (MclassControl()) Both Color_R Optional The Red parameter of the M_RGB888() macro (MclassControl() with M_CLASS_DRAW_COLOR) Both Color_G Optional The Green parameter of the M_RGB888() macro (MclassControl() with M_CLASS_DRAW_COLOR) Both Color_B Optional The Blue parameter of the M_RGB888() macro (MclassControl() with M_CLASS_DRAW_COLOR) Both Weight Optional M_CLASS_WEIGHT (MclassControl()) Both The following is an example of the content in a CSV file that you can use to import class definitions: Key,Name,Color_R,Color_G,Color_G,Weight 0,Apples,,,, 1,Oranges,,,, If you do not specify the optional headers, you need not specify the empty fields below; for example: Key,Name 0,Apples 1,Oranges Headers for entries To import entries (M_ENTRIES), use the headers below (and provide the corresponding lines of information after the headers) in the CSV file. Header name Required or optional Related MIL setting Images or features dataset Key Required M_ENTRY_KEY (MclassInquireEntry()) Both FilePath Required M_ENTRY_IMAGE_PATH (MclassControlEntry()) Images dataset only AuthorName Optional M_AUTHOR_NAME (MclassControlEntry()) Both AugmentationSource¹ Optional M_AUGMENTATION_SOURCE (MclassControlEntry()) Both RegionType² Optional M_REGION_TYPE (MclassInquireEntry()) Images dataset only Data_...³ Optional M_RAW_DATA (MclassControlEntry()) Features dataset only ClassIdxGroundTruth_...³ Required M_CLASS_INDEX_GROUND_TRUTH (MclassControlEntry()) Both ClassIdxPredicted_...³ Optional M_BEST_CLASS_INDEX (MclassGetResultEntry()) Both ClassScorePredicted_...³ Optional M_BEST_CLASS_SCORE (MclassGetResultEntry()) Both UserString Optional M_ENTRY_USER_STRING (MclassControlEntry()) Both EntryWeight Optional M_ENTRY_WEIGHT (MclassControlEntry()) Features dataset only UserConfidence Optional M_USER_CONFIDENCE (MclassControlEntry()) Images dataset only PredefinedNetworkType Optional M_CLASSIFIER_PREDEFINED_TYPE (MclassGetResultEntry()) Images dataset only ReceptiveFieldSizeX Optional M_RECEPTIVE_FIELD_SIZE_X (MclassGetResultEntry()) Images dataset only ReceptiveFieldSizeY Optional M_RECEPTIVE_FIELD_SIZE_Y (MclassGetResultEntry()) Images dataset only ReceptiveFieldOffsetX Optional M_RECEPTIVE_FIELD_OFFSET_X (MclassGetResultEntry()) Images dataset only ReceptiveFieldOffsetY Optional M_RECEPTIVE_FIELD_OFFSET_Y (MclassGetResultEntry()) Images dataset only ReceptiveFieldAsymetricOffsetX Optional M_RECEPTIVE_FIELD_OFFSET_X (MclassGetResultEntry()) Images dataset only ReceptiveFieldAsymetricOffsetY Optional M_RECEPTIVE_FIELD_OFFSET_Y (MclassGetResultEntry()) Images dataset only ReceptiveFieldStrideX Optional M_RECEPTIVE_FIELD_STRIDE_X (MclassGetResultEntry()) Images dataset only ReceptiveFieldStrideY Optional M_RECEPTIVE_FIELD_STRIDE_Y (MclassGetResultEntry()) Images dataset only ¹If an entry is not augmented, set the corresponding information field to 'NOT_AUGMENTED'. ²If an entry uses the whole image, set the corresponding information field to 'WholeImage' ³This header represents an array of values. You must replace the ellipsis with an integer, starting at 0 for the first array value, and increasing by 1 for each subsequent array value, such as: 'Data_0,Data_1,Data_2'. The following is an example of the content in a CSV file that you can use to import class entries that have 3 values for the header field named 'Data_...': Key,FilePath,AuthorName,AugmentationSource,Data_0,Data_1,Data_2,ClassIdxGroundTruth 0,E:\\Images\\Class1\\0000.mim,SC,NOT_AUGMENTED,99,66,87,0 1,E:\\Images\\Class1\\0001.mim,SM,NOT_AUGMENTED,33,29,30,1 2,E:\\Images\\Class1\\0002.mim,FX,NOT_AUGMENTED,4,19,77,2 When listing the header fields for an array of values, ensure that you separate them with commas and you do not skip a number in the sequence of integer suffixes; for example, listing 'Data_0' and 'Data_2' will cause a \"missing field\" error. Importing data from a folder or CSV file Folder CSV General CSV file format Headers for authors Headers for class definitions Headers for entries ",
      "wordCount": 1405,
      "subEntries": []
    },
    {
      "id": "UG_Datasets_Guidelines_for_managing_an_images_dataset",
      "version": null,
      "title": "Guidelines for managing an images dataset",
      "subTitles": [
        "Data on disk",
        "Folders",
        "Data preparation and training with a single dataset",
        "Exporting and importing",
        "Consolidation",
        "Creating portable and properly structured datasets",
        "Image classification",
        "Segmentation and object detection",
        "Adding portability to an existing dataset",
        "Exporting to disk",
        "Special DATASET path alternative"
      ],
      "location": "MIL UG P07: Machine learning fundamentals",
      "pageURL": "content\\UserGuide\\Datasets\\Guidelines_for_managing_an_images_dataset.htm",
      "text": " Guidelines for managing an images dataset This section relates some guidelines, recommendations, and details to help you make the most efficient use of an images dataset. The number of images in a dataset, and the various purposes it can serve, make it likely that you will want to make your dataset portable and well organized. This will allow you to easily reuse your dataset, or combine it with other datasets. Data on disk An images dataset stores the bulk of its data (images) on disk. For example, you must save your entry images on disk before adding them as entries in the dataset (an images dataset holds the paths to each entry image, not the image itself). It is recommended to decouple dataset creation from usage. Since constructing a dataset involves disk storage, it should be done first, properly, and on its own. Once the dataset is ready, you can use it (by restoring or importing it) for training or prediction. Data stored on disk should be well organized and follow a recommended folder structure which, among other guidelines, separates information according to the dataset to which it belongs. Segmentation, as well as operations such as data preparation, training, and prediction, automatically adds information related to datasets to disk (including images) according to the folders, paths, and structures specified below. Although certain operations save new dataset information on disk, they do not typically delete it (with the exception of training with a single dataset). You must therefore be mindful of the resources required to save this data, and manage your disk storage accordingly. Folders The variety of information that an images dataset can hold is organized on disk according to folders. By default, some operations produce folders in the public user's document folder. To change this default location, use the MilConfig utility. For example, this folder path stores the images, segmentation and object detection regions (masks), and scores produced by MclassPrepareData() and MclassTrain(). You can also specify a root path by calling MclassControl() with M_ROOT_PATH. This can be set while the dataset is empty, or if the dataset file paths are made relative (M_MAKE_FILE_PATHS_RELATIVE). This represents the top-most path for all other paths explicitly specified in an image dataset (for example, entry images). All images datasets require, at least, entry images. An images dataset that is for segmentation or object detection holds additional data, such as the segmentation masks containing the ground truth information, which is saved at the folder specified by the M_REGION_MASKS_FOLDER control. The path of this folder must be set before you call MclassEntryAddRegion() with regions based on images. Segmentation prediction scores that MclassPredict() produces (every pixel has a class score) are saved with the destination dataset at the folder specified by the M_SEGMENTATION_FOLDER control. You must also set the path of this folder. Prediction scores for object detection (only instances are scored) are saved in the dataset. Data preparation and training with a single dataset New images and masks generated by MclassPrepareData() are saved at the folder specified by the M_PREPARED_DATA_FOLDER control before their paths are added to the destination dataset. Images and masks prepared when calling MclassTrain() with a single dataset are saved at the location specified by the M_TRAIN_DESTINATION_FOLDER control. The content of this folder is overwritten at each new training; prepared datasets must be copied to new datasets using MclassCopyResult() (M_PREPARED_DEV_DATASET / M_PREPARED_TRAIN_DATASET). You should then ensure such data is made portable, as described in the Creating portable and properly structured datasets subsection of this section. Exporting and importing The disk content of an images dataset that you exported using MclassExport() with M_IMAGE_DATASET_FOLDER and M_COMPLETE is copied under the destination folder specified with the FileNameOrFolderPath parameter (the dataset from which you exported is left unchanged). Since this is a complete copy, all data, along with CSV files holding complementary information (such as authors and segmentation prediction results), is copied in a structured set of folders (all under the destination folder). If you are importing data to an images dataset, the folder from which you are importing must adhere to the same type of folder structure the MIL uses when exporting datasets. For more information about this folder structure and the content therein, see the Importing data from a folder or CSV file section earlier in this chapter. Consolidation You can copy and reorganize a dataset according to a specified destination directory using the M_CONSOLIDATE_ENTRIES_INTO_FOLDER control. The structure of the directory is the same as when exporting a dataset, except it does not include class definition icons and CSV files. The following is an example: DstFolder +---Icons¹ ¦ +---Class1.mim ¦ +---Class2.mim +---Images ¦ +---Class1¹ ¦ ¦ +---Image1.mim ¦ ¦ +---Image2.mim ¦ +---Class2 ¦ ¦ +---Image1.mim ¦ ¦ +---Image2.mim ¦ +---Image1.mim² ¦ +---Image2.mim +---Masks³ ¦ +---GUID1.mim ¦ +---GUID2.mim +---Segmentations4 ¦ +---GUID1_0.mbufi ¦ +---GUID2_0.mbufi ¦ +---GUID3_0.mbufi ¹Sub-folders at this level contain images labeled, with respect to image classification, for each class. ²Images labeled for segmentation and object detection are under the Images folder. ³Contains the segmentation ground truth masks. 4Contains the segmentation prediction score buffers. In general, consolidation (M_CONSOLIDATE_ENTRIES_INTO_FOLDER) means that, within the destination folder, images of the dataset are copied to the images folder by class. If the dataset holds segmentation or object detection data, mask regions are copied in the Masks folder and segmentation scores will be copied in the Segmentations folder (object detection scores are in the dataset). The paths of all entries, masks, segmentation and object detection files, are updated as well as the Masks (M_REGION_MASKS_FOLDER) and Segmentations (M_SEGMENTATION_FOLDER) folders. Consolidation does not affect the source files. The disk content of datasets consolidated using MclassControl() (M_CONSOLIDATE_ENTRIES_INTO_FOLDER) is copied under the destination folder and all the paths in the dataset are updated to reflect the changes. Creating portable and properly structured datasets When creating an images dataset, it is recommended that it is portable (for example, easily transferred between computers) and properly structured. Such datasets are simpler to manage and expand which, in the long run, will save you time, since datasets are often fluid and changing (they are rarely static and unaltered). Even if the following guidelines are not followed initially, it is often worthwhile to regroup the information of each dataset according to them, given the benefits portability and proper structure provides. The initial steps for creating a portable and structured dataset are as follows, for image classification, segmentation, and object detection: Steps Notes Allocate a new dataset, using MclassAlloc() (M_DATASET_IMAGES). Save the dataset to an empty folder, using MclassSave(). Reload the dataset saved on disk, using MclassRestore(), or MclassStream() with M_LOAD, to properly set the special path ///DATASET///. Make the dataset file paths relative to ///DATASET///, using MclassControl() and M_ROOT_PATH set to MIL_TEXT(\"///DATASET///\")). In the dataset folder, create the subfolder Images.¹ ¹Once you create the Images subfolder, you should follow the steps specific to the task you are forming (Image classification, Segmentation, or Object detection). Regardless of whether an images dataset is for image classification, segmentation, or object detection, note that: When the dataset comes from a file on a disk, the special path ///DATASET/// refers to the parent directory of the dataset file (.mclassd). For example, if your dataset is saved in the location C:/Documents/Dataset.mclassd, ///DATASET/// will be interpreted as C:/Documents. The file name itself is not included in ///DATASET///, which helps make the dataset portable. Image classification Once you have created the Images folder, as previously discussed, perform the following for image classification datasets: Steps Notes In the Images subfolder, make a subfolder for each class of the dataset and name it accordingly.¹ Add classes to the dataset, using MclassControl() (M_CLASS_ADD)². Add images to the dataset, using MclassControl() (M_ENTRY_ADD) and MclassControlEntry() (M_ENTRY_IMAGE_PATH). Label the images using MclassControlEntry() (M_CLASS_INDEX_GROUND_TRUTH). Call MclassSave() to update the dataset saved on disk with these new changes. ¹Save labeled images in their appropriate class subfolder; save unlabeled images directly in the Images subfolder. ²It is a good practice to make the name of the classes the same as the name of the subfolders in step 1. Segmentation and object detection Once you have created the Images folder, as previously discussed, perform the following for segmentation and object detection datasets: Steps Notes Save the image files to the Images subfolder. Add classes to the dataset, using MclassControl() (M_CLASS_ADD). Add images to the dataset, using MclassControl() (M_ENTRY_ADD) and MclassControlEntry() (M_ENTRY_IMAGE_PATH). Optionally, if you intend to label the dataset, you can perform additional steps. Make the subfolder Masks under ///DATASET///. Set it using MclassControl() (M_REGION_MASKS_FOLDER, MIL_TEXT(\"Masks\")). Label the images using MclassEntryAddRegion(). When labeling the dataset, there are additional recommendations to follow¹. If necessary, MIL also provides some alternatives². Optionally, if you intend to use MclassPredict() with this dataset as destination (for segmentation), you should make the subfolder Segmentations under ///DATASET///, and set it using MclassControl() (M_SEGMENTATION_FOLDER, MIL_TEXT(\"Segmentations\")). Call MclassSave() to update the dataset saved on disk with these new changes. ¹Additional recommendations to follow, if you intend to label the dataset: For segmentation, make the ground truth image by labeling each of its pixel to the class index/color and use MclassEntryAddRegion() (M_GROUND_TRUTH_IMAGE / M_GROUND_TRUTH_IMAGE_COLOR). This call will create any class definitions missing when you added classes to the dataset (M_CLASS_ADD), extract the mask of each class, and save it in the Masks subfolder. ²Alternatives, if you intend to label the dataset: Make individual binary mask buffers and add them to the dataset, using MclassEntryAddRegion() with M_DESCRIPTOR_TYPE_MASK and the identifier of the image buffer from which to establish the added region. This call will save the mask buffers as files in the Masks subfolder. Save the individual segmentation masks in the Masks subfolder. Then, add individual segmentation or object detection regions (masks) to each entry, using MclassEntryAddRegion() with M_DESCRIPTOR_TYPE_MASK and the file name (along with the path) of the image from which to establish the added region. Adding portability to an existing dataset If you created a dataset that is not portable, as previously discussed, it is recommended to make it portable by exporting it to disk or, alternatively, modifying it to rely on the special path ///DATASET///. Exporting to disk To make an already existing dataset portable, it is recommended to: Write the dataset to an empty folder, using MclassExport() with M_IMAGE_DATASET_FOLDER and M_COMPLETE (this exports the entire dataset, including all CSV files). You can now manage the dataset as a folder, such as compressing it or transferring it to another computer. Read the exported dataset folder back into a MIL, using MclassImport() with M_IMAGE_DATASET_FOLDER and M_COMPLETE (this imports the entire dataset, including all CSV files). Special DATASET path alternative The following steps provide an alternative for making datasets portable. Although more complex, this does not involve CSV files or importing. Save the dataset to an empty folder, using MclassSave(). Reload the dataset saved on disk using MclassRestore(), or MclassStream() with M_LOAD. Consolidate the dataset into that folder, using MclassControl() with M_CONSOLIDATE_ENTRIES_INTO_FOLDER and specify MIL_TEXT(\"///DATASET///\")). Make the dataset file paths relative to ///DATASET///, using MclassControl() with M_MAKE_FILE_PATHS_RELATIVE. Call MclassSave() to update the dataset saved on disk with these new changes. You can now manage the dataset as a folder, such as compressing it or transferring it to another computer. Read the dataset back into MIL, using MclassRestore(). Guidelines for managing an images dataset Data on disk Folders Data preparation and training with a single dataset Exporting and importing Consolidation Creating portable and properly structured datasets Image classification Segmentation and object detection Adding portability to an existing dataset Exporting to disk Special DATASET path alternative ",
      "wordCount": 1909,
      "subEntries": []
    }
  ]
}]
[{
  "id": "UG_multiprocessing",
  "version": "2024020714",
  "title": "Multi-processing, multi-core, and multi-threading",
  "subTitles": null,
  "location": "MIL UG P11: Miscellaneous",
  "pageURL": "content\\UserGuide\\multiprocessing\\ChapterInformation.htm",
  "text": " Chapter 65: Multi-processing, multi-core, and multi-threading This chapter describes how MIL handles multi-processing on a single core, processing on multiple cores, and multi-threading. Multi-processing Transparent multi-core use Setting the maximum number of CPU cores per thread Controlling where and how the processing occurs Core affinity Priority NUMA Support Basic steps to using multi-core processing Multi-threading MIL and multi-threading Creating threads using MIL Thread execution Synchronization and mutex Thread control Using image processing and analysis contexts in multiple threads Thread-safety in MIL Error reporting ",
  "wordCount": 85,
  "subEntries": [
    {
      "id": "UG_multiprocessing_Multiprocessing",
      "version": null,
      "title": "Multi-processing",
      "subTitles": null,
      "location": "MIL UG P11: Miscellaneous",
      "pageURL": "content\\UserGuide\\multiprocessing\\Multiprocessing.htm",
      "text": " Multi-processing Multi-processing is the ability to execute various processes (applications) simultaneously. MIL applications are autonomous processes (or executables) designed to execute a complete operation or series of operations. Therefore, they can profit from multi-processing by executing independently, without interference from each other. All currently supported MIL systems support multi-processing. This means that the same board can be allocated by more than one process simultaneously; CPU and memory resources will be split between each process as needed. However, many resources (such as individual digitizers on some boards) cannot be allocated by more than one process simultaneously. Refer to the documentation for your particular hardware to determine which resources cannot be allocated by more than one process simultaneously. Multi-processing ",
      "wordCount": 119,
      "subEntries": []
    },
    {
      "id": "UG_multiprocessing_Transparent_MultiCore_Use",
      "version": null,
      "title": "Transparent multi-core use",
      "subTitles": [
        "Setting the maximum number of CPU cores per thread",
        "Controlling where and how the processing occurs",
        "Core affinity",
        "Priority",
        "NUMA Support",
        "Basic steps to using multi-core processing"
      ],
      "location": "MIL UG P11: Miscellaneous",
      "pageURL": "content\\UserGuide\\multiprocessing\\Transparent_MultiCore_Use.htm",
      "text": " Transparent multi-core use When dealing with a system allocated on a computer with multiple CPU cores (processors), MIL can transparently split the processing work of a MIL function between the CPU cores available to the process running your MIL application. Typically, this division of labor can greatly increase the overall execution speed of MIL functions. When multi-core processing is enabled for a thread (and supported in hardware), the work of most MIL functions on the thread is split into several parts and transparently sent to multiple CPU cores. When the work is done on all parts, the function returns and the result is made available. The goal of multi-core processing is to increase the function's processing speed. The following is a list of characteristics that can affect the speed gained using multi-core processing: Original speed. The splitting and merging of the work adds a certain amount of overhead to any function performed using multi-core processing. Therefore, if the original function is normally very fast, performing the function using multi-core processing might not provide a speed increase. This is often observed when processing small buffers. MIL tries to dynamically determine the best multi-core processing fit for the current function call to minimize the overhead; but it cannot be eliminated. I/O access. If the function has a high ratio of I/O accesses (such as, memory accesses) compared to the amount of actual processing, the function is I/O dependent. In such a case, performing the function using multi-core processing can cause all the CPU cores involved to fight for access to the same resource (such as, memory). The result is a smaller gain than expected and sometimes even less performance than the original single-core execution. Computer architecture. There are a lot more variables affecting the performance of a multiple CPU core host computer than a traditional single processor host computer. Each of these variables (for example, CPU, cache, bus, and memory) becomes more important in the final performance for each additional processor. Therefore, a small change in the architecture of the host computer can easily result in very different performance numbers. Scalability. Usually, multi-core processing will get faster as the number of CPU cores increases, but will gain less and less as more CPU cores are added. Often, the processing performance will even decrease beyond a certain number of CPU cores. This behavior is as much dependent on the hardware architecture as on the software, and nothing can be done to have linear scalability throughout the whole range of hardware and software requirements. MIL tries to reduce the performance decreases by applying predictive throughput algorithms, but cannot eliminate all possible performance losses as the number of CPU cores gets large. You can enable and change the multi-core processing default settings using the MILConfig utility and programmatically using MappControlMp() and MthrControlMp() functions. Setting the maximum number of CPU cores per thread You can manage the CPU core utilization of the multi-core processing part of MIL functions on each thread in a MIL application. By default, when enabled, multi-core processing spreads processing among all the CPU cores available to the process running the MIL application. In most cases, this provides good performance gains without further configuration. In some cases, for instance when using multiple threads, spreading multi-core processing among all the available CPU cores might lead to multiple threads requesting the same CPU cores at the same time. This might reduce the overall efficiency of your MIL application. To help reduce this problem, you can use MappControlMp() with M_CORE_MAX or MthrControlMp() with M_CORE_MAX. The former control type sets the default maximum number of CPU cores to which each thread can spread its multi-core processing, while the latter applies the same setting only to the specified thread. You should only adjust the M_CORE_MAX control types if using multiple threads and fine-tuning the performance of your MIL application. The total number of CPU cores available to run the multi-core processing part of MIL functions on a thread is always limited by your operating system and the number of CPU cores installed in your computer. Note that, the first call to MappAlloc() or MappAllocDefault() determines the number of CPU cores available from the operating system. This information is stored in MIL and not updated dynamically. Changing the number of processors available at the operating-system level, after your application is allocated in MIL, can result in erratic and unpredictable behavior. If, for example, the process has access to eight CPU cores and it has two processing threads, with multi-core processing enabled, each thread will try to spread its multi-core processing among all eight CPU cores. However, it might be more efficient to split the CPU cores between the two threads. If the threads have similar processing loads, you could restrict each thread to use four CPU cores, using MappControlMp() with M_CORE_MAX set to four. This configuration is typically more efficient than allowing both threads access to all eight CPU cores. If one thread is processing-intensive while the other is not, you could give the processing-intensive thread access to the majority of the available CPU cores using MthrControlMp() with M_CORE_MAX. For example, if the process running your application has access to 8 CPU cores, you could give your processing-intensive thread access to seven CPU cores, and the other thread access to a single CPU core. Note that MIL might not be the only application running that requires some form of processing; in this case, you should limit the number of CPU cores your MIL application uses, using MappControlMp() with M_CORE_MAX. This has the effect of limiting the number of cores each thread in your application can use. Controlling where and how the processing occurs In special circumstances, and for extreme optimization on very stable computing platforms, other options are available to control multi-core processing. Note that in many cases, the operating system might interfere with these controls. Therefore, it is often safer, and sometimes even faster, to leave them at their default settings. Furthermore, these controls are very dependent on the underlying hardware architecture, and their successful use can only be accomplished with a high degree of knowledge of the architecture details. Core affinity You can control exactly which CPU cores are used for multi-core processing within the entire application or for a specific thread using core affinities. A core affinity indicates on which CPU core(s) (processor(s)) should the scheduler of the operating system allow processing. In MIL, core affinity is represented by a bit-mask, where each bit represents the index of a CPU core. When a bit is set (1) at a specific position, multi-core processing is allowed to occur on that CPU core. When a bit is not set (0), multi-core processing is not allowed to occur on that CPU core. CPU cores always have the same indices, as long as the hardware in your computer and the operating system does not change. In MIL, core affinity bit-masks are passed in an array of MIL_UINT64 integers, where the least-significant bit of the first element represents CPU core 0, and the most-significant bit, CPU core 63. The least-significant bit of the second element (if needed), represents CPU core 64, and the most-significant bit, CPU core 127. This follows similarly for all of the following elements in the array. A normal core affinity bit-mask should have at least one bit enabled so that at least one CPU core is enabled for processing. A core affinity bit-mask whose bits are all set to zero is therefore a special case and represents the default setting of all CPU cores being enabled for processing. You can set the core affinity bit-mask using MappControlMp() with M_CORE_AFFINITY_MASK or MthrControlMp() with M_CORE_AFFINITY_MASK, where the latter only applies to the multi-core processing part of MIL functions on the specified thread. In addition, if an Intel processor is used with hyper-threading enabled, you can execute as if hyper-threading was disabled and restrict multi-core processing to one logical core per physical CPU core (hyper-threading), minimizing logical core interactions. To do so, use MappControlMp() with M_CORE_SHARING or MthrControlMp() with M_CORE_SHARING, where the latter only applies to the multi-core processing part of MIL functions on the specified thread. Note that the process or thread might not have exclusive access to the CPU cores; other processes or threads might still use the other logical cores of a physical CPU core and might impede multi-core processing restricted this way. In instances where you have multiple independent processing threads (for instance, when you have a multi-camera setup), it is advised to limit the interaction of the threads in each core. This can be done by setting the core affinity mask for each thread (MthrControlMp() with M_CORE_AFFINITY_MASK) and explicitly limiting the CPU cores used by each processing thread, taking care to avoid core-use overlap. Note that M_CORE_MAX, M_CORE_AFFINITY_MASK, and M_CORE_SHARING reduce the number of effective CPU cores to only those that meet the requirements of all three specifications. Priority You can set the processing priority of the multi-core processing part of MIL functions. To do so, use MappControlMp() with M_MP_PRIORITY or MthrControlMp() with M_MP_PRIORITY. In the latter case, only the multi-core processing part of MIL functions on the specified thread will be affected. Any priority set this way only affects multi-core processing. To set the priority of a thread, use MthrControl() with M_THREAD_PRIORITY. NUMA Support Non-uniform memory access (NUMA) is a computer architecture introduced by AMD, but has now been adopted by Intel as well. It is the use of multiple memory banks in a computer system, where each memory bank is local to a subset of the available processing CPU cores. Memory accesses are more efficient when using memory banks that are local to their respective CPU cores. NUMA is supported in MIL; you can allocate buffers in specific memory banks. Note that the explicit use of memory banks is an advanced optimization and scalability technique, and you must have a full understanding of the underlying computer architecture to successfully use it. The typical way to use NUMA is to first inquire information to help decide which memory bank to use. You can inquire about the memory banks that are local to the CPU cores available to the process running your application, using MappInquireMp() with M_MEMORY_BANK_NUM and MappInquireMp() with M_MEMORY_BANK_AFFINITY_MASK. The former returns the number of memory banks local to the process's CPU cores, while the latter returns a memory bank affinity mask representing the memory banks local to the process's CPU cores. You can inquire about the memory bank local to a specified CPU core using MappInquireMp() with M_CORE_MEMORY_BANK. This returns the index of the memory bank local to the specified core. As well, you can inquire about the CPU cores local to a specified memory bank, using MappInquireMp() with M_MEMORY_BANK_CORE_AFFINITY_MASK. This inquire type returns a core affinity mask representing the CPU cores local to the specified memory bank. When enough memory bank information is known, you can allocate buffers in more efficient memory banks using MbufAlloc...() with M_MEMORY_BANK_n, where n is the index of the memory bank. Basic steps to using multi-core processing The following steps provide a basic methodology for using multi-core processing controls and inquires: Enable multi-core processing using either the MILConfig utility or MappControlMp() with M_MP_USE set to M_ENABLE. Optionally, set the maximum number of CPU cores to use per thread, using either the MILConfig utility or MappControlMp() with M_CORE_MAX. Note that the effective number of CPU cores is always limited by the number of CPU cores available to the process running your MIL application, as per your operating system. To determine the number of CPU cores available to the process, use MappInquireMp() with M_CORE_NUM_PROCESS. Optionally, disable multi-core processing for one or more threads using MthrControlMp() with M_MP_USE set to M_DISABLE. Optionally, set the maximum number of CPU cores to use for a specific thread using MthrControlMp() with M_CORE_MAX. To determine the effective number of CPU cores that a specific thread can use, use MthrInquireMp() with M_CORE_NUM_EFFECTIVE. Note that, alternatively you can disable multi-core processing at the application level (using MappControlMp() with M_MP_USE set to M_DISABLE) and enable it for a specific thread (using MthrControlMp() with M_MP_USE set to M_ENABLE). Transparent multi-core use Setting the maximum number of CPU cores per thread Controlling where and how the processing occurs Core affinity Priority NUMA Support Basic steps to using multi-core processing ",
      "wordCount": 2053,
      "subEntries": []
    },
    {
      "id": "UG_multiprocessing_Multithreading",
      "version": null,
      "title": "Multi-threading",
      "subTitles": [
        "MIL and multi-threading",
        "Creating threads using MIL",
        "Thread execution",
        "Synchronization and mutex",
        "Thread control",
        "Using image processing and analysis contexts in multiple threads",
        "Thread-safety in MIL",
        "Error reporting"
      ],
      "location": "MIL UG P11: Miscellaneous",
      "pageURL": "content\\UserGuide\\multiprocessing\\Multithreading.htm",
      "text": " Multi-threading MIL also supports multi-threading. Multi-threading is the ability to perform multiple operations simultaneously in the same process. This is done by creating different threads (execution queues) to ensure sequential execution of operations within the same thread, while allowing simultaneous yet independent execution of operations in other threads. Threads within a process share the same data. Individual threads can communicate with each other and exchange data such as MIL identifiers. Multi-threading is most appropriate for applications where independent tasks can be done simultaneously but need to share data or to be controlled and synchronized within a main task. Multi-threading does not always result in an increase of speed and efficiency. Threads running simultaneously on the same CPU share the same resources (such as memory). When using a machine with multiple CPUs under Windows, the threads generally run on separate CPUs and provide more processing power. However, since they share the same memory, operations that are I/O intensive and require only simple processing might not be accelerated. For better performance, it is recommended to limit the interaction between multiple threads (for example, the synchronization between threads, and any shared resources). Furthermore, in a multi-threaded application, such as a multi-camera application, you will get better performance if the total number of threads, including the multi-processing threads, is equal to, or less than the number of cores. In such an application, you should either minimize core interaction or disable multi-core processing altogether. This can be done by using MthrControlMp() and limiting M_CORE_MAX for each thread, so that the sum of the value of M_CORE_MAX for all the threads is less than or equal to the number of cores on your computer. Alternatively, you can use MappControlMp() and disable multi-core use altogether, by setting M_MP_USE to M_DISABLE. Most applications do not require the use of multiple threads since there are other ways of multi-tasking. Mechanisms such as asynchronous grab and call-back functions can be used (see MdigControl() and MdigHookFunction()). Applications resolved by alternative means are often simpler to implement and easier to maintain than multi-threaded applications. MIL and multi-threading When your application contains several independent processing tasks that can be performed in parallel, you can design it so that each part is controlled by a separate thread (or task). Creating threads using MIL Under multi-thread operating systems, you can create as many threads as you require. You can create threads using commands provided by the operating system, or using the MthrAlloc() function provided by MIL. The MthrAlloc() function is portable, which means that it can be called from user-defined MIL functions that are executed on systems with multiple processors. For information on executing functions on systems with on-board processors, see the Master/slave dynamics on a remote system section of Chapter 67: The MIL function development module. There are two methods for creating threads using the MthrAlloc() function: With the first method (M_THREAD), the MthrAlloc() function creates a MIL thread context for the new thread, and allows you to specify a pointer to a function that will be executed by the thread. This user-created function must include all operations, and call all of the functions that you consider as being part of one thread. When a thread contains a function call whose target processor is an on-board processor that supports multi-threading, MIL automatically creates a corresponding thread on that system's on-board processor. The functions of the Thread module allow you to synchronize threads running on the Host and/or various MIL systems. With the second method, MthrAlloc() creates a selectable thread (M_SELECTABLE_THREAD). Selectable threads are threads executed on an on-board processor that supports multi-threading and can be controlled from a single corresponding thread on the Host. Use MthrControl() with M_THREAD_SELECT to send MIL functions to be executed by a selectable thread. Every thread in a MIL application, including the main thread which initially called MappAlloc(), shares the same application context settings. Calling MappControl() in any thread will affect the application context settings for every thread, unless M_THREAD_CURRENT is added to the ControlType. When M_THREAD_CURRENT is added to the ControlType, the specified application context setting applies only to the thread in which the call was made. When a thread changes an application context setting using M_THREAD_CURRENT, only the specified ControlType in the thread will be unique, and it will remain unique. For example, in a MIL application with three threads (Thread A, Thread B, and the main thread), all threads share the same application context settings, by default. The following calls to MappAlloc() demonstrate the shared and unique application context settings of this example. If Thread A calls MappControl() with M_ERROR + M_THREAD_CURRENT, then Thread A will have the new error setting, while Thread B and the main thread will still share the original default error setting. After that call, if either Thread B or the main thread calls MappControl() with M_ERROR, but without adding M_THREAD_CURRENT, Thread B and the main thread will change their error setting, while Thread A will remain with its unique setting. Finally, if after the first two calls any thread calls MappControl() with M_PARAMETER, but without M_THREAD_CURRENT, all three threads will share the new setting. While application context settings are generally shared between all threads (except when made unique with M_THREAD_CURRENT), thread context settings, controlled using MthrControl(), are unique to each thread. Thread execution MIL functions in any thread are executed as follows: If the target processor is the Host CPU, processing in each thread is determined by the operating system. If the target processor is an on-board processor of a system that supports multi-threading, MIL automatically creates, and eventually terminates, an on-board thread for each thread that sends commands to the board. Important Since the creation of on-board threads is done automatically, you do not have to specify the system on which to create a specific thread. However, you can do so by creating selectable threads on a particular system. Synchronization and mutex Thread synchronization is generally done using the Host synchronization services (such as Windows event objects). However, when using a system with an on-board processor, the activities of this processor cannot be synchronized by the Host. This means that threads continue execution without waiting for the execution of the on-board functions to complete. In most cases, this behavior is acceptable, since it leaves the Host available for other tasks. However, for operations that require sequential execution of functions to return valid results (for example, MbufGet() after an MdigGrab()), MIL automatically synchronizes the threads, forcing the Host to wait for completion of the earlier function(s). Explicit synchronization of threads is necessary if functions sharing a common resource might conflict with each other. For example, if two threads sharing the same image buffer are not synchronized, and each thread tries to clear the buffer to a different value, these functions might execute at the same time and the buffer could be cleared to either value or even to a combination of both. Use the synchronization features of the MthrControl(), MthrWait(), and MthrWaitMultiple() functions to synchronize the flow of threads. MthrWait() forces the current thread to wait for the completion of the specified thread or the change of state of the specified MIL event. MthrWaitMultiple() forces the current thread to wait for a change in state in one or all of the MIL events identified in a user-supplied array of events. MthrControl() allows you to use mutexes to synchronize the flow of threads. A mutual exclusion object allows threads to synchronize access to shared resources. Once a mutex is allocated on the specified system, you can lock and unlock critical sections of code. Locking a critical section of code ensures that no two threads can access the same data at the same time. To lock a MIL mutex, you must call MthrControl() with M_LOCK or M_LOCK_TRY immediately preceding the section of code to lock. If you lock a mutex, you must unlock it at the end of the critical section of code using MthrControl() with M_UNLOCK. Once you are finished using the mutex, free it using MthrFree(). The use of a mutex is illustrated in the following example with MimArith and MimFlip accessing the same data. MthrControl() with M_LOCK and M_LOCK_TRY are very similar; the difference occurs when one attempts to lock a mutex that is already locked. MthrControl() with M_LOCK will block the thread, forcing it to wait for the mutex to become unlocked before executing the critical section of code. MthrControl() with M_LOCK_TRY will not wait for the mutex to unlock. If the mutex is currently locked, the thread will proceed to execute, skipping the critical section of code. Note that once you call M_LOCK_TRY, you should immediately call MthrInquire() with M_LOCK_TRY to inquire whether the mutex was locked. Thread control Windows operating systems are multi-process and multi-thread operating systems. They provide various thread control services, including events (used to synchronize threads). The MthrAlloc() function serves as a link between MIL and the operating system; the function allows you to create an operating-system-independent MIL version of these services. Threads and events created using MthrAlloc() can be used in addition to, or instead of, the events created using commands of the operating system. MthrControl() controls and coordinates both threads and events. The MthrWait() function synchronizes thread processing by forcing a \"wait\" state. The MthrInquire() function inquires about both the settings of a MIL thread context and the state of an event allocated using MthrAlloc(). MthrFree() frees the allocated MIL thread context or event. The MthrAlloc() function allows you to specify a particular system on which to allocate MIL thread contexts or events. This permits you to synchronize the execution of functions on a specific MIL system. When creating multiple threads on a multi-core computer, use MappControlMp() and MthrControlMp() to restrict the number of cores used by each thread. Restricting the number of cores used by each thread might be more efficient; see the Transparent multi-core use section earlier in this chapter for more information. Using image processing and analysis contexts in multiple threads Multiple threads cannot share an image processing or analysis context, but you can duplicate the context for use across more than one thread. For all the image processing and analysis modules, use their M...Stream() function to duplicate their context. Using M...Stream() is a faster and more convenient way of duplicating contexts than with M...Save() and M...Restore(). The basic steps to duplicate a context are: Call M...Stream() with M_INQUIRE_SIZE_BYTE to return the number of bytes needed to store the context. Allocate some memory of the same size as was established in step 1. Call M...Stream() with M_SAVE and M_MEMORY to save the context to the allocated memory. Call M...Stream() with M_LOAD or M_RESTORE to create a duplicate context. At the end of each application, all the duplicated contexts, including the original context, must be freed using M...Free(). Thread-safety in MIL To avoid race conditions, it is important to know the guarantees that MIL provides with regards to thread-safety. There are multiple approaches to thread-safety and MIL implements a few of these mechanisms through support for re-entrancy and the mutual exclusion (M_MUTEX) mechanism. Re-entrancy is an approach to thread-safety which focuses on avoiding shared state conditions. With regards to MIL, it means that a MIL function can be safely called concurrently from multiple threads, as long as the data passed to the function's parameters (for example, image buffers) is not shared between threads. Unless otherwise specified, all MIL functions are considered to be re-entrant. You can use the second approach to thread-safety which deals with synchronization in situations where shared states cannot be avoided. The mutual exclusion mechanism allows you to serialize access to shared data such as a MIL object. This means that only one thread can read from and/or write to the shared data at a time. The mutual exclusion mechanism is implemented with a platform independent mechanism using the M_MUTEX object. In MIL, unless otherwise stated, functions that access or modify a MIL object cannot be called concurrently on the same object in multiple threads without using the mutual exclusion mechanism (M_MUTEX). For more information, see the Synchronization and mutex subsection of this section. Error reporting To check for errors, use the MappGetError() function. In multi-thread environments, a call to MappGetError() returns the last error that occurred in the current thread. Some functions in MIL are asynchronous, that is, they queue their command to the hardware and then immediately return control to the Host. Errors are logged once the function is executed on the processor of the specified system, and are reported the next time a MIL function is executed on the same system. By default, MappGetError() is asynchronous; it will therefore not report the most recent error in this situation. you can specify that MappGetError() should wait for all pending function calls to complete before returning by specifying M_SYNCHRONOUS. Multi-threading MIL and multi-threading Creating threads using MIL Thread execution Synchronization and mutex Thread control Using image processing and analysis contexts in multiple threads Thread-safety in MIL Error reporting ",
      "wordCount": 2164,
      "subEntries": []
    }
  ]
}]
[{
  "id": "UG_image",
  "version": "2024020714",
  "title": "Fundamental image processing",
  "subTitles": null,
  "location": "MIL UG P03: 2D processing and analysis",
  "pageURL": "content\\UserGuide\\image\\ChapterInformation.htm",
  "text": " Chapter 3: Fundamental image processing This chapter describes the steps to developing a typical application with the MIL Image Processing module. Image processing overview MIL and image processing Steps to performing a typical application A typical application How to encode these steps Image quality Techniques to improve images Averaging an input sequence Denoise using spatial filtering and area open and close operations Low-pass spatial linear filters Rank filters Adaptive filters Area open and area close Erosion and dilation Erosion Dilation An example Opening and closing Image statistics Generating a histogram Calculating general statistics Steps to calculate general statistics Considering pixel values as unit vectors and calculating angular statistics Coherence Dominant angle Dominant orientation Calculating statistics across multiple images Finding the image extremes Locating events Counting image differences Projecting an image to one dimension Calculating the integral of an image Adjusting the intensity distribution Window leveling Window leveling using MimRemap() Window leveling using MimLutMap() Window leveling using MimMorphic() Histogram equalization Adaptive histogram equalization Thresholding your images Binarizing Determining the threshold value from the histogram Specifying the threshold value(s) manually Clipping Adaptive binarizing Adaptive binarize contexts Controlling an adaptive binarize context Threshold with Niblack Threshold with local mean Threshold with Bernsen Threshold with pseudomedian Hysteresis Controlling an adaptive binarize context that uses seeds Threshold with geodesic reconstruction Threshold with leveling Threshold with toggling Removing uneven lighting from grabbed images How to remove non-uniform lighting from grabbed images Adjusting image focus Auto-focusing Edge enhancement Extending your depth of field Enhancing and detecting edges Edge enhancement Sharpening edge enhancement Edge detection Horizontal and vertical edge detection Laplacian-based edge detection Gradient-based edge detection Basic geometric transforms Interpolation modes Nearest-neighbor interpolation Bilinear interpolation Bicubic interpolation Average interpolation M_INTERPOLATE interpolation Maximum/minimum interpolation Mathematics with images Basic mathematical operations Computing the integral of an image Mapping an image ",
  "wordCount": 302,
  "subEntries": [
    {
      "id": "UG_image_Image_processing_in_general",
      "version": null,
      "title": "Image processing overview",
      "subTitles": [
        "MIL and image processing"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\image\\Image_processing_in_general.htm",
      "text": " Image processing overview Pictures, or images, are important sources of information for interpretation and analysis. These might be images of a building undergoing renovations, a planet's surface transmitted from a spacecraft, plant cells magnified with a microscope, or electronic circuitry. Human analysis of these images or objects presents inherent difficulties: the visual inspection process is time-consuming and subject to inconsistent interpretations and assessments. Computers, on the other hand, are ideal for performing these tasks. In order for computers to process images, the images must be numerically represented. This process is known as image digitization. Once images are represented digitally, computers can reliably automate the extraction of useful information through the use of digital image processing. Digital image processing performs various types of image enhancements, distortion corrections, and measurements. MIL and image processing MIL provides a comprehensive set of image processing operations. There are two main types of image processing operations: Those that enhance or transform an image. Those that analyze an image (that is, generate a numeric or graphic report that relates specific image information). MIL supports such operations as: Point-to-point operations. These operations include constant thresholding, image comparison, image subtraction, and image mapping. They compute each pixel result as a function of the pixel value at a corresponding location in either one or two source images. Statistical operations. These extract statistical information from a given image, such as the minimum or maximum image pixel value or a histogram. They condense a frame of pixels into a smaller, more functional set of values for analysis. Spatial filtering operations. These operations are also known as convolutions. They include operations that can smooth images, enhance and detect image edges, and remove 'noise' from an image. Most of these operations compute results based on an underlying neighborhood process: the weighted sum of a pixel value and its neighbors' values. Morphological operations. These operations include erosion, dilation, opening, and closing of images. They compute new values according to geometric relationships and matches to known patterns in the input image. Geometric transformation operations. These operations are used to resolve distortion problems. They can properly orient an image, flip the image vertically or horizontally, and resize an image to obtain the right aspect ratio. Image processing overview MIL and image processing ",
      "wordCount": 377,
      "subEntries": []
    },
    {
      "id": "UG_image_Steps_to_performing_a_typical_application",
      "version": null,
      "title": "Steps to performing a typical application",
      "subTitles": [
        "A typical application",
        "How to encode these steps"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\image\\Steps_to_performing_a_typical_application.htm",
      "text": " Steps to performing a typical application We have described the broad set of operations included in the MIL Image Processing module. Most applications do not require all of these operations. Image processing pertains to more than one field and no single application program can solve the problems associated with each one of these fields. Therefore, this section describes what we believe to be a typical problem, where the solution makes use of most of the supported operations. It also outlines the steps to take to implement this solution. A typical application In analyzing an image of a tissue sample, you might want to know the number of cell nuclei that are larger than a certain size, indicating an abnormality. The following steps provide a basic methodology for performing image processing with MIL: Grab or load an image of a magnified tissue sample. Smooth the image to reduce noise produced during the grab. Binarize the image so that the cell nuclei or particles and the background have different values: represent particles in white and the background in black. This will allow you, later, to label each particle with a unique number. Perform an opening operation to remove small particles from the image. Label each particle with a unique consecutive number starting with the label 1. Calculate and read the extreme value of the image. This value also corresponds to the largest label. Since the image particles are labeled with consecutive unique numbers, the largest valued particle is also labeled with a number that corresponds to the number of particles in the image. How to encode these steps The following sample program (MImProcessing.cpp) shows you how to encode these steps, using an existing image of a tissue sample (Cell.mim). mimprocessing.cpp Steps to performing a typical application A typical application How to encode these steps ",
      "wordCount": 304,
      "subEntries": []
    },
    {
      "id": "UG_image_Image_quality",
      "version": null,
      "title": "Image quality",
      "subTitles": [
        "Techniques to improve images"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\image\\Image_quality.htm",
      "text": " Image quality Prior to manipulating and extracting information from an image, many applications require that you obtain the best possible digital representation of it. Several factors affect the quality of an image. These include: Random noise. There are two main types of random noise: Gaussian noise. When this type of noise is present, the exact value of any given pixel is different for each grabbed image; this type of noise adds to or subtracts from the actual pixel value. Salt-and-pepper noise (also known as impulse or shot noise). This type of noise introduces pixels of arbitrary values (usually high-frequency values) that are generally noticeable because they are completely unrelated to the neighboring pixels. Random noise can be caused, for example, by the camera or digitizer because electronic devices tend to generate a certain amount of noise. If the images were transmitted, the distance between the sending and the receiving devices also magnifies the random noise problem because of interference. Systematic noise. Unlike random noise, this type of noise can be predicted, appearing as a group of pixels that should not be part of the actual image. This can be caused, for example, by the camera or digitizer or by uneven lighting. If the image was magnified, microscopic dust particles, on either the object or a camera lens, can appear to be part of the image. Distortions. Distortions appear as geometric transforms of the actual image. These can be caused, for example, by the position of the camera relative to the object (not perpendicular), the curvature in the optical lenses, or a non-unity aspect ratio of an acquisition device. Focus. Focus issues result in objects or features of objects appearing blurry in the image. These issues can be caused, for example, by grabbing the image such that the objects are at different focus depth levels, given the proximity of the camera. An image with focus issues can be more difficult to process with accurate results, since the edges and other features present within the image appear less well-defined. Techniques to improve images Most interference problems cannot be adjusted very easily at the source; therefore, preprocessing will probably be required to improve the image as much as possible, without affecting the information that you are seeking. There are several techniques that you can use to improve your image: Grab the object of interest several times, averaging each image frame with the previous. This technique is generally effective on Gaussian random noise. Apply a low-pass spatial filter to your image to reduce Gaussian random noise and systematic noise with small scale variations. This technique replaces each pixel with a weighted sum of its neighborhood. Apply a median filter to your image to reduce salt-and-pepper noise. This technique replaces each pixel with the median pixel value of its neighborhood. Perform a morphological opening operation to remove small particles and break isthmuses between objects in your image. Perform a morphological closing operation to remove small holes in objects. Make sure that your camera and digitizer grab the image with square pixels (that is, a 1:1 aspect ratio), to reduce object-shape distortions. If this is not possible or does not correct the problem, you can resize the image, using MimResize(); alternatively, you can calibrate your camera setup and then associate this camera calibration with your image, using the MIL Camera Calibration module. Apply a dead-pixel correction to your image to replace dead-pixels with valid image data based on neighboring pixels. Apply a gain and offset correction to remove electrical bias, thermal agitation, and sensitivity variations from your camera's charge-coupled device (CCD). Note that, some of these techniques are not possible using MIL-Lite. Image quality Techniques to improve images ",
      "wordCount": 614,
      "subEntries": []
    },
    {
      "id": "UG_image_Averaging_an_input_sequence",
      "version": null,
      "title": "Averaging an input sequence",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\image\\Averaging_an_input_sequence.htm",
      "text": " Averaging an input sequence An effective technique to remove random noise is to average a grabbed sequence of the same target image. For instance, Gaussian noise affects the value of any given pixel for each grabbed frame, adding to or subtracting from the actual pixel value. Therefore, over several image acquisitions, this noise averages out to zero. As a rule of thumb, the standard deviation of the noise is reduced by a factor roughly equal to the square root of the number of averaged frames. In other words, the signal-to-noise ratio increases as the square root of the number of frames increases. With MIL, you can average an input sequence, using either one of the following operations: Adding all input frames and then dividing the result by a specified weight factor. To specify this operation, use MimArith(). Adding weighted input frames to a weighted accumulator buffer ((Iacc = aIin + (1 - a)Iacc) or (Iacc = a(Iin - Iacc) + Iacc)). To specify this operation, use MimArithMultiple() with M_WEIGHTED_AVERAGE. The latter approach also acts as a temporal filter if the input is changing. This allows you to filter out moving objects from a constant background. To not lose any frames in your sequence, you can use a technique called multiple buffering, where you can grab data into one buffer while other buffers are processed. For more information, see the Multiple buffering subsection of the Grabbing and processing section of Chapter 27: Grabbing with your digitizer, and for an example on how to implement multiple buffering, see the MdigProcess.cpp file in MIL's example directory. Averaging an input sequence ",
      "wordCount": 269,
      "subEntries": []
    },
    {
      "id": "UG_image_Denoising",
      "version": null,
      "title": "Denoise using spatial filtering and area open and close operations",
      "subTitles": [
        "Low-pass spatial linear filters",
        "Rank filters",
        "Adaptive filters",
        "Area open and area close"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\image\\Denoising.htm",
      "text": " Denoise using spatial filtering and area open and close operations Spatial filtering and area open and close operations are effective ways to reduce noise. Spatial filtering operations determine each pixel's value based on its neighborhood values. They allow images to be separated into high-frequency and low-frequency components. There are two main types of spatial filters that can remove noise: low-pass linear filters and rank filters. Area open and close operations are efficient in removing salt-and-pepper noise in an image. Spatial filtering and area open and close operations can reduce noise, but are not especially effective at removing mean square errors (MSE). To minimize MSE, try using MimWaveletDenoise(). For more information, see the Wavelet denoising subsection of the Transform and denoise images using wavelets section of Chapter 5: Specialized image processing. Note that to remove Gaussian noise, you can use several techniques, such as spatial filtering, wavelet denoising, or averaging a sequence of grabbed images (see the Averaging an input sequence section earlier in this chapter). Low-pass spatial linear filters Low-pass spatial linear filters are effective in reducing Gaussian random noise (and high-frequency systematic noise), provided that the noise frequency is not too close to the spatial frequency of significant image data. These filters replace each pixel with a weighted sum of each pixel's neighborhood. Note, these filters have the side-effect of selectively smoothing your image and removing edge information. To reduce noise while also preserving edges, you can try using MimFilterAdaptive(); for more information, see the Adaptive filters subsection of this section. You can apply low-pass spatial linear filters using MimConvolve(). MIL provides a predefined low-pass linear filter called M_SMOOTH, which satisfies most applications (other predefined filters are also available). If you require more control over the filtering process, you can use MimConvolve() with a custom filter. For more information, see the Custom spatial filters section of Chapter 4: Advanced image processing. Rank filters Rank-filter operations are more suitable for removing salt-and-pepper type noise since they replace each pixel with a pixel in its neighborhood rather than a weighted sum of its neighborhood. The weighted sum generally creates a blotchy effect around each noise pixel. You can perform a rank-filter operation using MimRank(). In most cases, it is best to use a rank that is half of the number of elements in the neighborhood. This effectively replaces each pixel with the median of the neighborhood and is therefore called a median filter. To perform a median filter, use MimRank() with M_MEDIAN. You will find that the median filter will most often suit your application needs. For a rank filter operation that adapts to the content of the source image, while also preserving edge information, try using MimFilterAdaptive() with M_NOISE_PEAK_REMOVAL. Adaptive filters To remove noise while preserving edge information, use MimFilterAdaptive() with one of its adaptive filter operations. These operations adapt to the content of the source image. You can specify a bilateral filter (M_BILATERAL) or a noise peak elimination filter (M_NOISE_PEAK_REMOVAL). Both types of adaptive filters smooth an image while preserving edges as much as possible. M_BILATERAL is a nonlinear smoothing filter; M_NOISE_PEAK_REMOVAL is similar to a rank filter. Both filters adapt to the content of the source image, while allowing some attributes, such as kernel size or number of iterations, to be explicitly set for the filter operation. Note that adaptive filters are slower than other similar operations, such as rank filters (MimRank()) and most of the filters available using MimConvolve(). If edge preservation is less important than speed for your application, try the predefined or custom filters available in MimConvolve(). You can use MimFilterMajority() to replace pixels with the pixel value that appears most often in its neighborhood. Majority filter operations are more suitable for removing noise from images with large uniform areas because such images will likely have more than one pixel of the same value in each neighborhood. This is also useful, for example, to filter an index image (such as the segmentation map used in the MIL Classification module). Filtering using an average does not make sense in this case. For example, if index 1 represents a dog, index 2 represents a monkey, and index 3 represents a cat, using an averaging filter would not produce the intended results. An averaging filter might change a pixel to index 2 (monkey) in a region with only indices 1 (dog) and 3 (cat), whereas MimFilterMajority() would replace pixels with the most common pixel value in its neighborhood. Note that MimFilterMajority() does not preserve edges. The following example shows M_BILATERAL and M_NOISE_PEAK_REMOVAL adaptive filtering on an image, and also shows a Deriche smoothing filter result (using MimConvolve()) for comparison. adaptivefiltering.cpp To run this example, use the Matrox Example Launcher in the MIL Control Center. Area open and area close To eliminate salt-and-pepper noise in your image, you can also perform an area open or area close operation, using MimMorphic() with M_AREA_CLOSE or M_AREA_OPEN. Area open can be used to eliminate small regions of light noise pixels, while area close can do the same to small regions of dark noise pixels. To understand what the area open and area close operations do, it is useful to think of an image as a topographic surface with hills and valleys. The value of each pixel represents a certain height, with the lowest pixel value (the darkest pixel) representing the point of lowest elevation and the highest pixel value (the brightest pixel) representing the point of highest elevation. The area open operation clips the peaks of hills until each has a plateau with an area equal to or greater than the specified minimum area. If an area of this size never occurs, the peak will be clipped until all the pixels in the hill have the same intensity as the background pixels. The area close operation is similar to the area open operation, except the clipping of intensity levels begins at the lowest level. In this case, you can imagine the bottom of the basins being filled until they have a flat surface area equal to or greater than the specified minimum area. To implement the area open operation, MIL analyzes the image and clips off peaks one intensity level at a time, starting at the highest pixel intensity level (255 for an 8-bit image). It locks pixels at their current value if the specified minimum area is reached. At each intensity level, the following steps are performed: Pixels with an intensity level greater than the current intensity level and that have not been previously locked are set to the current intensity level. The algorithm then considers all pixels with a value equal to or greater than the current value to be foreground pixels, and connected foreground pixels as part of the same object. The area of each object is compared to the specified minimum area. If it is greater or equal to the specified minimum area, the pixels in the object are locked at their current value for the remainder of the algorithm. The first pixels locked in any area form a plateau. If the current intensity level is 0, the operation is complete. If not, the algorithm proceeds to the next lower intensity level, and returns to step 1. In the event that the area of an object at every pixel intensity level never equals or exceeds the specified minimum area, its pixels will eventually be clipped to the background pixel intensity level. Also, it is possible that distinct objects at one pixel intensity level merge together at a lower pixel intensity level. This situation is illustrated in the following images, which depict a few steps in the area open operation. As was mentioned earlier, an object is composed of connected foreground pixels. Determining which pixels are connected depends on the selected structuring element, M_3X3_RECT or M_3X3_CROSS. These are the only two structuring elements that can be used with the area open and area close operations. Denoise using spatial filtering and area open and close operations Low-pass spatial linear filters Rank filters Adaptive filters Area open and area close ",
      "wordCount": 1341,
      "subEntries": []
    },
    {
      "id": "UG_image_Erosion_and_dilation",
      "version": null,
      "title": "Erosion and dilation",
      "subTitles": [
        "Erosion",
        "Dilation",
        "An example"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\image\\Erosion_and_dilation.htm",
      "text": " Erosion and dilation Especially during cell analysis, it can be important to know the growth stages of cell particles. Using the image processing erosion and dilation operations, you can view the possible growth stages of these particles. Erosion operations peel off layers from objects or particles, removing extraneous pixels and small particles from the image. Dilation operations add layers to objects or particles, enlarging any particle. Dilation can return eroded particles to their original size (but not necessarily to their exact original shape). Erosion and dilation are neighborhood operations that determine each pixel's value according to its geometric relationship with neighborhood pixels, and as such, are part of a group of operations known as morphological operations. This section describes basic erosion and dilation performed using MimErode() and MimDilate(). Both these functions use predefined kernels to perform operations. For a description of advanced erosion and dilation, performed by MimMorphic(), see the Custom morphological operations section of Chapter 4: Advanced image processing. They are also the basic operations used to perform the opening and closing operations discussed in the following section. Note, zero pixels are considered background, while non-zero pixels are considered foreground and part of objects (more generally referred to as blobs). Erosion You can perform a basic erosion operation on 3 by 3 neighborhoods, using MimErode() with one of the following two options: If the erosion mode is set to M_BINARY, any pixel whose neighborhood is not completely white (any non-zero pixel is considered white) is changed to black (0 is considered black). If the erosion mode is set to M_GRAYSCALE, each pixel is replaced with the minimum value in its neighborhood. You can use the iteration parameter of MimErode() to perform an erosion on larger neighborhoods. Iterating the erosion is the equivalent to performing an erosion on a (1 + (2* i)) by (1 + (2* i)) neighborhood where i is the number of iterations. For example, two iterations of a 3x3 erosion is equivalent to a 5x5 erosion, and three iterations is equivalent to a 7x7 erosion. If you need to iterate a binary erosion on each blob until it is about to disappear, set the erosion mode to M_BINARY_ULTIMATE or M_BINARY_ULTIMATE_ACCUMULATE instead of M_BINARY, and set the iteration parameter to M_DEFAULT. M_BINARY_ULTIMATE_ACCUMULATE also keeps track of the number of iterations that occurred for each pixel that is not eroded. The following animation illustrates an example of MimErode() with M_BINARY_ULTIMATE (on the left) and M_BINARY_ULTIMATE_ACCUMULATE (on the right). Dilation You can perform a basic dilation operation on 3 by 3 neighborhoods, using MimDilate() with one of the following two options: If the dilation mode is set to M_BINARY, any pixel that has one or more white pixels (any non-zero pixel is considered white) in its neighborhood is set to white (0xff in an 8-bit image). If the dilation mode is set to M_GRAYSCALE, each pixel is replaced with the maximum value in its neighborhood. The MimDilate() function is similar to the MimErode() function in that iterating it will effectively cause a dilation on larger neighborhoods. Iterating the dilation is the equivalent to performing a dilation on a (1 + (2* i)) by (1 + (2* i)) neighborhood where i is the number of iterations. For example, two iterations of a 3x3 dilation is equivalent to a 5x5 dilation, and three iterations is equivalent to a 7x7 dilation. The following animation demonstrates the use of MimDilate() with a fixed number of iterations. The characters in the following animation are composed of dots. After four iterations, the characters are made up of solid lines and can be identified more easily. If you need to iterate a binary dilation on each blob until its holes and background are about to disappear, set the dilation mode to M_BINARY_ULTIMATE instead of M_BINARY, and set the iteration parameter to M_DEFAULT. If you need to keep track of the number of iterations that occurred for each remaining background or hole pixel during dilation, set the dilation mode to M_BINARY_ULTIMATE_ACCUMULATE instead. This will take the negative of the image (white pixels will become black and black pixels will become white) and then performs an erosion on the new blobs (on the new white pixels). The following animation illustrates an example of MimDilate() with M_BINARY_ULTIMATE (on the left) and M_BINARY_ULTIMATE_ACCUMULATE (on the right). An example You can use erosion or dilation to find the perimeter of objects. Erode or dilate a binary image and `XOR' the result with the original image, using MimArith(). The following example shows how to obtain the exoskeletons of objects in an image. simpledilateerode.cpp Erosion and dilation Erosion Dilation An example ",
      "wordCount": 770,
      "subEntries": []
    },
    {
      "id": "UG_image_Opening_and_closing",
      "version": null,
      "title": "Opening and closing",
      "subTitles": null,
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\image\\Opening_and_closing.htm",
      "text": " Opening and closing Another way of improving the image might be to remove, for example, small particles that have been introduced by dust, or holes in objects. These tasks can generally be accomplished with an opening or closing operation, respectively. Opening and closing operations determine each pixel's value according to its geometric relationship with neighborhood pixels, and as such are part of a larger group of operations known as morphological operations. Besides removing small particles, opening operations also break isthmuses or connections between touching objects. MIL provides the MimOpen() function to perform a basic opening operation on 3 by 3 neighborhoods taking all neighborhood pixels into account. Closing operations are very useful in filling holes in objects; however in doing so, they also connect close objects, as shown below. MimClose() performs a standard 3 by 3 closing operation taking all neighborhood pixels into account. Note, opening and closing operations work best on binary images. Since opening is the result of eroding and then dilating an image, and closing is the result of dilating and then eroding an image, you can also customize an opening or closing operation, using MimErode() and MimDilate(). For more information on erosion and dilation, see the Custom morphological operations section of Chapter 4: Advanced image processing. Opening and closing ",
      "wordCount": 215,
      "subEntries": []
    },
    {
      "id": "UG_image_Image_statistics",
      "version": null,
      "title": "Image statistics",
      "subTitles": [
        "Generating a histogram",
        "Calculating general statistics",
        "Steps to calculate general statistics",
        "Considering pixel values as unit vectors and calculating angular statistics",
        "Coherence",
        "Dominant angle",
        "Dominant orientation",
        "Calculating statistics across multiple images",
        "Finding the image extremes",
        "Locating events",
        "Counting image differences",
        "Projecting an image to one dimension",
        "Calculating the integral of an image"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\image\\Image_statistics.htm",
      "text": " Image statistics Many applications need to obtain some type of image statistic to condense a frame of pixels into a smaller, more functional set of values for analysis. The statistic might be required to perform some subsequent operation and/or might be used to summarize the effect of some image operation. The MIL Image Processing module offers a variety of functions to extract statistical information from an image. These functions allow you, for example, to: Generate the intensity histogram of an image buffer Generate the intensity histogram of an image buffer (MimHistogram()). Calculate a variety of general statistics for all pixels in an image that satisfy a specified condition (MimStatCalculate()). Calculate and accumulate a variety of general statistics for all pixels in a series of specified images (MimStatCalculate()). Find the minimum and maximum values of an image buffer (MimFindExtreme()). Find the location of certain pixel values (MimLocateEvent()). Find the number of differences between two image buffers (MimCountDifference()). Perform an image projection from two dimensions to one dimension (MimProjection()). Calculate the integral of an image (MimArith()). Note that, with the exception of MimHistogram(), these functions are not available with MIL-Lite. Generating a histogram A histogram is the intensity distribution of pixel values in an image and is generated by counting the number of times each pixel intensity occurs. Histograms store this information in bins, each of which represents an intensity. For example, if a histogram has 100 bins, it can represent 100 intensities. Information about the intensity distribution of pixel values in an image is useful for several applications. In particular, it is useful to help select a threshold level when binarizing an image (discussed later) and to change the image intensity distribution when trying to increase the image contrast. You can generate an image histogram using MimHistogram(), which takes an image buffer and stores the results in a previously allocated histogram result buffer. To allocate the result buffer, use MimAllocResult() with M_HIST_LIST. Ensure that you specify enough histogram bins to store the required number of intensities in the result, using the NbEntries parameter. For example, if you are using an 8-bit unsigned source image, and you want to have one bin for every possible intensity value, you should specify 256 entries. To read results, use MimGetResult(). Once results have been read from the result structure, you can then release it, using MimFree(). If you require further control over the generated histogram, you can specify the number of values that a histogram bin can hold, using MimControl() with M_HIST_BIN_SIZE_MODE, before calling MimHistogram(). You can also apply a smoothing factor to the histogram with the M_HIST_SMOOTHING_ITERATIONS control. Since the smoothing is an integer-based averaging of the histogram, the resulting number of values might be different than the number of values in the source image. To retrieve this type of information, use MimGetResult() with M_HIST_REAL_SIZE, M_HIST_VALUE_OFFSET, and M_HIST_VALUE_RANGE. The following example shows how to load an image, and calculate and draw its intensity histogram. mimhistogram.cpp You can use the MIL graphics functions to plot the histogram results on a graph, as shown below. The graphics functions (Mgra...()) are discussed later in this manual. Calculating general statistics You can calculate various statistics for all pixels that satisfy a specified condition in an image, using MimStatCalculate(). Setting the condition limits the number of pixels for which the statistics will be calculated. However, the condition can also be set to use all of the pixels in the image. You can limit some of MimStatCalculate() 's results to a region of an image buffer using a region of interest (ROI) set using MbufSetRegion(). The ROI must be defined in raster format (M_RASTER or M_VECTOR_AND_RASTER). An error is generated if the ROI is only in vector format (M_VECTOR). Note that not all statistics operations support ROIs; for a list of those that do, see MimControl(). The types of statistics that can be calculated include the following: Maximum pixel value. Maximum absolute pixel value. Mean pixel value. Minimum pixel value. Minimum absolute pixel value. Number of pixels (that satisfy the condition). Standard deviation value. Sum of pixel values. Sum of absolute pixel values. Sum of squared pixel values. Steps to calculate general statistics The following steps provide a basic methodology for calculating general statistics: Optionally, allocate a statistics context, using MimAlloc() with M_STATISTICS_CONTEXT. Note that if you want to calculate a single statistic, you can call MimStatCalculate() with a predefined statistics context, without having to allocate a statistics context beforehand. Allocate a statistics result buffer, using MimAllocResult() with M_STATISTICS_RESULT. If you have allocated a statistics context, control its settings using MimControl(). These settings determine which statistics to calculate. You cannot control the settings of a predefined statistics context. Call MimStatCalculate() to calculate the statistics. Retrieve results from the statistics result buffer using MimGetResult(). Alternatively, you can use MimProjection() to calculate the maximum, minimum, mean, and median pixel value for each row or column of pixels in the image. This is typically more efficient than using MimStatCalculate() when you need to calculate these statistics for regions of an image that are rows or columns of pixels, and calculate them for all rows or columns in that region. When you need to calculate statistics for regions of an image that are not rows or columns, or individual rows or columns, use MimStatCalculate(). For more information on using MimProjection() to calculate the maximum, minimum, mean, and median pixel value for each row or column of pixels in the image, see the Projecting an image to one dimension subsection of this section. Considering pixel values as unit vectors and calculating angular statistics If pixel values in your image represent angles instead of intensities, you can also choose to have MIL treat the pixels as unit vectors, where the angular value of a pixel is considered to be the angle of a vector with a length of one. Treating the pixels as unit vectors allows MIL to calculate directional statistics, which take into account the wrap-around characteristic of angles (so that, for example, the mean of 0 and 360 is either 0 or 360, not 180). This is useful, for example, when calculating color statistics of HSL images because the hue of an HSL image is measured on an angular scale, as opposed to a linear scale. The angles are considered to have been scaled to the type of the image buffer. For unsigned buffers, the minimum pixel value is considered as an angle of 0, and one plus the maximum possible pixel value is considered an angle of 360. For example, an 8-bit buffer would have 0° mapped to a value of 0, while 360° would be mapped to a value of 256 (since the maximum pixel value is 255). For signed buffers, the minimum pixel value is considered as an angle of -180° while one plus the maximum pixel value is considered as an angle of 180°. Floating point buffers have intensity values anywhere between 0 and 1, and these values are linearly mapped between 0° and 360°. Using MimStatCalculate(), there are three main statistics that can be calculated when pixel values that satisfy a specified condition are considered as unit vectors: The coherence of the unit vectors. The dominant angle of the unit vectors. The dominant orientation of the unit vectors. Coherence The coherence represents the directional trend of all the unit vectors in the image. It is a measure of how coherent or parallel the unit vectors are with respect to each other. To quantify this idea, MimControl() with M_STAT_ANGULAR_DATA_COHERENCE enables a vector addition on all the unit vectors. This is performed when MimStatCalculate() is called. The vector that results from this addition is known as the vector sum, and the length component of this vector sum is known as the norm. The function then calculates the ratio between the norm, or length, of the vector sum and its maximum possible length (which corresponds to the number of pixels considered, since each pixel is presumed to have a length of one). Numerically, this quantity lies between 0 and 1, where 0 represents absolutely no coherence (unit vectors pointing in random directions) and 1 is total coherence (unit vectors pointing in same direction). Below are images depicting three different coherences: perfect coherence, high coherence, and low coherence, respectively. In the following image, the coherence of 4 unit vectors with the same angle is calculated. Since the ratio between the norm and the maximum possible length is equal to 1, there is total coherence between individual unit vectors, confirming all unit vectors are pointing in the same direction. In the following image, the coherence of 4 unit vectors with differing angles is calculated. Since the ratio between the norm and the maximum possible length is equal to 0.905, there is high (but not total) coherence between individual unit vectors. Therefore, it can be concluded that the unit vectors are pointing in similar directions. In the following image, the coherence of 4 unit vectors with differing angles is calculated. Since the ratio between the norm and the maximum possible length is equal to 0.105, there is very low coherence between the individual unit vectors. Therefore, it can be concluded that the unit vectors are pointing in seemingly opposite or random directions. Dominant angle In addition to calculating the coherence (M_STAT_ANGULAR_DATA_COHERENCE), by treating the pixels as unit vectors, you can also calculate the dominant angle in the data. To do so, use MimControl() with M_STAT_ANGULAR_DATA_MEAN before calling MimStatCalculate(); this considers the angular component of the vector sum as the average angle. When the average angle is analyzed in context with the coherence of the unit vectors, it is much more representative of the average angle. For example, if your image has a coherence of 0.1 and an average angle of 65°, the average angle is not very meaningful because the unit vectors are pointing in random, incoherent directions. However, if your image has a coherence of 0.9 and an average angle of 65°, this indicates that the unit vectors are more or less pointing in the same direction, and their average value is around 65°. Dominant orientation If you are only interested in the dominant orientation of the unit vectors, you should enable MimControl() with M_STAT_ORIENTATION_DATA_MEAN before calling MimStatCalculate(); this only takes into consideration the inclination of the unit vectors, without their direction, returning their dominant orientation within the image (for example, are they horizontal, vertical, diagonal). For unit vectors that are antipodal to each other (for example, 45° and 225°), the resulting orientation angle will be one of the original angles (for the previous example, 45°). Whereas for unit vectors that have an angular distance of 90° (for example, 0° and 90°), the vectors will cancel each other out. The orientation operation does not typically deal with only 2 unit vectors, but with all unit vectors within an image; as such, it is very unlikely that all the resulting unit vectors' orientations will cancel each other out. Furthermore, because the operation deals with the average orientation of all unit vectors in an image, the approximate, dominant orientation of the image is returned. For a more accurate orientation operation, use the MimFindOrientation() function. The following image illustrates the difference between taking the dominant angle (M_STAT_ANGULAR_DATA_MEAN) and the dominant orientation (M_STAT_ORIENTATION_DATA_MEAN). The calculation of M_STAT_ANGULAR_DATA_MEAN results in a dominant angle of 45 o . Whereas, the calculation of M_STAT_ORIENTATION_DATA_MEAN results in a dominant orientation of 90 o . Calculating statistics across multiple images You can calculate statistics for each pixel location across multiple source images, passed upon iterative calls to MimStatCalculate(). One value is calculated for every pixel in the source image, for each statistical operation specified to be performed. This function stores results in a result buffer that should have been previously allocated using MimAllocResult() with M_STATISTICS_RESULT. The same result buffer should be specified for each call to MimStatCalculate(). Specify the statistics to perform using a cumulative statistics image processing context, allocated using MimAlloc() with M_STATISTICS_CUMULATIVE_CONTEXT. Enable the type of statistics to calculate using MimControl() with one or more of the statistics (for example, set M_STAT_MEAN to M_ENABLE). MIL can calculate one or more of the following types of statistics for each pixel position in the sequence of images: Maximum pixel value at the position. Maximum absolute pixel value at the position. Mean pixel value at the position. Minimum pixel value at the position. Minimum absolute pixel value at the position. Number of pixel values, at the position, that match the given condition. Standard deviation value at the position. Sum of pixel values at the position. Sum of absolute pixel values at the position. Sum of squared pixel values at the position. When you are ready to calculate the statistics for the source images, you must preprocess the context and result buffer. The preprocessing stage uses the specified source image and result buffer (if specified) to determine how to optimally calculate the specified statistics. If the preprocessing operation was not done explicitly (using M_PREPROCESS) it will be done when MimStatCalculate() is first called. Note that, if you do not provide a source image for the preprocessing operation, you must set M_SOURCE_SIZE_X and M_SOURCE_SIZE_Y to respective width and height of a typical image. Finding the image extremes You can find the minimum and maximum pixel values of your image with MimFindExtreme(). Perhaps the most common use for finding the minimum and maximum image pixel values is to fine-tune the black and white reference levels of your frame grabber, ensuring full-range digitization. Another use for finding the maximum image pixel value is to find the number of objects in a labeled image. If all objects in an image are labeled with unique consecutive values, using MimLabel() (discussed later in this chapter), the largest label value also corresponds to the number of objects in your image. MimFindExtreme() stores results in a result buffer that should have been previously allocated, using MimAllocResult() with M_EXTREME_LIST. You can get the resulting values, using MimGetResult(), and free the result buffer, using MimFree(). Locating events Once you have established certain values of interest in an image, you can find the location of pixels that satisfy conditions based on these values, using MimLocateEvent(). For example, you can use MimLocateEvent() to find the location of all pixels in an image equal to the image's maximum pixel value. Another use-case for MimLocateEvent() is to find the local minima or maxima in an image which meet the specified condition. To do so, add the respective M_LOCAL_... combination value when specifying the condition under which pixel values are considered an event. For example, you can use the M_LOCAL_MAX_NOT_STRICT or M_LOCAL_MAX_STRICT_MEDIUM combination value to find the local maxima in an image. A pixel is a local maximum if its value is greater than or equal to its surrounding pixels. Some combination values have stricter conditions for a pixel to be considered a local maximum. The following 3x3 neighborhood images illustrate the conditions under which a pixel is considered a local maximum for the respective combination value. The image below shows two buffers, each with a multi-pixel peak. The not-strict mode returns the positions of all pixels in the peak, whereas the strict mode typically returns a single pixel position for a narrow peak but can return multiple pixel positions for wider peaks. Note that M_LOCAL_MAX_NOT_STRICT and M_LOCAL_MAX_STRICT_MEDIUM are intended for use when peaks are narrow. Otherwise, multiple strict peaks are possible. Similarly, you can use the M_LOCAL_MIN_NOT_STRICT and M_LOCAL_MIN_STRICT_MEDIUM combination values to find the local minima in an image. Note that MimLocateEvent() stores results in a result buffer that should have been previously allocated, using MimAllocResult() with M_EVENT_LIST. Note, for ease of use, in cases when you only need to know if an event is present and not the location, you can use MimDetectEvent(). Counting image differences You can find the number of differences between two images, using MimCountDifference(). MimCountDifference() stores results in a result buffer that should have been previously allocated, using MimAllocResult() with M_COUNT_LIST. You can get the resulting values, using MimGetResult(), and free the result buffer, using MimFree(). Projecting an image to one dimension The MimProjection() function projects an image onto one dimension, based on a specified projection axis angle and operation. The projection operates on all pixel values along each lane of pixels in the image, perpendicular to the specified axis angle (for example, a 90° angle projects each row onto the Y-axis). You can use MimProjection() to calculate the maximum, minimum, mean, or median pixel value for each lane of pixels in the image. Although you can calculate the maximum and minimum pixel values for a lane using MimStatCalculate(), it is more efficient to use MimProjection() when you need to get these values for all lanes in an image. In this case, use MimProjection() with M_MAX, M_MIN, M_MEAN, or M_MEDIAN. You can also use MimProjection() to find the pixel value with the specified rank for each lane of pixels in the image. The pixels in a lane are ranked in ascending order based on pixel value. You can specify the rank explicitly using MimProjection() with M_RANK. Alternatively, you can specify the rank as a percentage of the total lane length using MimProjection() with M_RANK_PERCENTILE (for example, if you specify 0.5 with M_RANK_PERCENTILE, the median pixel value is selected in each lane). Another operation that MimProjection() supports is summation (M_SUM). In this case, it sums all pixel values in each lane. This projection is referred to as the pixel value density of each lane of pixels. The 90° projection of the image is known as the row profile, and the 0° projection is known as the column profile. The MimProjection() function can perform both grayscale and binary image summation projections. On simple binary images, the projection is useful to detect object locations. The following image demonstrates the row and column profiles for a binary image, as calculated using MimProjection() with M_SUM. You can use MimProjection() with a destination image buffer, or you can use a result buffer allocated using MimAllocResult() with M_PROJ_LIST. Writing results to a projection image processing result buffer allows you to manipulate results using an array once you have retrieved the results using MimGetResult() or MimGetResult1d(). You should define a result buffer with as many locations as there are lanes of pixels in the image, perpendicular to the specified axis angle. When using a region of interest in the source image of your projection, lanes of pixels that fall outside the specified region of interest will be marked as invalid in the result buffer. You can retrieve the validity of projection data for each lane using MimGetResult() or MimGetResult1d() with M_VALID. Note that for an M_SUM projection operation, if a lane of pixels does not contain valid projection data, a neutral pixel value of 0 is used for that lane. In this case, M_VALID will indicate valid projection data for all lanes of pixels in the image. Calculating the integral of an image You can calculate the integral of an image, using MimArith(). Each pixel of the integral image contains the cumulative sum of the source image pixels until that location. Calculating the integral of the image allows you to analyze regions of an image more efficiently. For more information on calculating the integral of an image, see Computing the integral of an image. Image statistics Generating a histogram Calculating general statistics Steps to calculate general statistics Considering pixel values as unit vectors and calculating angular statistics Coherence Dominant angle Dominant orientation Calculating statistics across multiple images Finding the image extremes Locating events Counting image differences Projecting an image to one dimension Calculating the integral of an image ",
      "wordCount": 3268,
      "subEntries": []
    },
    {
      "id": "UG_image_Adjusting_the_intensity_distribution",
      "version": null,
      "title": "Adjusting the intensity distribution",
      "subTitles": [
        "Window leveling",
        "Window leveling using MimRemap()",
        "Window leveling using MimLutMap()",
        "Window leveling using MimMorphic()",
        "Histogram equalization",
        "Adaptive histogram equalization"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\image\\Adjusting_the_intensity_distribution.htm",
      "text": " Adjusting the intensity distribution There are two ways to adjust the intensity distribution of an image: Window leveling. Histogram equalization mapping. Window leveling Window leveling takes a range of pixel values in the source image and linearly maps these values to a different range in the destination image. By stretching the range of values actually present in an image to the full range available, window leveling allows you to use all available intensities. Window leveling is especially useful when examining 10- or 12-bit images that have been grabbed into 16-bit images so that the image can occupy all of the available intensity range. When using MIL, you will sometimes be limited to using 8-bit image buffers. If your images have a greater bit depth, you can use window leveling to map your images to an 8-bit range. For example, to convert a 10-bit image into an 8-bit image, you can set the destination to an 8-bit image buffer and set the output range of the window leveling as 0 to 255. Window leveling using MimRemap() The function MimRemap() performs a point-to-point linear remapping of pixel values, from the source range to the destination range, according to the specified mode. For the source range, you can specify to use the minimum and maximum values supported by the source image buffer (M_FIT_SRC_RANGE), or you can have MIL find the pixel value extremes of the source image (M_FIT_SRC_DATA). In either case, source values are mapped to the destination buffer's specified range of possible values. To control the pixel value range of the source or destination buffer, use MbufControl() with M_MIN and M_MAX. When you use MimRemap() with M_FIT_SRC_RANGE, and the source buffer has data below the specified M_MIN or above the specified M_MAX, this data will be mapped to unpredictable values. If the goal is to clip this data to the M_MIN and M_MAX values of the destination buffer, use MimClip() with M_OUT_RANGE on the source buffer first to set the data outside the range to the specified source M_MIN and M_MAX values. You can choose to keep the destination range centered to zero (M_CENTERED). In the centered case, the values are remapped such that the value zero in the source buffer maps to the value zero in the destination buffer. You can use MimRemap() to compress or expand an image to a different bit depth, since the bit depth can differ between the specified source and destination image buffers. For example, you can use MimRemap() with M_FIT_SRC_RANGE to convert down to an 8-bit depth for display purposes or to be able to use analysis modules that only accept 8-bit input. The table below shows the formulas used by MimRemap(), for both the centered and non-centered cases. Note that, in the centered case, some exceptions to the multiplier calculation apply. If both SrcMax and DstMax are less than zero, the multiplier is DstMin/SrcMin . If both SrcMin and DstMin are greater than or equal to zero, the multiplier is DstMax/SrcMax . MimRemap() is not available with MIL-Lite. Window leveling using MimLutMap() You can use a LUT when performing window leveling. In this case, use MimLutMap() after generating a LUT and associated ramp. You can choose specific intensity ranges within both the source and destination images. The image below illustrates the effects of window leveling using MimLutMap(). To implement window leveling using MimLutMap(), you must first allocate a LUT buffer using MbufAlloc1d() with a size equivalent to the maximum possible value in the source image. Generate a ramp for the LUT using MgenLutRamp(), specifying the StartValue and EndValue as the required range of the destination image buffer. Finally, map the source image buffer through the LUT using MimLutMap(). You can use window leveling if you want to improve the contrast in your images and highlight distinct areas. Stretching a particular range of values in an image will make some features of the image closely resemble other features or the background, which can be useful for analysis. You can also stretch a portion of an image's intensity range to see certain details (for example, in medical x-ray applications, various tissue types occupy different value ranges). The following image illustrates how you can highlight different features using window leveling: The following example demonstrates how to perform window leveling using a LUT. /* Calculate the minimum and maximum pixel values of the source image. */ MIL_ID MilStatContext = MimAlloc(MilSystem, M_STATISTICS_CONTEXT, M_DEFAULT, M_NULL); MIL_ID MilStatResult = MimAllocResult(MilSystem, M_DEFAULT, M_STATISTICS_RESULT, M_NULL); MimControl(MilStatContext, M_STAT_MAX, M_ENABLE); MimControl(MilStatContext, M_STAT_MIN, M_ENABLE); MimStatCalculate(MilStatContext, MilOriginalImage, MilStatResult, M_DEFAULT); MimGetResult(MilStatResult, M_STAT_MAX + M_TYPE_MIL_INT, &amp;ImageMaxValue); MimGetResult(MilStatResult, M_STAT_MIN + M_TYPE_MIL_INT, &amp;ImageMinValue); //. . . /* Allocate a LUT buffer */ MbufAlloc1d(MilSystem, 255 + 1, 8 + M_UNSIGNED, M_LUT, &amp;MilLut); /* Generate a LUT with a full range ramp. */ MgenLutRamp(MilLut, ImageMinValue, 0.0, ImageMaxValue, 255.0); MimLutMap(MilOriginalImage, MilResultImage, MilLut); //. . . For a more interactive example, see the following example. mdispwindowleveling.cpp Window leveling using MimMorphic() To perform window leveling on a pixel neighborhood basis, rather than point-to-point, use MimMorphic() with M_LEVEL. This operation finds the minimum and maximum pixel values in a neighborhood, and then creates a linear mapping such that these map to the minimum and maximum possible values of the image buffer. It then replaces the center pixel of the neighborhood with its corresponding value in the mapping. Similar to using MimRemap() with M_FIT_SRC_DATA, the MimMorphic() operation is like an adaptive window leveling, since only local pixel extremes are calculated. MimMorphic() is not available with MIL-Lite. Histogram equalization A histogram equalization can be performed to obtain a more uniform distribution of the grayscale values in your image. For example, if the intensity distribution of an image results in a clump in one area of the grayscale range, there might be objects that are not easily distinguished because of their similarity in color. You might want to adjust the image's intensity distribution to solve this problem by giving it a more uniform (M_UNIFORM) distribution, using MimHistogramEqualize(). The MimHistogramEqualize() function first generates a histogram of the source image buffer. The histogram and a selected density function are then used to calculate a transformation LUT. If the destination buffer is an image, the transformation LUT is applied to the source buffer to produce the destination image. If the destination buffer is an LUT, the transformation LUT is copied into the destination LUT; this LUT can then be used to enhance the source image, either permanently (using MimLutMap()) or upon display (using MdispLut()). The transformation LUT can also be applied directly to images as they are being grabbed; to do so, first associate the LUT buffer with the digitizer, using MdigControl() with M_LUT_ID and then grab the image. Adaptive histogram equalization For a more refined histogram equalization, use MimHistogramEqualizeAdaptive(), which enhances an image using a contrast limited adaptive histogram equalization (CLAHE). In this case, you must allocate an adaptive histogram equalization context, using MimAlloc() with M_HISTOGRAM_EQUALIZE_ADAPTIVE_CONTEXT. Adaptive histogram equalization is not available with MIL-Lite. Unlike a conventional histogram equalization (MimHistogramEqualize()), which generates a histogram for the entire image as a whole, MimHistogramEqualizeAdaptive() partitions the specified image into a set of rectangular sections of equal size, referred to as tiles, and calculates histograms for each one. The multiple histograms, along with the specified equalization operation, are then used to transform the image and produce an enhanced version of it. Though this can take slightly longer than a conventional histogram equalization, it can help minimize the over-amplification of noise and the potential for saturation, particularly in images with inconsistent lighting where some sections are significantly brighter than others. By default, MimHistogramEqualizeAdaptive() uses 8 tiles along the image's X- and Y-direction. The specified image is therefore processed as 64 congruent tiles. To modify this behavior, use MimControl() with M_NUMBER_OF_TILES_X and M_NUMBER_OF_TILES_Y. Every tile of the image has its own histogram, and every histogram has a set of bins, each indicating the number of pixels with the same intensity. The maximum percentage of pixels that a bin can represent is limited, by default, to 1% of all the pixels in that tile. To modify the maximum percentage, use the M_CLIP_LIMIT control. This essentially limits the contrast. Exceeding values are distributed evenly among the tile's other histogram bins. You can modify the number of histogram bins for each tile, as well as the equalization operation to perform, using the M_HIST_SIZE and M_OPERATION control types, respectively. By default, MimHistogramEqualizeAdaptive() determines the number of histogram bins automatically according to the specified source image, and performs a uniform-type of equalization. Theoretically, the equalization can process each pixel and its neighborhood for every tile. To avoid this lengthy calculation, MimHistogramEqualizeAdaptive() actually performs a type of interpolation between sample pixel regions of adjacent tiles to determine pixel value results, which significantly improves efficiency, while only negligibly diminishing the quality of the transformed image. For more information about this, and other specifics regarding CLAHE, refer to \"S. M. Pizer, E. P. Amburn, J. D. Austin, et al. Computer Vision, Graphics, and Image Processing: Adaptive Histogram Equalization and Its Variations . U.S.A.: Elsevier, 1986. 355­-368.\". The following example demonstrates how to perform an adaptive histogram equalization: histogramequalizeadaptive.cpp Adjusting the intensity distribution Window leveling Window leveling using MimRemap() Window leveling using MimLutMap() Window leveling using MimMorphic() Histogram equalization Adaptive histogram equalization ",
      "wordCount": 1538,
      "subEntries": []
    },
    {
      "id": "UG_image_Thresholding_your_images",
      "version": null,
      "title": "Thresholding your images",
      "subTitles": [
        "Binarizing",
        "Determining the threshold value from the histogram",
        "Specifying the threshold value(s) manually",
        "Clipping"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\image\\Thresholding_your_images.htm",
      "text": " Thresholding your images Thresholding images means reducing each pixel to a certain range of values. Some operations can be performed more efficiently on thresholded images. Images with full grayscale levels are useful for some tasks, but have redundant information for others. The MIL package provides the following types of threshold operations: Binarizing, using MimBinarize(). Adaptive binarizing, using MimBinarizeAdaptive(). Clipping, using MimClip(). Binarizing A binarizing operation reduces an image to two grayscale values by comparing each pixel value in the image against one or two specified threshold values (for example, whether each pixel value is above a threshold value, or within the range of two threshold values). Pixels that meet the specified condition are typically set to the maximum possible value of the image (for example, 255 if the image is 8-bit or 1 if the image is a floating-point buffer), while other pixels are set to 0. Binary images are useful when trying to identify geometric patterns and objects in your image since they are not cluttered with shading information. The following animation illustrates the difference between the grayscale and binary version of an image. When using MimBinarize(), it is important to select threshold values that preserves the required information. For example, in the previous image, an incorrect threshold value might inappropriately change pixels of particles into background pixels, resulting in fewer particles than the number that actually exist. If you are unable to properly preserve the required information when binarizing your images with MimBinarize(), try an adaptive binarization with MimBinarizeAdaptive(). Though typically longer to execute, an adaptive binarization is more robust than a conventional binarization, particularly when dealing with poor or varying illumination or contrast. For more information, see the Adaptive binarizing section later in this chapter. Determining the threshold value from the histogram If the binarizing condition uses only one threshold value, MimBinarize() can automatically determine the threshold value from the source image's characteristics. It can establish the threshold using one of the following threshold modes: Bimodal. Dominant. Triangular bisection. In bimodal threshold mode (using M_BIMODAL), a histogram of the source image is internally generated. Then, if the histogram contains only two peaks, the threshold value is set to the intensity value at the lowest point between these two peaks, on the assumption that these peaks represent the object and the background. If the histogram contains more than two peaks, the threshold value will typically be set between the intensity values at the two principal peaks, although exceptions exist for the situations where the principal peaks occur closest to the minimum intensity value (pure black) or the maximum intensity value (full saturation). Note that the bimodal threshold mode works best when the histogram contains two visible peaks. In dominant threshold mode (using M_DOMINANT), a histogram of the source image is internally generated. If the histogram contains only two peaks, the threshold is set to the intensity value at the lowest point between these two peaks. If the histogram contains more than two peaks, the threshold value is set at the lowest point between the dominant (highest) peak and its nearest peak on either the left or right side, depending on the pixel distribution in the histogram. If there are more pixels represented on the left side of the dominant peak, the threshold will be on the left side. If there are more pixels represented on the right side of the dominant peak, the threshold will be on the right side. In the example histogram below, M_DOMINANT places the threshold on the right side of the dominant peak because there is a greater pixel density (indicated by the area under the curve) on the right side of the peak. In triangular bisection threshold mode (using either M_TRIANGLE_BISECTION_DARK or M_TRIANGLE_BISECTION_BRIGHT), MIL internally generates an intensity histogram of the source image. A line is drawn between the highest peak in the histogram and the lowest point either heading towards the minimum intensity value (in the dark region of the histogram) or heading towards the maximum intensity value (in the bright region of the histogram), depending on whether performing a dark or bright bisection, respectively. The threshold value is the intensity value at the inflection point (that is, the point furthest below the line). Note that all points above the line are ignored. Triangular bisection threshold mode works best when there is only one peak in the histogram. To determine whether to use triangular bisection dark or bright mode, examine the distance between the most commonly-occurring pixel value (the top of the peak) and the least commonly-occurring pixel value (the bottom of the peak). If the distance between the most commonly-occurring pixel value and the least commonly-occurring pixel value is greatest as the values go towards the minimum intensity value, try the triangular bisection dark mode. If, however, the distance is greatest as the values go towards the maximum intensity value, try the triangular bisection bright mode. Specifying the threshold value(s) manually Regardless of the number of threshold values that the binarize condition uses, you can pass MimBinarize() the threshold values manually. You can manually pass them in one of the following threshold modes: Percentile. Fixed. The percentile threshold mode (specified using M_PERCENTILE_VALUE) allows you to specify the threshold value(s) for the condition as a percentage of the histogram data. This mode uses an internally-generated cumulative histogram, whereby the X-axis represents the intensity value and the Y-axis represents the percentage of pixels equal to or less than the intensity value. In this mode, you specify a threshold value by specifying the percentage of pixels that must be equal to or less than the required threshold value. For example, if the specified value is 90, the cumulative histogram shown below depicts that 90% of the values are below 170. Therefore, the threshold value is 170. Percentile thresholds are typically used when the histogram varies greatly from image to image. Using a percentage instead of a fixed value, leads the threshold to be linked to the histogram rather than a fixed value. Alternatively, you can use the fixed threshold mode (with M_FIXED) to explicitly specify the threshold value(s) manually. Clipping Clipping changes the image data less dramatically than binarizing. It changes the data to include only the range of pixel values in which you are interested. MimClip() takes a condition with at most two threshold points and replaces only those pixels that meet the condition with given values. Pixels that do not meet the condition are unaffected. This can be useful to change data from one data type to another. For example, if you have a 16-bit result, but most of the pixels are less than 256, you could clip the result into an 8-bit buffer, and set all the pixels that are too big to the largest possible value, that is, 255. Thresholding your images Binarizing Determining the threshold value from the histogram Specifying the threshold value(s) manually Clipping ",
      "wordCount": 1148,
      "subEntries": []
    },
    {
      "id": "UG_image_Adaptive_binarizing",
      "version": null,
      "title": "Adaptive binarizing",
      "subTitles": [
        "Adaptive binarize contexts",
        "Controlling an adaptive binarize context",
        "Threshold with Niblack",
        "Threshold with local mean ",
        "Threshold with Bernsen",
        "Threshold with pseudomedian",
        "Hysteresis",
        "Controlling an adaptive binarize context that uses seeds",
        "Threshold with geodesic reconstruction",
        "Threshold with leveling",
        "Threshold with toggling"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\image\\Adaptive_binarizing.htm",
      "text": " Adaptive binarizing To perform an adaptive binarization, call MimBinarizeAdaptive(). Similar to a conventional binarization (MimBinarize()), an adaptive binarization reduces an image to two grayscale values (such as white and black) by comparing each source pixel value against its corresponding threshold value. Pixels greater than their threshold are set to the highest unsigned destination buffer value. For example, in an 8-bit destination buffer, the highest value is 0xFF (255). Pixels less than or equal to their corresponding threshold are set to 0. This process can help identify pixels that are part of the object (pixels set to the highest value) and pixels that aren't (pixels set to 0). MimBinarizeAdaptive() establishes threshold values according to the threshold mode control (MimControl() with M_THRESHOLD_MODE). Available threshold modes refer to adaptive algorithms, which perform multiple calculations using numerous sections of an image. When compared to a conventional binarization, which calculates threshold values using an image as a whole, an adaptive binarization is more robust at processing images containing pixels that are difficult to identify. Such difficulties typically come from poor or varying illumination or contrast. This type of robustness can cause an adaptive binarization to take longer to execute than a conventional one. The following illustration shows that an adaptive binarization can better identify a crack in a cup, when compared to a conventional binarization, given poor lighting and contrast. In this source image, pixel intensities transition from black (background) to gray (cup) to white (crack) to less white (reflectance on cup). Despite these complex shifts in intensity, an adaptive binarization is able to discern which pixels do not belong to the general quality of the image (the crack). Conventional binarizations are typically less robust at handling such intensity shifts. The following example illustrates adaptive binarizations, and also compares them with conventional binarizations. mimbinarizeadaptive.cpp By default, MIL binzarizes objects that are lighter than the background. To binarize objects that are darker than the background, set the M_FOREGROUND_VALUE control to M_FOREGROUND_BLACK. You would change the default to, for example, binarize a black cup on a white background. Adaptive binarize contexts When you call MimBinarizeAdaptive(), you must specify an adaptive binarize type of image processing context, allocated using MimAlloc() with M_BINARIZE_ADAPTIVE_CONTEXT or M_BINARIZE_ADAPTIVE_FROM_SEED_CONTEXT. Although both contexts perform an adaptive binarization, the threshold algorithms available for an M_BINARIZE_ADAPTIVE_FROM_SEED_CONTEXT context type use seeds in its calculations. Seeds refer to positions of significance, in the source, that affect resulting threshold values, so required information is preserved. You can either specify your own seed images or let MIL determine the seeds. M_BINARIZE_ADAPTIVE_FROM_SEED_CONTEXT typically applies when there is some foreknowledge about how images should look. Without such information, an M_BINARIZE_ADAPTIVE_CONTEXT type of context is usually best. Customizing an adaptive binarization generally requires changing the context's threshold mode, and its related controls, using MimControl(). Available controls typically depend on the type of the context. Use MimInquire() to inquire the control's settings. Some threshold modes use morphological processes, such as erode and dilate or open and close. For more information, see the Erosion and dilation section earlier in this chapter and the Opening and closing section earlier in this chapter. After MIL establishes the threshold values, you can adjust them for binarization, using M_GLOBAL_OFFSET. For example, if you set M_GLOBAL_OFFSET to 1, MIL increases all threshold values by 1, and then binarizes the source image. The specified offset is reflected in the threshold destination image, which you can retrieve using MimBinarizeAdaptive() and the ThresholdImageBufId parameter. The default global offset is 0 (no adjustment). For an adaptive binarize context that does not use seeds, you can use M_GLOBAL_MIN and M_GLOBAL_MAX to specify a minimum and maximum threshold value with which to binarize. The threshold destination image will not hold values that surpass these limits. Exceeding values are clipped. By default, MIL binarizes pixels with an intensity higher than the maximum as part of the foreground (object). To change this behavior, use the M_FOREGROUND_VALUE control. By default, there is no maximum threshold value restriction. For any adaptive binarize context type, the source image can have a minimum or maximum value restriction (MbufControl() with M_MIN or M_MAX). These limits are reflected in the resulting threshold destination image. If the source image has a minimum or maximum value, and you are using M_GLOBAL_MIN or M_GLOBAL_MAX (for an adaptive binarize context that does not use seeds), MIL uses the greater minimum value, or the lesser maximum value, as the actual limits. Controlling an adaptive binarize context To control how MIL establishes the threshold values for an M_BINARIZE_ADAPTIVE_CONTEXT context type, you can set M_THRESHOLD_MODE to M_NIBLACK, M_LOCAL_MEAN, M_BERNSEN, or M_PSEUDOMEDIAN. You can specify to use a hysteresis process by setting M_THRESHOLD_TYPE to M_HYSTERESIS. For more information, see the Hysteresis subsection of this section. Threshold with Niblack By default, MIL uses Niblack's adaptive threshold algorithm (M_NIBLACK). This setting offers the highest precision. The processing time is usually quite quick. The following represents the general logic for calculating threshold values using Niblack. If (M_NIBLACK_BIAS x StandardDeviationToLocalMean) &lt; M_MINIMUM_CONTRAST: ThresholdValue = M_GLOBAL_MIN. Otherwise: ThresholdValue = LocalMean + (M_NIBLACK_BIAS x StandardDeviationToLocalMean) + M_GLOBAL_OFFSET. To specify the size of the neighborhood around each source pixel with which to calculate the local mean (LocalMean) and the standard deviation (StandardDeviationToLocalMean), use M_LOCAL_DIMENSION. The size should be set to the largest square that represents a uniform background, and is bigger than the expected thickness of the object. M_MINIMUM_CONTRAST represents the minimum distance between a pixel and the local mean, for MimBinarizeAdaptive() to consider the pixel a part of the object. M_NIBLACK_BIAS gives you some global control over the thresholding. A higher bias classifies fainter values as part of the object; as a result, noise can be more likely classified as part of the object. A lower bias is more likely to ignore noise, though it can also ignore values you consider part of the object. Threshold with local mean A local mean adaptive threshold algorithm (M_LOCAL_MEAN) is a simplified version of Niblack (M_NIBLACK). The differences are: M_NIBLACK_BIAS must be 0 and M_MINIMUM_CONTRAST is ignored. A local mean threshold usually results in a faster, though less precise, binarization than Niblack. Threshold with Bernsen Bernsen's adaptive threshold algorithm (M_BERNSEN) represents a type of morphological erosion and dilation. This threshold results in the fastest process. The following represents the general logic for calculating threshold values using Bernsen. If (MaxNeighborhoodValue - MinNeighborhoodValue) &lt; M_MINIMUM_CONTRAST: ThresholdValue = M_GLOBAL_MIN. Otherwise: ThresholdValue = (MaxNeighborhoodValue + MinNeighborhoodValue) / 2 + M_GLOBAL_OFFSET. To specify the size of the neighborhood around each source pixel with which to calculate the minimum (MinNeighborhoodValue) and maximum (MaxNeighborhoodValue) neighborhood value, use M_LOCAL_DIMENSION. The size should be set to a value that is somewhat wider than the thickness of objects. This ensures that both background and object pixels are present in the neighborhood of any object pixel. When using M_BERNSEN, MIL compares the minimum contrast (M_MINIMUM_CONTRAST) to the difference between maximum and minimum values of the neighborhood. This guards against over binarizing relatively homogeneous regions. Typically, you should set the minimum contrast to a value above 5 (the default). Threshold with pseudomedian A pseudomedian adaptive threshold algorithm (M_PSEUDOMEDIAN) is similar to a Bernsen threshold, except it represents a type of morphological open and close process instead of erosion and dilation. The following represents the general logic for calculating threshold values using M_PSEUDOMEDIAN. If (MorphOpenValue - MorphCloseValue) &lt; M_MINIMUM_CONTRAST: ThresholdValue = M_GLOBAL_MIN. Otherwise: ThresholdValue = (MorphOpenValue + MorphCloseValue) / 2 + M_GLOBAL_OFFSET. With pseudomedian, MIL determines the threshold value for each pixel using the mean of grayscale values obtained by performing a morphological open (MorphOpenValue) and close (MorphCloseValue) operation on the source image. To specify the size of the neighborhood around each source pixel with which to perform the morphological open and close operations, use M_LOCAL_DIMENSION. Set the size to a value that is approximately half the expected thickness of objects. When using M_PSEUDOMEDIAN, MIL compares the minimum contrast (M_MINIMUM_CONTRAST) to the difference between the opened and closed values of pixels. This guards against over binarizing relatively homogeneous regions. Hysteresis The adaptive binarization threshold that you specify with M_THRESHOLD_MODE uses a hysteresis process if you specify a threshold type (with M_THRESHOLD_TYPE) other than M_SINGLE. In this case, a seed type thresholding (geodesic reconstruction) is performed after a second pass of the specified threshold. Specifically, MIL internally calls MimBinarizeAdaptive() twice. The first pass processes as usual, according to the M_THRESHOLD_MODE setting. For the second pass, MimBinarizeAdaptive() uses the M_..._SECOND_PASS settings while reapplying the same threshold. MIL then performs a geodesic reconstruction (a type of morphological erosion or dilation), using the results of the first pass as the source data and the results of the second pass as the seed data. This is similar to an M_RECONSTRUCT threshold mode (available with an M_BINARIZE_ADAPTIVE_FROM_SEED_CONTEXT context type). For more information, see the Threshold with geodesic reconstruction subsection of this section. Regardless of threshold mode, MIL uses M_GLOBAL_OFFSET_SECOND_PASS instead of M_GLOBAL_OFFSET during the second pass of MimBinarizeAdaptive(). For an M_NIBLACK threshold mode, the second pass also uses M_NIBLACK_BIAS_SECOND_PASS instead of M_NIBLACK_BIAS. MIL generates an error if every M_..._SECOND_PASS value it uses is the same as its first pass counterpart. You can set M_THRESHOLD_TYPE to specify which values to use as foreground. Setting M_THRESHOLD_TYPE to M_IN_RANGE specifies to use as foreground the values inside the range defined by the two hysteresis passes. The M_OUT_RANGE setting specifies to use the values outside this range as the foreground. Controlling an adaptive binarize context that uses seeds To control how MIL establishes the threshold values for an M_BINARIZE_ADAPTIVE_FROM_SEED_CONTEXT context type, you can set M_THRESHOLD_MODE to M_RECONSTRUCT, M_LEVEL, or M_TOGGLE. By default, MIL applies the specified threshold mode iteratively until the process reaches idempotence. Idempotence refers to the number of iterations at which subsequent iterations do not alter results. To specify a specific number of iterations, use M_NB_ITERATIONS. The following illustrates how the threshold value is determined iteratively from the seed image. Threshold with geodesic reconstruction By default, MIL uses an adaptive geodesic reconstruction threshold (M_RECONSTRUCT). This represents a type of morphological erosion or dilation, with seeds. When you call MimBinarizeAdaptive(), you can provide a seed image. If you do not, MIL internally establishes the seed data. MIL performs the geodesic reconstruction according to the foreground (M_FOREGROUND_VALUE). If you specify an image and the foreground is white (default), MIL dilates the pixels of the seed until they reach darker pixels in the source. For a black foreground, MIL erodes the pixels of the seed until they reach lighter pixels in the source. MIL then applies the offset (M_GLOBAL_OFFSET) to the resulting image and uses it as the threshold with which to perform the binarization. The following illustrates a one-dimensional grayscale profile of a source image and a seed image that you can use with a geodesic reconstruction threshold. If you do not specify a seed image, MIL either erodes (for a white foreground) or dilates (for a black foreground) the source image, according to the number of seed iterations (M_NB_SEED_ITERATIONS). The processing time resulting from an M_RECONSTRUCT threshold is typically faster than M_LEVEL and slower than M_TOGGLE. Threshold with leveling A level threshold (M_LEVEL) represents a type of morphological erosion and dilation, with seeds. When you call MimBinarizeAdaptive(), you can provide a seed image. If you do not, MIL internally establishes the seed data. Leveling essentially performs two geodesic reconstructions (M_RECONSTRUCT). One that processes the foreground as white, and the other that processes the foreground as black. For the white foreground, MIL dilates the pixels of the seed until they reach darker pixels in the source. For the black foreground, MIL erodes the pixels of the seed until they reach lighter pixels in the source. MIL then applies the offset (M_GLOBAL_OFFSET) to the resulting image and uses it as the threshold with which to perform the binarization. The following illustrates a one-dimensional grayscale profile of a source image and a seed image that you can use with a level threshold. If you do not specify a seed image, MIL establishes them by performing a convolution on the source image using a smoothing filter (kernel), according to the number of seed iterations (M_NB_SEED_ITERATIONS). The processing time resulting from an M_LEVEL threshold is typically longer than M_RECONSTRUCT or M_TOGGLE. M_LEVEL generally takes twice as long as M_RECONSTRUCT. Threshold with toggling A toggle threshold (M_TOGGLE) depends on two types of seed data (typically min and max values), each of which MIL can use to establish the threshold values with which to binarize. When you call MimBinarizeAdaptive(), you can provide two seed images. If you do not, MIL internally establishes the seed data. For every source pixel, MIL applies the offset (M_GLOBAL_OFFSET) and compares the result to both seed values. The threshold with which to binarize is the seed to which the result is closest. This can be seen as snapping each threshold value to one of the two seed values. Unlike other threshold modes, this is a single iteration process (M_NB_ITERATIONS is always one). If the source pixel is perfectly equidistant from both seeds, MIL uses the source pixel value itself as the threshold value. The following illustrates a one-dimensional grayscale profile of a source image and two seed images that you can use with a toggle threshold. The threshold is the closest seed. If you do not specify a seed image, MIL performs an erosion on its automatically established first seeds, and a dilation on its automatically established second seeds, according to the number of seed iterations (M_NB_SEED_ITERATIONS). The processing time resulting from an M_TOGGLE threshold is typically faster than M_RECONSTRUCT or M_LEVEL. Since toggling categorizes threshold values as one of two possibilities, it typically implies that you have already preprocessed your images. Adaptive binarizing Adaptive binarize contexts Controlling an adaptive binarize context Threshold with Niblack Threshold with local mean Threshold with Bernsen Threshold with pseudomedian Hysteresis Controlling an adaptive binarize context that uses seeds Threshold with geodesic reconstruction Threshold with leveling Threshold with toggling ",
      "wordCount": 2311,
      "subEntries": []
    },
    {
      "id": "UG_image_Removing_nonuniform_lighting_from_grabbed_images",
      "version": null,
      "title": "Removing uneven lighting from grabbed images",
      "subTitles": [
        "How to remove non-uniform lighting from grabbed images"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\image\\Removing_nonuniform_lighting_from_grabbed_images.htm",
      "text": " Removing uneven lighting from grabbed images The quality of processing results can be degraded if the source images have been taken under non-uniform lighting. Non-uniform lighting can be caused by many different scenarios. It is detectable by taking a line profile (for example, along the diagonal) in an image of a uniform gray object in the imaging setup. To remove uneven lighting, you can use MimFlatField(). To use MimFlatField(), you must set up a flat-field image processing context, using MimControl(). The flat-field image processing context must contain a flat image, a gain constant, and optionally two dark images. MimFlatField() applies the flat-field image processing context to the source image using the following modified formula. The flat image provides an example of the uneven lighting without any objects in the foreground, while the gain constant is required to normalize (or scale) the result, typically back to the full dynamic range of the destination image. Note that the gain factor can be automatically generated using M_GAIN_CONST set to M_AUTOMATIC. In this case, the gain factor is calculated based on the average of the denominator (that is, Avg(AvgFlatImg - AvgDarkImg02)). The two dark images can be replaced by constants (for example, to remove these two images, replace them with a zero in the calculation). As an example of using images, this formula combines the various images of the processing context in the following way: Note that you can also use MimFlatField() to remove CCD artifacts from grabbed images. For more information, refer to the Removing CCD artifacts from grabbed images section of Chapter 5: Specialized image processing. How to remove non-uniform lighting from grabbed images Perform the following steps to remove non-uniform lighting from your grabbed images: Allocate a flat-field image processing context, using MimAlloc() with M_FLAT_FIELD_CONTEXT. If necessary, using the same exposure time as was used for the source image, grab multiple images taken with the lens cap placed on the lens. Average the grabbed images by performing a mean operation (using MimStatCalculate() when MimControl() with M_STAT_MEAN is set to M_ENABLE). Add this image to the flat-field image processing context, using MimControl() with M_DARK_IMAGE. This averaged image will be used as AvgDarkImg01 for the calculation. Note that, in most cases, if you provide an offset image (AvgDarkImg02), you should provide a dark image (AvgDarkImg01). If a dark image is not necessary, set MimControl() with M_DARK_CONST to 0. Using an exposure time set so that no pixel is saturated, grab multiple images of the same area as in your source image, except place a uniform gray object in the field of view (such as, grabbing an image of a blank gray piece of paper). Average the grabbed images by performing a mean operation (using MimStatCalculate() when MimControl() with M_STAT_MEAN is set to M_ENABLE). Add this image to the flat-field image processing context, using MimControl() with M_FLAT_IMAGE. This averaged image will be used as AvgFlatImg for the calculation. If necessary, using the same exposure time as for the flat image, grab multiple images taken with the lens cap placed on the lens. Average the grabbed images by performing a mean operation (using MimStatCalculate() when MimControl() with M_STAT_MEAN is set to M_ENABLE). Add this image to the flat-field image processing context, using MimControl() with M_OFFSET_IMAGE. This averaged image will be used as AvgDarkImg02 for the calculation. Note that, in most cases, if you provide a dark image (AvgDarkImg01), you should provide an offset image (AvgDarkImg02). If a dark image is not necessary, set MimControl() with M_OFFSET_CONST to 0. Specify the gain factor to apply, using MimControl() with M_GAIN_CONST. To automatically calculate an appropriate gain factor to scale the results, typically back to the full dynamic range of the destination image, use M_GAIN_CONST set to M_AUTOMATIC. Alternatively, you can set M_GAIN_CONST to any value greater than 0. The automatically calculated gain factor might not always result in the best gain factor to use for your image; this is typically because the range of pixel values in the AvgFlatImg image is too great. In this case, inquire the automatically calculated gain factor, using MimInquire() with M_EFFECTIVE_GAIN_CONST. If darkening light pixels (removing saturation) from the image, set M_GAIN_CONST to a value lower than the automatically calculated gain factor. Whereas, if lightening dark pixels, set M_GAIN_CONST to a value higher than the automatically calculated gain factor. Preprocess the context by calling MimFlatField() with M_PREPROCESS. Both the source and/or destination image buffers can be set to M_NULL. If, however, the source or destination image is provided, it should be a typical source or destination image, respectively, and it will be used in the preprocess operation to better optimize future calls. If the preprocess operation is not done explicitly, it will be done when MimFlatField() is first called. Call MimFlatField() with the configured flat-field image processing context and your source image. The non-uniform lighting in the image should be improved. Note that you might have to perform the operation several times before determining the best value for the gain. Removing uneven lighting from grabbed images How to remove non-uniform lighting from grabbed images ",
      "wordCount": 844,
      "subEntries": []
    },
    {
      "id": "UG_image_Adjusting_image_focus",
      "version": null,
      "title": "Adjusting image focus",
      "subTitles": [
        "Auto-focusing",
        "Edge enhancement",
        "Extending your depth of field"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\image\\Adjusting_image_focus.htm",
      "text": " Adjusting image focus If your image is not in focus, you might want to adjust it to achieve better results when processing. Focus issues occur when the focus distance of the camera results in some or all portions of the image being out of focus, and appearing blurry. As unfocused objects in images often appear with poorly-defined edges and other features, there are a range of techniques to adjust for focus issues, both before and after the image is grabbed. Auto-focusing If your lens can be adjusted using a motor, you can use MdigFocus() to adjust the lens motor of the camera to a position that produces optimum focus in your grabbed images. This process is known as auto-focusing. Auto-focusing can be used in any imaging setup in which your lens can be adjusted by a motor, and is most useful in an imaging setup where precise measurements of objects in images are required, and frequent focus adjustments are required to grab acceptable images. In some cases, it might be impossible to have all objects in a grabbed image appear in focus, because having some objects in focus causes others to be outside the tolerated distance of the focus distance of your lens. In these cases, you can create a child buffer (or an ROI) that encompasses only the object in focus and operate only on the child buffer. Then, adjust the focus for a different area, grab, and move the child buffer to a new area in focus. Alternatively, you can try the other suggestions in this section. For more information on auto-focusing, see the Auto-focusing section of Chapter 27: Grabbing with your digitizer. Edge enhancement You can improve the focus of your image by sharpening the edges present in the image. Sharpening the edges allows you to accentuate the details of the image. For more information on edge enhancement, see the Edge enhancement subsection of the Enhancing and detecting edges section later in this chapter. Extending your depth of field At a given focus distance for a grab, the entire image is not always in focus, resulting in blurry or poorly-defined objects in the image. This is due to objects resting outside of the tolerated distance, or depth of field from that focus distance. You can use MregCalculate() to extend your depth of field to correct for focus issues in images; this function uses the best possible focus sections of multiple images of the same objects, taken at different focus levels. This emulates a larger depth of field, such that all objects of interest appear in focus, and is especially useful when you need to grab and process images of multiple objects at different depths. For more information on extending the depth of field of an image, see the Extending your depth of field section of Chapter 11: Registration. Adjusting image focus Auto-focusing Edge enhancement Extending your depth of field ",
      "wordCount": 484,
      "subEntries": []
    },
    {
      "id": "UG_image_Enhancing_and_detecting_edges",
      "version": null,
      "title": "Enhancing and detecting edges",
      "subTitles": [
        "Edge enhancement",
        "Sharpening edge enhancement",
        "Edge detection",
        "Horizontal and vertical edge detection",
        "Laplacian-based edge detection",
        "Gradient-based edge detection"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\image\\Enhancing_and_detecting_edges.htm",
      "text": " Enhancing and detecting edges Many applications perform various edge operations on images to increase the quality of the image or to limit some other operation on the image. In general, edges can be established from intensity transitions between two or more adjacent pixels in an image. Horizontal edges are created when horizontally connected pixels have values that are different from those immediately above or below them. Vertical edges are created when vertically connected pixels have values that are different from those immediately to the left or right of them. Oblique edges are created from a combination of horizontal and vertical components. There are three main categories of edge operations: Operations that enhance edges to sharpen the image. Operations that detect edges in the image. Operations that extract edges in the image. These edge operations are performed using convolutions (or neighborhood operations that replace each pixel with a weighted sum of each pixel's neighborhood). The weights applied to the neighborhood determine the type of operation that is performed. For example, certain weights produce a horizontal edge detection, while others produce a vertical one. For the first two types of edge operations, the weights are specified using either a Finite or Infinite Impulse Response (FIR or IIR) filter. FIR filters operate on a finite neighborhood to calculate the value of a pixel. IIR filters take into account all values in an image. The MimConvolve() function offers predefined FIR filters for most common edge enhancement and detection operations. If the predefined filters do not meet your needs, you can define a custom edge enhancement or detection FIR filter or a custom edge detection IIR filter to use with MimConvolve(). You can also use a custom edge enhancement IIR filter to use with MimDifferential(); this function combines multiple convolutions to perform the edge enhancing operation. To define a custom FIR filter, allocate a kernel buffer using MbufAlloc1d() or MbufAlloc2d() and load the kernel buffer with values, using MbufPut(). To define a custom IIR filter, allocate a linear IIR filter image processing context, using MimAlloc() with M_LINEAR_FILTER_IIR_CONTEXT, and then specify the appropriate operation control type settings using MimControl() with Linear IIR filter image processing context ID. For more information on custom filters, see the Custom spatial filters section of Chapter 4: Advanced image processing. For information on edge extraction operations, see the MIL Edge Finder module section of Chapter 10: Edge Finder. Edge enhancement Edge enhancement operations amplify edges, accentuating details in the image. Note that these operations might not produce good results for further processing because when you enhance edges, you also enhance noise pixels. Sharpening edge enhancement Sharpening edges can be useful to enhance edges in the image while preserving other parts of the image. You can obtain approximately the same result by performing a Laplacian-based edge detection operation on the image and adding the found edges to the original image. To sharpen edges in an image, you can use MimConvolve() with the M_SHARPEN_8 or M_SHARPEN_4 predefined FIR filters. When using predefined FIR filters, the M_SHARPEN_8 filter tends to produce more enhanced or sharpened edges than the M_SHARPEN_4 filter. You can also sharpen edges in an image using MimDifferential() with the Operation parameter set to M_SHARPEN. Edge detection Edge detection operations reveal intensity transitions in the image. The smoother the image, the more gradual the change in intensity, and the weaker the detection will be. Horizontal and vertical edge detection Detecting the horizontal and vertical edges in the image can be useful to enhance edges in a certain direction and remove those in another. To detect the horizontal edges, you can use MimConvolve() with the M_HORIZONTAL_EDGE_PREWITT or M_HORIZONTAL_EDGE_SOBEL predefined FIR filters. To detect the vertical edges, you can use MimConvolve() with the M_VERTICAL_EDGE_PREWITT or M_VERTICAL_EDGE_SOBEL predefined FIR filters. Alternatively, you can use custom IIR filters with MimControl() to detect edges. To detect horizontal edges, you can use M_FIRST_DERIVATIVE_Y. To detect vertical edges, you can use M_FIRST_DERIVATIVE_X. Laplacian-based edge detection The Laplacian-based operations place emphasis on the maximum values, or peaks, within the image. The edge representation of the resulting image generally looks very similar to the actual image. To detect the Laplacian-based edges from an image, you can use MimConvolve() with the M_LAPLACIAN_4 or M_LAPLACIAN_8 predefined FIR filters. When using predefined FIR filters, the M_LAPLACIAN_8 filter tends to produce more enhanced or sharpened edges than the M_LAPLACIAN_4 filter. You can also detect the Laplacian-based edges from an image using MimDifferential() with the Operation parameter set to M_LAPLACIAN. Gradient-based edge detection When performing a gradient-based edge detection operation, edges are determined from the rate of change between pixel values in the image, without regard to the direction of the edges. The resulting image contains only positive values. To detect the gradient-based edges from an image, you can use MimConvolve() with the M_EDGE_DETECT_SOBEL_FAST or M_EDGE_DETECT_PREWITT_FAST predefined FIR filters. You can also detect the gradient-based edges from an image using MimDifferential() with the Operation parameter set to M_GRADIENT or M_GRADIENT_SQR. For an advanced gradient-based edge detection operation, you can use MimEdgeDetect(). This function produces a gradient intensity image and/or a gradient angle image. Enhancing and detecting edges Edge enhancement Sharpening edge enhancement Edge detection Horizontal and vertical edge detection Laplacian-based edge detection Gradient-based edge detection ",
      "wordCount": 871,
      "subEntries": []
    },
    {
      "id": "UG_image_Basic_geometrical_transform",
      "version": null,
      "title": "Basic geometric transforms",
      "subTitles": [
        "Interpolation modes",
        "Nearest-neighbor interpolation",
        "Bilinear interpolation",
        "Bicubic interpolation",
        "Average interpolation",
        "M_INTERPOLATE interpolation",
        "Maximum/minimum interpolation"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\image\\Basic_geometrical_transform.htm",
      "text": " Basic geometric transforms Image distortions can affect application results. For example, in a medical application that analyzes blood cells, if the camera does not have a one-to-one aspect ratio and no correction is performed, the cells appear distorted and elongated, and incorrect interpretations might result. Rotating such an image causes even more serious object distortion. To resolve distortion problems, the MIL Image Processing module offers basic, as well as advanced, geometric functions. Since the advanced geometric functions (MimPolarTransform() and MimWarp()) are slower than the basic geometric functions, they should only be used when the required transform cannot be performed using a basic geometric function. For information on the advanced geometric functions, see Chapter 4: Advanced image processing. Note that MIL-Lite does not support advanced geometric functions, nor does it support rotating, translating, or flipping the image. In some instances, the orientation of an image can also cause erroneous conclusions. When an object is rotated from its original position, you can realign it by the required angle, using MimRotate(). If the image has prominent edges that identify the right orientation, you can have MIL automatically calculate the required angle of rotation for a proper orientation using MimFindOrientation(). MimFindOrientation() establishes the best orientations based on the edges in your image and calculates the angles at which they occur. You can then call MimRotate() with the angle that was returned with the best score to rotate your image. For more information on finding the dominant orientations of your image, see the Finding dominant image orientations section of Chapter 4: Advanced image processing. MimTranslate() displaces an image by a specified number of pixels in the X and/or Y direction, with subpixel accuracy. MimFlip() flips an image horizontally (left to right) or vertically (top to bottom). Note that flipping horizontally allows you to get a mirror copy of the original image. The MimResize() function resizes an image along the horizontal and/or vertical axis. This can help resolve aspect-ratio problems. If both the horizontal and vertical resizing factors are set to the same value, this function can reduce or magnify an image to an appropriate size. Interpolation modes When you perform a geometric transformation, each pixel position in the destination buffer (xd, yd), gets associated with a specific point in the source buffer (xs, ys), and a computed intensity value for (xs, ys) is then copied to (xd, yd). The destination coordinates have integer values but the source coordinates, in general, do not. Therefore, the pixel value at (xd, yd) is determined from several source pixels that are near (xs, ys), according to a specified interpolation mode. The various interpolation modes available in MIL are: A standard nearest neighbor interpolation (M_NEAREST_NEIGHBOR). A standard bilinear interpolation (M_BILINEAR). A standard bicubic interpolation (M_BICUBIC). A weighted average interpolation (M_AVERAGE). An interpolation based on the minimum or maximum pixel value in the neighborhood of the source point (M_MIN or M_MAX). Nearest-neighbor interpolation Nearest-neighbor interpolation mode determines the intensity value of the point based on the intensity of the nearest source pixel in the source image. No other neighboring values are taken into account. In the following image, a source image goes through a resize operation (MimResize()) using a nearest neighbor interpolation: Bilinear interpolation Bilinear interpolation mode calculates an intensity value for the source point by taking a weighted average of the four source pixels nearest to the source point; it takes the 2 nearest pixels in the X-direction, and the 2 nearest pixels in the Y-direction. The pixels closest to the point are given the most weight. Although processing is slightly slower, this results in a smoother and more accurate interpolation than when using a nearest neighbor operation. In the following image, a resize operation (MimResize()) is performed on the source image using a bilinear interpolation: Bicubic interpolation Bicubic interpolation mode calculates an intensity value for the source point by taking a weighted average of the sixteen source pixels nearest to the source point (a 4x4 area); it takes an area established by the 4 nearest pixels in the X-direction, and the 4 nearest pixels in the Y-direction. Again, the pixels closest to the point are given the most weight. Note that the sum of the weights used for bicubic interpolation might be greater than one. If this occurs, the result is saturated according to the depth and data type of the destination buffer. This interpolation mode requires a relatively large amount of processing time, but yields the best and the most accurate results of all the general interpolation modes. Average interpolation Average interpolation mode calculates an intensity value for the source point by taking a weighted average of the pixel area that surrounds the source point, whereby the area size is established from the scaling factor and the size of the source image. This mode is only available when using MimResize() and can only be used to reduce the size of an image (a scaling factor smaller than or equal to 1). Essentially, this mode takes a weighted average of the source pixel area that the destination pixel represents. For example, if the source buffer is a 25x25 buffer and you are using a scale factor of 0.2, the destination buffer required will be a 5x5 buffer. Therefore, each destination pixel represents a 5x5 area in the source buffer, so a weighted average of the 25 source pixels nearest to the source point (xs, ys) is taken. The weight of each source pixel is determined by the area it contributes to the destination pixel. The greater the area that a source pixel contributes to a destination pixel, the larger the weight of the source pixel. This interpolation mode performs the most accurate interpolation for resizing; however, depending on the scaling factor, the operation can be more processing intensive than other interpolation modes, due to the fact that the neighborhood being evaluated could be large. In the image below, an image is resized (MimResize()) using an M_AVERAGE interpolation mode): In this case, the source point Sp is an average of the source pixels S1, S2, S3, and S4. The weights of each source pixel depends on its contributing area, and therefore the intensity of D1 will be: Note that 2.25 is the total area contributed by the source pixels. In the above example, when solving for the source point's resulting intensity, the following result is obtained: M_INTERPOLATE interpolation M_INTERPOLATE interpolation mode calculates an intensity value for the source point based on the scaling operation. When performing a \"zoom in\" resizing operation, a bilinear interpolation will be used. When performing a \"zoom out\" resizing operation, averaging interpolation will be used. This mode is only available when using MimResize(). This provides a good quality resulting image and an adequate processing speed. Maximum/minimum interpolation Maximum and minimum interpolation modes calculate an intensity value for the source point by taking the maximum or minimum pixel value in the source pixel area that surrounds the source point, whereby the area's size is established from the scaling factor and the size of the source image. These modes are only available when using MimResize() and can only be used when reducing the size of an image (a scaling factor smaller than or equal to 1). For example, if the source image is 4x4 and the scaling factor is 0.5, each destination pixel represents a 2x2 area in the source buffer. If using an M_MAX interpolation mode, the highest pixel value in the 2x2 area around the source point, is used as the intensity value of the source point. If performing a resize operation to reduce the size of an image using other interpolation modes (for example, M_AVERAGE) introduces blurriness in certain features you want to preserve, such as text and edges, try using M_MIN or M_MAX. These interpolations are similar to morphological operations. Although they can decrease an object's thickness and amplify noise, they can be effective at maintaining the distinctiveness of certain features. Basic geometric transforms Interpolation modes Nearest-neighbor interpolation Bilinear interpolation Bicubic interpolation Average interpolation M_INTERPOLATE interpolation Maximum/minimum interpolation ",
      "wordCount": 1336,
      "subEntries": []
    },
    {
      "id": "UG_image_Arithmetic_with_images",
      "version": null,
      "title": "Mathematics with images",
      "subTitles": [
        "Basic mathematical operations",
        "Computing the integral of an image",
        "Mapping an image"
      ],
      "location": "MIL UG P03: 2D processing and analysis",
      "pageURL": "content\\UserGuide\\image\\Arithmetic_with_images.htm",
      "text": " Mathematics with images It is often useful to perform mathematical operations on images. The MIL Image Processing module allows you to perform mathematical operations on images using MimArith(). If MimArith() does not support a required operation, you can map pixel values to their expected result, using MimLutMap(). Basic mathematical operations You can apply the following basic mathematical operations, using MimArith(): You can add, subtract, multiply, divide, AND, NAND, OR, XOR, NOR, or XNOR two images or an image and a constant. You can negate, take the absolute value, NOT, square, square root, or cube the image. You can perform rounding operations such as the ceiling, floor, round, round half up, and truncation operations on the image. You can perform the trigonometric atan2(x, y) function using the first image for the X-coordinates and the second image for the Y-coordinates. You can copy a constant to the entire result buffer. For example, for a surveillance application, you can extract the constant background from the grabbed image and display only changes in the image. The following example shows how this can be done. MimArith(GrabImage, BackgroundImage, MilImage, M_SUB_ABS); The basic mathematical operations are point-to-point, that is, they apply the specified operator on individual pixel values in a source image or on pixels at corresponding locations in two source images and do not depend on neighboring values. Computing the integral of an image You can also use MimArith() to take the integral of an image. Calculating the integral of the image allows you to quickly calculate the summation of pixels in a rectangular area. Each pixel in the integral image contains the total sum of the pixels in the rectangular section of the source defined by the pixel (0, 0) and itself, inclusively. This is known as the inclusive integral of the image. The following image demonstrates a source image and its corresponding integral image. Notice how the value at (2, 2) in the integral image equals the sum of pixels in the source image from (0, 0) to (2, 2). Once the integral image has been computed, you can use it to quickly calculate the summation of pixel values in a selected area of pixels in the source image. To do so, you subtract the summation of the area above and to the left of the selected area from the summation of the area (0, 0) to the bottom-right pixel of the selected area; then, add the summation of the area until the top left-most pixel bordering the selected area. The latter is done to account for the overlap of the subtracted regions. Essentially, this means looking at four points of reference in the integral image. The following image illustrates how you can perform this quick calculation, as well as a selected area in the source image, and its corresponding location in the integral image. Therefore, the sum of the selected area above is 31. The calculation of the integral image can be done in constant time, and in some cases, it is more efficient to calculate the integral of the entire image when you need to obtain the sum of multiple rectangular regions of pixels in an image, instead of calculating the sum of those specific areas individually. Mapping an image You can perform complex operations (such as scaling and logarithms) on an image buffer, using MimLutMap(). This function performs the operation simply by mapping the source image buffer through a specified lookup table (LUT) and storing results in the specified destination image buffer. You allocate a LUT buffer, using MbufAlloc1d(), specifying the buffer attribute as M_LUT. You can assign mapping values to it by copying custom data from an array into it, using MbufPut1d(). You can also generate data directly into a LUT buffer according to a specified function, using MgenLutFunction(). If you simply want to invert the image or set the image to a constant, you can alternatively use MgenLutRamp(). Mathematics with images Basic mathematical operations Computing the integral of an image Mapping an image ",
      "wordCount": 666,
      "subEntries": []
    }
  ]
}]